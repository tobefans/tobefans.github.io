<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>vetch的小小世界</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://weiquanfan.xyz/"/>
  <updated>2020-05-13T16:51:28.692Z</updated>
  <id>http://weiquanfan.xyz/</id>
  
  <author>
    <name>Weiquan Fan</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>CNN的进击之路——讲讲ResNet, Inception, ResNeXt和Densenet等常见网络</title>
    <link href="http://weiquanfan.xyz/2020/05/13/resnet/"/>
    <id>http://weiquanfan.xyz/2020/05/13/resnet/</id>
    <published>2020-05-13T06:50:15.000Z</published>
    <updated>2020-05-13T16:51:28.692Z</updated>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;本文是一篇大杂烩，按照发布时间总结了CNN的一些常见网络。&lt;/p&gt;
&lt;h2 id=&quot;AlexNet&quot;&gt;&lt;a href=&quot;#AlexNet&quot; 
      
    
    </summary>
    
      <category term="深度学习模型" scheme="http://weiquanfan.xyz/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B/"/>
    
    
  </entry>
  
  <entry>
    <title>cv中Attention的奇妙旅途——讲讲Self-Attention, SENet和CBAM</title>
    <link href="http://weiquanfan.xyz/2020/05/08/SENet/"/>
    <id>http://weiquanfan.xyz/2020/05/08/SENet/</id>
    <published>2020-05-08T07:57:04.000Z</published>
    <updated>2020-05-08T12:29:06.433Z</updated>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;由于注意力机制的高速发展，我尝试着对attention形成一种比较系统化的理解，选了比较有代表性的Self-Attention, SENet
      
    
    </summary>
    
      <category term="深度学习模型" scheme="http://weiquanfan.xyz/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B/"/>
    
    
      <category term="attention" scheme="http://weiquanfan.xyz/tags/attention/"/>
    
  </entry>
  
  <entry>
    <title>讲讲横扫nlp任务的BERT模型</title>
    <link href="http://weiquanfan.xyz/2020/05/07/BERT/"/>
    <id>http://weiquanfan.xyz/2020/05/07/BERT/</id>
    <published>2020-05-07T08:08:20.000Z</published>
    <updated>2020-05-07T12:58:59.538Z</updated>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;本文讲解Google在2019年发表的论文&lt;a href=&quot;https://arxiv.org/pdf/1810.04805.pdf&quot; ta
      
    
    </summary>
    
      <category term="深度学习模型" scheme="http://weiquanfan.xyz/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B/"/>
    
    
      <category term="BERT" scheme="http://weiquanfan.xyz/tags/BERT/"/>
    
      <category term="Transformer" scheme="http://weiquanfan.xyz/tags/Transformer/"/>
    
      <category term="GLUE" scheme="http://weiquanfan.xyz/tags/GLUE/"/>
    
  </entry>
  
  <entry>
    <title>Transfomer以及Self-Attention讲解</title>
    <link href="http://weiquanfan.xyz/2020/05/05/transfomer/"/>
    <id>http://weiquanfan.xyz/2020/05/05/transfomer/</id>
    <published>2020-05-05T12:23:39.000Z</published>
    <updated>2020-05-08T12:30:41.778Z</updated>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;这一篇主要讲解谷歌发表的&lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot; target=&quot;_blank&quot;
      
    
    </summary>
    
      <category term="深度学习模型" scheme="http://weiquanfan.xyz/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B/"/>
    
    
      <category term="Transformer" scheme="http://weiquanfan.xyz/tags/Transformer/"/>
    
      <category term="attention" scheme="http://weiquanfan.xyz/tags/attention/"/>
    
  </entry>
  
  <entry>
    <title>常见的梯度下降算法原理</title>
    <link href="http://weiquanfan.xyz/2020/05/04/gradient-descent/"/>
    <id>http://weiquanfan.xyz/2020/05/04/gradient-descent/</id>
    <published>2020-05-04T04:58:54.000Z</published>
    <updated>2020-05-04T16:45:54.475Z</updated>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;梯度下降算法（Gradient Descent Optimization）是神经网络模型训练最常用的优化算法。对于深度学习模型，基本都是采用
      
    
    </summary>
    
      <category term="机器学习" scheme="http://weiquanfan.xyz/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>opensmile 工具的使用和批处理</title>
    <link href="http://weiquanfan.xyz/2020/05/02/opensmile/"/>
    <id>http://weiquanfan.xyz/2020/05/02/opensmile/</id>
    <published>2020-05-02T12:32:13.000Z</published>
    <updated>2020-05-03T03:48:28.519Z</updated>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;openSMILE是一款以命令行形式运行的工具，通过配置config文件来提取音频特征。主要应用于语音识别、情感计算、音乐信息获取。2.0版
      
    
    </summary>
    
      <category term="语音特征" scheme="http://weiquanfan.xyz/categories/%E8%AF%AD%E9%9F%B3%E7%89%B9%E5%BE%81/"/>
    
    
  </entry>
  
  <entry>
    <title>语谱图的matlab提取和python提取</title>
    <link href="http://weiquanfan.xyz/2020/05/02/specgram/"/>
    <id>http://weiquanfan.xyz/2020/05/02/specgram/</id>
    <published>2020-05-02T08:43:28.000Z</published>
    <updated>2020-05-03T03:48:23.090Z</updated>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;语谱图（spectrogram或specgram），也叫声谱图，可以简单看做一个二维矩阵，其纵轴表示频率，横轴表示时间，矩阵的值表示能量强弱
      
    
    </summary>
    
      <category term="语音特征" scheme="http://weiquanfan.xyz/categories/%E8%AF%AD%E9%9F%B3%E7%89%B9%E5%BE%81/"/>
    
    
  </entry>
  
  <entry>
    <title>使用Github Pages和Hexo搭建自己的独立博客</title>
    <link href="http://weiquanfan.xyz/2019/08/11/How-to-use-Hexo-to-build-your-blog/"/>
    <id>http://weiquanfan.xyz/2019/08/11/How-to-use-Hexo-to-build-your-blog/</id>
    <published>2019-08-11T11:10:47.000Z</published>
    <updated>2019-08-14T17:28:12.904Z</updated>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Github Pages: Github Pages可以被认为是用户编写的、托管在github上的静态网页。&lt;/li&gt;
&lt;li&gt;
      
    
    </summary>
    
    
  </entry>
  
</feed>

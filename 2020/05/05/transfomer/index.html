<!DOCTYPE html>
<html lang=zh>
<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="renderer" content="webkit">
  <meta http-equiv="Cache-Control" content="no-transform">
  <meta http-equiv="Cache-Control" content="no-siteapp">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <meta name="format-detection" content="telephone=no,email=no,adress=no">
  <!-- Color theme for statusbar -->
  <meta name="theme-color" content="#000000">
  <!-- 强制页面在当前窗口以独立页面显示,防止别人在框架里调用页面 -->
  <meta http-equiv="window-target" content="_top">
  
  
  <title>Transfomer以及Self-Attention讲解 | Hexo</title>
  <meta name="description" content="前言这一篇主要讲解谷歌发表的Attention Is All You Need。这篇论文提出了驰名的一种注意力机制 —— self-attention 模块，并进一步提出了 Transformer 架构，从而将以往用的计算代价较大的RNN替换掉了。目前，nlp任务中效果非常好的BERT模型就是大量应用了Transformer架构的Encoder。 下边是一个很好的使用Transformer进行机器">
<meta name="keywords" content="Transformer,attention">
<meta property="og:type" content="article">
<meta property="og:title" content="Transfomer以及Self-Attention讲解">
<meta property="og:url" content="http://weiquanfan.xyz/2020/05/05/transfomer/index.html">
<meta property="og:site_name" content="vetch的小小世界">
<meta property="og:description" content="前言这一篇主要讲解谷歌发表的Attention Is All You Need。这篇论文提出了驰名的一种注意力机制 —— self-attention 模块，并进一步提出了 Transformer 架构，从而将以往用的计算代价较大的RNN替换掉了。目前，nlp任务中效果非常好的BERT模型就是大量应用了Transformer架构的Encoder。 下边是一个很好的使用Transformer进行机器">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://weiquanfan.xyz/2020/05/05/transfomer/transfomer_framework.jpg">
<meta property="og:image" content="http://weiquanfan.xyz/2020/05/05/transfomer/transfomer_framework2.png">
<meta property="og:image" content="http://weiquanfan.xyz/2020/05/05/transfomer/position_embedding.png">
<meta property="og:image" content="http://weiquanfan.xyz/2020/05/05/transfomer/self_attention_example.jpg">
<meta property="og:image" content="http://weiquanfan.xyz/2020/05/05/transfomer/self_attention_example2.jpg">
<meta property="og:image" content="http://weiquanfan.xyz/2020/05/05/transfomer/self_attention_example3.jpg">
<meta property="og:image" content="http://weiquanfan.xyz/2020/05/05/transfomer/self_attention_fomula.png">
<meta property="og:image" content="http://weiquanfan.xyz/2020/05/05/transfomer/Multi-Head.jpg">
<meta property="og:image" content="http://weiquanfan.xyz/2020/05/05/transfomer/Multi-Head2.jpg">
<meta property="og:image" content="https://pic1.zhimg.com/80/v2-01188c490446b01b55fe316723576070_1440w.jpg?30">
<meta property="og:image" content="https://pic1.zhimg.com/80/v2-7fb3ac456169471c26ac795b6d208b7c_1440w.png?50">
<meta property="og:updated_time" content="2020-06-20T07:02:34.144Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Transfomer以及Self-Attention讲解">
<meta name="twitter:description" content="前言这一篇主要讲解谷歌发表的Attention Is All You Need。这篇论文提出了驰名的一种注意力机制 —— self-attention 模块，并进一步提出了 Transformer 架构，从而将以往用的计算代价较大的RNN替换掉了。目前，nlp任务中效果非常好的BERT模型就是大量应用了Transformer架构的Encoder。 下边是一个很好的使用Transformer进行机器">
<meta name="twitter:image" content="http://weiquanfan.xyz/2020/05/05/transfomer/transfomer_framework.jpg">
  <!-- Canonical links -->
  <link rel="canonical" href="http://weiquanfan.xyz/2020/05/05/transfomer/index.html">
  
    <link rel="alternate" href="/atom.xml" title="vetch的小小世界" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png" type="image/x-icon">
  
  <link rel="stylesheet" href="/css/style.css">
  
    <link href="//cdn.jsdelivr.net/npm/katex@0.9.0/dist/katex.min.css" rel="stylesheet"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  
  
  
  
</head>


<body class="main-center" itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><header class="header" itemscope itemtype="http://schema.org/WPHeader">
  <div class="slimContent">
    <div class="navbar-header">
      
      
      <div class="profile-block text-center">
        <a id="avatar" href="https://github.com/tobefans" target="_blank">
          <img class="img-circle img-rotate" src="/images/photo.jpg" width="200" height="200">
        </a>
        <h2 id="name" class="hidden-xs hidden-sm">Weiquan Fan</h2>
        <h3 id="title" class="hidden-xs hidden-sm hidden-md">AI练习生</h3>
        <small id="location" class="text-muted hidden-xs hidden-sm"><i class="icon icon-map-marker"></i> Shenzhen, China</small>
      </div>
      
      <div class="search" id="search-form-wrap">

    <form class="search-form sidebar-form">
        <div class="input-group">
            <input type="text" class="search-form-input form-control" placeholder="Search" />
            <span class="input-group-btn">
                <button type="submit" class="search-form-submit btn btn-flat" onclick="return false;"><i class="icon icon-search"></i></button>
            </span>
        </div>
    </form>
    <div class="ins-search">
  <div class="ins-search-mask"></div>
  <div class="ins-search-container">
    <div class="ins-input-wrapper">
      <input type="text" class="ins-search-input" placeholder="Type something..." x-webkit-speech />
      <button type="button" class="close ins-close ins-selectable" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
    </div>
    <div class="ins-section-wrapper">
      <div class="ins-section-container"></div>
    </div>
  </div>
</div>


</div>
      <button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>
    <nav id="main-navbar" class="collapse navbar-collapse" itemscope itemtype="http://schema.org/SiteNavigationElement" role="navigation">
      <ul class="nav navbar-nav main-nav ">
        
        
        <li class="menu-item menu-item-home">
          <a href="/.">
            
            <i class="icon icon-home-fill"></i>
            
            <span class="menu-title">Home</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-archives">
          <a href="/archives">
            
            <i class="icon icon-archives-fill"></i>
            
            <span class="menu-title">Archives</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-categories">
          <a href="/categories">
            
            <i class="icon icon-folder"></i>
            
            <span class="menu-title">Categories</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-tags">
          <a href="/tags">
            
            <i class="icon icon-tags"></i>
            
            <span class="menu-title">Tags</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-repository">
          <a href="/repository">
            
            <i class="icon icon-project"></i>
            
            <span class="menu-title">Repository</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-books">
          <a href="/books">
            
            <i class="icon icon-book-fill"></i>
            
            <span class="menu-title">Books</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-links">
          <a href="/links">
            
            <i class="icon icon-friendship"></i>
            
            <span class="menu-title">Links</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-about">
          <a href="/about">
            
            <i class="icon icon-cup-fill"></i>
            
            <span class="menu-title">About</span>
          </a>
        </li>
        
      </ul>
      
	
    <ul class="social-links">
    	
        <li><a href="https://github.com/tobefans" target="_blank" title="Github" data-toggle=tooltip data-placement=top><i class="icon icon-github"></i></a></li>
        
        <li><a href="https://github.com/tobefans" target="_blank" title="Weibo" data-toggle=tooltip data-placement=top><i class="icon icon-weibo"></i></a></li>
        
        <li><a href="https://github.com/tobefans" target="_blank" title="Twitter" data-toggle=tooltip data-placement=top><i class="icon icon-twitter"></i></a></li>
        
        <li><a href="https://github.com/tobefans" target="_blank" title="Behance" data-toggle=tooltip data-placement=top><i class="icon icon-behance"></i></a></li>
        
        <li><a href="/atom.xml" target="_blank" title="Rss" data-toggle=tooltip data-placement=top><i class="icon icon-rss"></i></a></li>
        
    </ul>

    </nav>
  </div>
</header>

  
    <aside class="sidebar" itemscope itemtype="http://schema.org/WPSideBar">
  <div class="slimContent">
    
      <div class="widget">
    <h3 class="widget-title">Board</h3>
    <div class="widget-body">
        <div id="board">
            <div class="content">
                <p>欢迎交流与分享经验!</p>
            </div>
        </div>
    </div>
</div>

    
      
  <div class="widget">
    <h3 class="widget-title">Categories</h3>
    <div class="widget-body">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/工具使用/">工具使用</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/强化学习/">强化学习</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习/">机器学习</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/深度学习模型/">深度学习模型</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/爬虫/">爬虫</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/界面/">界面</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/语音特征/">语音特征</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/语音识别/">语音识别</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/语音预处理/">语音预处理</a><span class="category-list-count">2</span></li></ul>
    </div>
  </div>


    
      
  <div class="widget">
    <h3 class="widget-title">Tags</h3>
    <div class="widget-body">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/BERT/">BERT</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GLUE/">GLUE</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Transformer/">Transformer</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/attention/">attention</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/卷积/">卷积</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/工具使用/">工具使用</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/强化学习/">强化学习</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/界面/">界面</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/端点检测/">端点检测</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/语音识别/">语音识别</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/语音预处理/">语音预处理</a><span class="tag-list-count">2</span></li></ul>
    </div>
  </div>


    
      
  <div class="widget">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget-body tagcloud">
      <a href="/tags/BERT/" style="font-size: 13px;">BERT</a> <a href="/tags/GLUE/" style="font-size: 13px;">GLUE</a> <a href="/tags/Transformer/" style="font-size: 14px;">Transformer</a> <a href="/tags/attention/" style="font-size: 14px;">attention</a> <a href="/tags/卷积/" style="font-size: 13px;">卷积</a> <a href="/tags/工具使用/" style="font-size: 13px;">工具使用</a> <a href="/tags/强化学习/" style="font-size: 13px;">强化学习</a> <a href="/tags/界面/" style="font-size: 13px;">界面</a> <a href="/tags/端点检测/" style="font-size: 13px;">端点检测</a> <a href="/tags/语音识别/" style="font-size: 14px;">语音识别</a> <a href="/tags/语音预处理/" style="font-size: 14px;">语音预处理</a>
    </div>
  </div>

    
      
  <div class="widget">
    <h3 class="widget-title">Archive</h3>
    <div class="widget-body">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/08/">August 2021</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/07/">July 2021</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/04/">April 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/11/">November 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/06/">June 2020</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">May 2020</a><span class="archive-list-count">11</span></li></ul>
    </div>
  </div>


    
      
  <div class="widget">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget-body">
      <ul class="recent-post-list list-unstyled no-thumbnail">
        
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                
              </p>
              <p class="item-title">
                <a href="/2021/08/22/nvidia-driver/" class="title">nvidia-driver</a>
              </p>
              <p class="item-date">
                <time datetime="2021-08-22T14:10:15.000Z" itemprop="datePublished">2021-08-22</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/语音特征/">语音特征</a>
              </p>
              <p class="item-title">
                <a href="/2021/08/22/audio-features/" class="title">语音特征小结</a>
              </p>
              <p class="item-date">
                <time datetime="2021-08-22T12:36:20.000Z" itemprop="datePublished">2021-08-22</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/工具使用/">工具使用</a>
              </p>
              <p class="item-title">
                <a href="/2021/07/28/vscode/" class="title">VSCode的服务器和github同步</a>
              </p>
              <p class="item-date">
                <time datetime="2021-07-28T15:27:44.000Z" itemprop="datePublished">2021-07-28</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/语音预处理/">语音预处理</a>
              </p>
              <p class="item-title">
                <a href="/2021/07/28/vad/" class="title">语音的预处理--端点检测</a>
              </p>
              <p class="item-date">
                <time datetime="2021-07-28T15:27:39.000Z" itemprop="datePublished">2021-07-28</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/语音预处理/">语音预处理</a>
              </p>
              <p class="item-title">
                <a href="/2021/07/27/denose/" class="title">语音的预处理--去噪</a>
              </p>
              <p class="item-date">
                <time datetime="2021-07-27T15:50:34.000Z" itemprop="datePublished">2021-07-27</time>
              </p>
            </div>
          </li>
          
      </ul>
    </div>
  </div>
  

    
  </div>
</aside>

  
  
<main class="main" role="main">
  <div class="content">
  <article id="post-transfomer" class="article article-type-post" itemscope itemtype="http://schema.org/BlogPosting">
    
    <div class="article-header">
      
        
  
    <h1 class="article-title" itemprop="name">
      Transfomer以及Self-Attention讲解
    </h1>
  

      
      <div class="article-meta">
        <span class="article-date">
    <i class="icon icon-calendar-check"></i>
	<a href="/2020/05/05/transfomer/" class="article-date">
	  <time datetime="2020-05-05T12:23:39.000Z" itemprop="datePublished">2020-05-05</time>
	</a>
</span>
        
  <span class="article-category">
    <i class="icon icon-folder"></i>
    <a class="article-category-link" href="/categories/深度学习模型/">深度学习模型</a>
  </span>

        
  <span class="article-tag">
    <i class="icon icon-tags"></i>
	<a class="article-tag-link" href="/tags/Transformer/">Transformer</a>, <a class="article-tag-link" href="/tags/attention/">attention</a>
  </span>


        

	<span class="article-read hidden-xs">
    	<i class="icon icon-eye-fill" aria-hidden="true"></i>
    	<span id="/2020/05/05/transfomer/" class="leancloud_visitors"  data-flag-title="Transfomer以及Self-Attention讲解">0</span>
    </span>

        <span class="post-comment"><i class="icon icon-comment"></i> <a href="/2020/05/05/transfomer/#comments" class="article-comment-link">Comments</a></span>
        
      </div>
    </div>
    <div class="article-entry marked-body" itemprop="articleBody">
      
        <h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>这一篇主要讲解谷歌发表的<a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention Is All You Need</a>。这篇论文提出了驰名的一种注意力机制 —— self-attention 模块，并进一步提出了 Transformer 架构，从而将以往用的计算代价较大的RNN替换掉了。目前，nlp任务中效果非常好的BERT模型就是大量应用了Transformer架构的Encoder。</p>
<p>下边是一个很好的使用Transformer进行机器翻译任务的例子。在预测过程中，编码阶段，输入的“I arrived at the”中的每个单词都会计算与所有单词的注意力权重，并加权求和得出新的自己的表示，逐层编码。解码阶段，输入由encoder出来的所有单词的表示和上一个位置输出的embedding，经过类似的注意力操作得到这一个位置的输出，是一种随着预测位置移动的迭代过程。<br><a href="https://pic4.zhimg.com/v2-b1b7cd5637f7c844510fd460e0e2c807_b.webp?80" target="_blank" rel="noopener">戳我看例子</a></p>
<h2 id="总体框架与流程"><a href="#总体框架与流程" class="headerlink" title="总体框架与流程"></a>总体框架与流程</h2><h3 id="框架"><a href="#框架" class="headerlink" title="框架"></a>框架</h3><p>对照着以上例子，看下边的Transfomer总体框架图。左边为编码器，右边为解码器。编码器和解码器中都包含了Positional Encoding模块，Multi-Head Attention模块，Feed-Forward模块。下一章节会对此着重讲解。<br><img src="/2020/05/05/transfomer/transfomer_framework.jpg" alt></p>
<h3 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h3><p>定义一下符号。</p>
<ul>
<li>emb_dim：嵌入的尺寸</li>
<li>input_length：输入序列的长度</li>
<li>target_length：目标序列的长度+1。+1是因为要移位。</li>
<li>vocab_size：目标词汇表中的单词数量。</li>
</ul>
<p>则Transformer的流程可表示为：</p>
<ol>
<li>该模型将每个token表示为维度emb_dim的向量。然后，对于特定的输入序列，我们有了尺寸为（input_length）x（emb_dimb）的矩阵。</li>
<li>然后添加位置信息（位置编码）。与上一步一样，此步骤将返回尺寸为（input_length）x（emb_dim）的矩阵。</li>
<li>数据通过N=6个编码器块。之后，我们获得尺寸为（input_length）x（emb_dim）的矩阵。</li>
<li>目标序列经过等同于1和2的操作，并进行mask屏蔽。输出的尺寸为（target_length）x（emb_dim）。</li>
<li>4的结果经过N=6个解码器块。在每个迭代中，解码器都使用编码器的输出3）。这在总框图中由从编码器到解码器的箭头表示。输出的尺寸为（target_length）x（emb_dim）。</li>
<li>最后，逐行使用全连接层和softmax。输出的尺寸为（target_length）x（vocab_size）。</li>
</ol>
<p><img src="/2020/05/05/transfomer/transfomer_framework2.png" alt></p>
<p>编码器对于训练阶段和测试阶段是一样的编码过程，而解码器的流程则有所不同，因此先讲解一下解码器的训练和测试。<br>在<strong>测试阶段</strong>，由于没有groundtruth，所以我们需要从零开始不断迭代一个词一个词地生成。具体操作如下：</p>
<ol>
<li>计算输入序列的嵌入表示。</li>
<li>使用起始token例如’<ss>‘，作为第一个目标序列。该模型将预测输出一个token。</ss></li>
<li>将最后一个预测token添加到目标序列，并使用它生成新的预测。</li>
<li>重复执行步骤3，每次的输入token和输出token都增加，直到预测的token是表示序列结束的token，例如<eos>。</eos></li>
</ol>
<p>在<strong>训练阶段</strong>中，由于我们事先有roundtruth，因此我们将直接为模型提供整个已移位目标序列，并要求其预测未移位目标。<br>举个例子，目标是将句子从英语翻译成西班牙语：<br>X = [‘Hello’，’，’，’how’，’are’，’you’，’？’]（输入序列）<br>Y = [‘Hola’，’，’，’como’，’estas’， ‘？’]（目标序列）<br>在前面的示例之后，我们将给解码器输入：<br>[‘<ss>‘，’Hola’，’，’，’como’，’estas’，’？’]<br>预期的预测将是：<br>[‘Hola’，’，’，’como’，’estas’，’？’，’<eos>‘]</eos></ss></p>
<p>因此可以看到，解码器在训练时直接从target_length-&gt;target_length，而测试时则是从1-&gt;1 2-&gt;2 3-&gt;3 … target_length-&gt;target_length的过程，最后预测的是每次迭代中最后一个预测的token串联起来。</p>
<h2 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h2><p>Transformer抛弃了RNN，而RNN最大的优点就是在时间序列上对数据的抽象，所以文章中作者提出两种Positional Encoding的方法，将encoding后的数据与embedding数据求和，加入了相对位置信息。</p>
<ul>
<li>用不同频率的sine和cosine函数直接计算</li>
<li>学习出一份positional embedding<br>实验后发现两者结果一样，所以用了第一种方法，优点是不需要训练参数，而且即使在训练集中没有出现过的句子长度上也能用</li>
</ul>
<p><img src="/2020/05/05/transfomer/position_embedding.png" alt></p>
<p>对于输入序列，经过word embedding后，加上positional embedding后即可得到该序列的 representation，序列中的每个token都转换成包含 word 的特征和 word 在句子中的位置信息的向量。</p>
<h2 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h2><p>Multi-Head Attention其实就是多个Self-Attention结构的结合。因此，首先我们需要着重学习论文的重点Self-Attention。</p>
<h3 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h3><p>从一个比较知名的<a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener">例子</a>讲起。<br>假如我们要翻译一个词组Thinking Machines，其中Thinking的输入的embedding vector用x1表示，Machines的embedding vector用x2表示。当我们处理Thinking这个词时，我们需要计算句子中所有词与它的Attention Score，这就像将当前词作为搜索的query，去和句子中所有词（包含该词本身）的key去匹配（点乘），看看相关度有多高。相关度进行尺度缩放与softmax归一化可以得到注意力权重，注意力与相应的value加权求和就得到新的表达。</p>
<p><img src="/2020/05/05/transfomer/self_attention_example.jpg" alt></p>
<p>如果将输入的所有向量合并为矩阵形式，则所有query, key, value向量也可以合并为矩阵形式表示</p>
<p><img src="/2020/05/05/transfomer/self_attention_example2.jpg" alt></p>
<p>则上述操作可简化为矩阵形式</p>
<p><img src="/2020/05/05/transfomer/self_attention_example3.jpg" alt></p>
<p>这就是著名的注意力公式：</p>
<p><img src="/2020/05/05/transfomer/self_attention_fomula.png" alt></p>
<h3 id="Multi-Head-Attention-1"><a href="#Multi-Head-Attention-1" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h3><p>基于上边的Self-Attention， 我们进一步拓展，对输入序列使用不同的Q，K，V进行多次以上操作，而后拼接起来，再转换成最终的表示。这样每个head可以学习到在不同表示空间中的特征。</p>
<p><img src="/2020/05/05/transfomer/Multi-Head.jpg" alt></p>
<p>可视化如下：</p>
<p><img src="/2020/05/05/transfomer/Multi-Head2.jpg" alt></p>
<h3 id="Masked-Multi-Head-Attention"><a href="#Masked-Multi-Head-Attention" class="headerlink" title="Masked Multi-Head Attention"></a>Masked Multi-Head Attention</h3><p>在训练过程的解码器中，需要对输入的注意力矩阵（即上边QK经过softmax的矩阵）进行masked操作，从而不给模型看见未来信息，解决了信息泄露问题。举例来说，对于目标序列（I have a dream），I作为第一个单词，只能有和自身的attention。have作为第二个单词，有和I, have 两个attention。 a 作为第三个单词，有和I,have,a 前面三个单词的attention。到了最后一个单词dream的时候，才有对整个句子4个单词的attention。</p>
<p><img src="https://pic1.zhimg.com/80/v2-01188c490446b01b55fe316723576070_1440w.jpg?30" alt></p>
<p>其它操作和上述的Multi-Head Attention一致。</p>
<h3 id="Encoder-Decoder-Multi-Head-Attention"><a href="#Encoder-Decoder-Multi-Head-Attention" class="headerlink" title="Encoder-Decoder Multi-Head Attention"></a>Encoder-Decoder Multi-Head Attention</h3><p>在解码器的第二层attention里，需要整合encoder的输入序列和decoder的目标序列的信息，算出相互之间的注意力。与Multi-Head Attention的不同点在于，Encoder-Decoder Multi-Head Attention的Q矩阵来自decoder，而K和V来自encoder。其实也很好理解，就是注意力矩阵是由来自解码器的Query和来自编码器的Key之间计算得来，其它操作都相同。</p>
<h2 id="Feed-Forward"><a href="#Feed-Forward" class="headerlink" title="Feed-Forward"></a>Feed-Forward</h2><p>这个就很简单了，就是简单的映射层。</p>
<p><img src="https://pic1.zhimg.com/80/v2-7fb3ac456169471c26ac795b6d208b7c_1440w.png?50" alt></p>
<h2 id="Produce-Output-Probabilities"><a href="#Produce-Output-Probabilities" class="headerlink" title="Produce Output Probabilities"></a>Produce Output Probabilities</h2><p>这个其实也是普通的映射层，它将每一个目标序列的token由emb_dim映射到vocab_size，因此就可得到各个token，串成目标序列了。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>不得不说，这确实是一篇很经典的论文，将seq2seq模型推到了一个新高度，避免了RNN的大量计算代价，从此用CNN操作序列信号就有很好的效果了。另外，Self-Attention还跨界在cv行业也有了非常多的研究。可以说cv和nlp是同源的，只需要将图像的长宽拉成一列（空间信息）类比成序列信号的序列，图像的通道类比成序列信号的embedding即可。因此，Self-Attention模块的输入在nlp上是time × embedding，在cv上是spatial × channel。此外，当下横扫nlp的BERT模型也是基于Transfomer的encoder，这也表明这个模型的重要性了。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul>
<li><a href="https://medium.com/dissecting-bert/dissecting-bert-appendix-the-decoder-3b86f66b0e5f" target="_blank" rel="noopener">https://medium.com/dissecting-bert/dissecting-bert-appendix-the-decoder-3b86f66b0e5f</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/80986272" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/80986272</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/44121378" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/44121378</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/39034683" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/39034683</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/47282410" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/47282410</a></li>
</ul>

      
    </div>
    <div class="article-footer">
      <blockquote class="mt-2x">
  <ul class="post-copyright list-unstyled">
    
    <li class="post-copyright-link hidden-xs">
      <strong>本文链接：</strong>
      <a href="http://weiquanfan.xyz/2020/05/05/transfomer/" title="Transfomer以及Self-Attention讲解" target="_blank" rel="external">http://weiquanfan.xyz/2020/05/05/transfomer/</a>
    </li>
    
    <li class="post-copyright-license">
      <strong>版权声明： </strong> 本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by/4.0/deed.zh" target="_blank" rel="external">CC BY 4.0 CN协议</a> 许可协议。转载请注明出处！
    </li>
  </ul>
</blockquote>


<div class="panel panel-default panel-badger">
  <div class="panel-body">
    <figure class="media">
      <div class="media-left">
        <a href="https://github.com/tobefans" target="_blank" class="img-burn thumb-sm visible-lg">
          <img src="/images/photo.jpg" class="img-rounded w-full" alt="">
        </a>
      </div>
      <div class="media-body">
        <h3 class="media-heading"><a href="https://github.com/tobefans" target="_blank"><span class="text-dark">Weiquan Fan</span><small class="ml-1x">AI练习生</small></a></h3>
        <div>陌上花开，可缓缓归矣</div>
      </div>
    </figure>
  </div>
</div>


    </div>
  </article>
  
    
  <section id="comments">
  	
      <div id="vcomments"></div>
    
  </section>


  
</div>

  <nav class="bar bar-footer clearfix" data-stick-bottom>
  <div class="bar-inner">
  
  <ul class="pager pull-left">
    
    <li class="prev">
      <a href="/2020/05/07/BERT/" title="讲讲横扫nlp任务的BERT模型"><i class="icon icon-angle-left" aria-hidden="true"></i><span>&nbsp;&nbsp;Newer</span></a>
    </li>
    
    
    <li class="next">
      <a href="/2020/05/04/gradient-descent/" title="常见的梯度下降算法原理"><span>Older&nbsp;&nbsp;</span><i class="icon icon-angle-right" aria-hidden="true"></i></a>
    </li>
    
    
  </ul>
  
  
  <!-- Button trigger modal -->
  <button type="button" class="btn btn-fancy btn-donate pop-onhover bg-gradient-warning" data-toggle="modal" data-target="#donateModal"><span>$</span></button>
  <!-- <div class="wave-icon wave-icon-danger btn-donate" data-toggle="modal" data-target="#donateModal">
    <div class="wave-circle"><span class="icon"><i class="icon icon-bill"></i></span></div>
  </div> -->
  
  
  <div class="bar-right">
    
    <div class="share-component" data-sites="weibo,qq,wechat,facebook,twitter" data-mobile-sites="weibo,qq,qzone"></div>
    
  </div>
  </div>
</nav>
  
<!-- Modal -->
<div class="modal modal-center modal-small modal-xs-full fade" id="donateModal" tabindex="-1" role="dialog">
  <div class="modal-dialog" role="document">
    <div class="modal-content donate">
      <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
      <div class="modal-body">
        <div class="donate-box">
          <div class="donate-head">
            <p>Maybe you could buy me a cup of coffee.</p>
          </div>
          <div class="tab-content">
            <div role="tabpanel" class="tab-pane fade active in" id="alipay">
              <div class="donate-payimg">
                <img src="/images/donate/alipay.png" alt="Scan Qrcode" title="Scan" />
              </div>
              <p class="text-muted mv">Scan this qrcode</p>
              <p class="text-grey">Open alipay app scan this qrcode, buy me a coffee!</p>
            </div>
            <div role="tabpanel" class="tab-pane fade" id="wechatpay">
              <div class="donate-payimg">
                <img src="/images/donate/wachat.png" alt="Scan Qrcode" title="Scan" />
              </div>
              <p class="text-muted mv">Scan this qrcode</p>
              <p class="text-grey">Open wechat app scan this qrcode, buy me a coffee!</p>
            </div>
          </div>
          <div class="donate-footer">
            <ul class="nav nav-tabs nav-justified" role="tablist">
              <li role="presentation" class="active">
                <a href="#alipay" id="alipay-tab" role="tab" data-toggle="tab" aria-controls="alipay" aria-expanded="true"><i class="icon icon-alipay"></i> alipay</a>
              </li>
              <li role="presentation" class="">
                <a href="#wechatpay" role="tab" id="wechatpay-tab" data-toggle="tab" aria-controls="wechatpay" aria-expanded="false"><i class="icon icon-wepay"></i> wechat payment</a>
              </li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>



</main>

  <footer class="footer" itemscope itemtype="http://schema.org/WPFooter">
	
	
    <ul class="social-links">
    	
        <li><a href="https://github.com/tobefans" target="_blank" title="Github" data-toggle=tooltip data-placement=top><i class="icon icon-github"></i></a></li>
        
        <li><a href="https://github.com/tobefans" target="_blank" title="Weibo" data-toggle=tooltip data-placement=top><i class="icon icon-weibo"></i></a></li>
        
        <li><a href="https://github.com/tobefans" target="_blank" title="Twitter" data-toggle=tooltip data-placement=top><i class="icon icon-twitter"></i></a></li>
        
        <li><a href="https://github.com/tobefans" target="_blank" title="Behance" data-toggle=tooltip data-placement=top><i class="icon icon-behance"></i></a></li>
        
        <li><a href="/atom.xml" target="_blank" title="Rss" data-toggle=tooltip data-placement=top><i class="icon icon-rss"></i></a></li>
        
    </ul>

    <div class="copyright">
    	
        <div class="publishby">
        	Theme by <a href="https://github.com/cofess" target="_blank"> cofess </a>base on <a href="https://github.com/cofess/hexo-theme-pure" target="_blank">pure</a>.
        </div>
    </div>
</footer>
  <script src="//cdn.jsdelivr.net/npm/jquery@1.12.4/dist/jquery.min.js"></script>
<script>
window.jQuery || document.write('<script src="js/jquery.min.js"><\/script>')
</script>
<script src="/js/plugin.min.js"></script>
<script src="/js/application.js"></script>

    <script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>
<script src="/js/insight.js"></script>





   


<!-- custom analytics part create by xiamo -->
<script defer src="https://cdn1.lncld.net/static/js/av-min-1.2.1.js"></script>
<script defer>
AV.init({
  appId: 'G55DkRGHWai3lkkyvBotwMTl-gzGzoHsz',
  appKey: 'F0aXAudrJr6wsdAvrVvSp1T2'
});

function showTime(Counter) {
	var query = new AV.Query(Counter);
		var visitors= $('.leancloud_visitors');
		query.greaterThanOrEqualTo("time", 0);		
		query.find({
			success: function(results) {
				if (results.length == 0) {				
					return;
				}
				var data = results;
				visitors.each(function(){
					var url = $(this).attr('id').trim();					
					for (var i = 0; i < data.length; i++) {
						var object = data[i];
						var content = object.get('time');
						var _url = object.get('url')
						if(url == _url){
							$(this).text(content);
						}
					}
				})
				
			},
			error: function(object, error) {
				console.log("Error: " + error.code + " " + error.message);
			}
		});
}

function addCount(Counter) {
	var Counter = AV.Object.extend("Counter");
	url = $(".leancloud_visitors").attr('id').trim();
	title = $(".leancloud_visitors").attr('data-flag-title').trim();
	var query = new AV.Query(Counter);
	query.equalTo("url", url);
	query.find({
		success: function(results) {
			if (results.length > 0) {
				var counter = results[0];
				counter.fetchWhenSave(true);
				counter.increment("time");
				counter.save(null, {
					success: function(counter) {
						var content = counter.get('time');
						$(document.getElementById(url)).text(content);
					},
					error: function(counter, error) {
						console.log('Failed to save Visitor num, with error message: ' + error.message);
					}
				});
			} else {
				var newcounter = new Counter();
				newcounter.set("title", title);
				newcounter.set("url", url);
				newcounter.set("time", 1);
				newcounter.save(null, {
					success: function(newcounter) {
					    console.log("newcounter.get('time')="+newcounter.get('time'));
						var content = newcounter.get('time');
						$(document.getElementById(url)).text(content);
					},
					error: function(newcounter, error) {
						console.log('Failed to create');
					}
				});
			}
		},
		error: function(error) {
			console.log('Error:' + error.code + " " + error.message);
		}
	});
}
$(function() {
	var Counter = AV.Object.extend("Counter");
	if ($('.leancloud_visitors').length == 1) {
		addCount(Counter);
	} else {
		showTime(Counter);
	}
}); 
</script>



   
    
  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/valine"></script>
  <script type="text/javascript">
  var GUEST = ['nick', 'mail', 'link'];
  var meta = 'nick,mail,link';
  meta = meta.split(',').filter(function(item) {
    return GUEST.indexOf(item) > -1;
  });
  new Valine({
    el: '#vcomments',
    verify: false,
    notify: false,
    appId: 'G55DkRGHWai3lkkyvBotwMTl-gzGzoHsz',
    appKey: 'F0aXAudrJr6wsdAvrVvSp1T2',
    placeholder: '此时一位路人路过...',
    avatar: 'mm',
    meta: meta,
    pageSize: '10' || 10,
    visitor: false
  });
  </script>

     







<script src="/js/hexo_resize_image.js"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>
</html>
<!DOCTYPE html>
<html lang=zh>
<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="renderer" content="webkit">
  <meta http-equiv="Cache-Control" content="no-transform">
  <meta http-equiv="Cache-Control" content="no-siteapp">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <meta name="format-detection" content="telephone=no,email=no,adress=no">
  <!-- Color theme for statusbar -->
  <meta name="theme-color" content="#000000">
  <!-- 强制页面在当前窗口以独立页面显示,防止别人在框架里调用页面 -->
  <meta http-equiv="window-target" content="_top">
  
  
  <title>暴力即正义——讲讲横扫nlp任务的BERT模型 | Hexo</title>
  <meta name="description" content="前言本文讲解Google在2019年发表的论文BERT: Pre-training of Deep Bidirectional Transformers forLanguage Understanding 。从标题可以看出，该论文基于Transformer模型，提出了一款用于语言理解的预训练模型，并在GLUE, SQuAD等nlp任务中都取得了很好的效果。该模型的创新点实际上不在于模型结构，而在于">
<meta name="keywords" content="BERT,Transformer,GLUE">
<meta property="og:type" content="article">
<meta property="og:title" content="暴力即正义——讲讲横扫nlp任务的BERT模型">
<meta property="og:url" content="http://weiquanfan.xyz/2020/05/07/BERT/index.html">
<meta property="og:site_name" content="vetch的小小世界">
<meta property="og:description" content="前言本文讲解Google在2019年发表的论文BERT: Pre-training of Deep Bidirectional Transformers forLanguage Understanding 。从标题可以看出，该论文基于Transformer模型，提出了一款用于语言理解的预训练模型，并在GLUE, SQuAD等nlp任务中都取得了很好的效果。该模型的创新点实际上不在于模型结构，而在于">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://pic1.zhimg.com/80/v2-d942b566bde7c44704b7d03a1b596c0c_1440w.jpg">
<meta property="og:image" content="https://pic2.zhimg.com/80/v2-11505b394299037e999d12997e9d1789_1440w.jpg">
<meta property="og:image" content="https://pic2.zhimg.com/80/v2-b054e303cdafa0ce41ad761d5d0314e1_1440w.jpg">
<meta property="og:updated_time" content="2020-05-07T12:25:15.780Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="暴力即正义——讲讲横扫nlp任务的BERT模型">
<meta name="twitter:description" content="前言本文讲解Google在2019年发表的论文BERT: Pre-training of Deep Bidirectional Transformers forLanguage Understanding 。从标题可以看出，该论文基于Transformer模型，提出了一款用于语言理解的预训练模型，并在GLUE, SQuAD等nlp任务中都取得了很好的效果。该模型的创新点实际上不在于模型结构，而在于">
<meta name="twitter:image" content="https://pic1.zhimg.com/80/v2-d942b566bde7c44704b7d03a1b596c0c_1440w.jpg">
  <!-- Canonical links -->
  <link rel="canonical" href="http://weiquanfan.xyz/2020/05/07/BERT/index.html">
  
    <link rel="alternate" href="/atom.xml" title="vetch的小小世界" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png" type="image/x-icon">
  
  <link rel="stylesheet" href="/css/style.css">
  
    <link href="//cdn.jsdelivr.net/npm/katex@0.9.0/dist/katex.min.css" rel="stylesheet"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  
  
  
  
</head>


<body class="main-center" itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><header class="header" itemscope itemtype="http://schema.org/WPHeader">
  <div class="slimContent">
    <div class="navbar-header">
      
      
      <div class="profile-block text-center">
        <a id="avatar" href="https://github.com/tobefans" target="_blank">
          <img class="img-circle img-rotate" src="/images/photo.jpg" width="200" height="200">
        </a>
        <h2 id="name" class="hidden-xs hidden-sm">Weiquan Fan</h2>
        <h3 id="title" class="hidden-xs hidden-sm hidden-md">AI练习生</h3>
        <small id="location" class="text-muted hidden-xs hidden-sm"><i class="icon icon-map-marker"></i> Shenzhen, China</small>
      </div>
      
      <div class="search" id="search-form-wrap">

    <form class="search-form sidebar-form">
        <div class="input-group">
            <input type="text" class="search-form-input form-control" placeholder="Search" />
            <span class="input-group-btn">
                <button type="submit" class="search-form-submit btn btn-flat" onclick="return false;"><i class="icon icon-search"></i></button>
            </span>
        </div>
    </form>
    <div class="ins-search">
  <div class="ins-search-mask"></div>
  <div class="ins-search-container">
    <div class="ins-input-wrapper">
      <input type="text" class="ins-search-input" placeholder="Type something..." x-webkit-speech />
      <button type="button" class="close ins-close ins-selectable" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
    </div>
    <div class="ins-section-wrapper">
      <div class="ins-section-container"></div>
    </div>
  </div>
</div>


</div>
      <button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>
    <nav id="main-navbar" class="collapse navbar-collapse" itemscope itemtype="http://schema.org/SiteNavigationElement" role="navigation">
      <ul class="nav navbar-nav main-nav ">
        
        
        <li class="menu-item menu-item-home">
          <a href="/.">
            
            <i class="icon icon-home-fill"></i>
            
            <span class="menu-title">Home</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-archives">
          <a href="/archives">
            
            <i class="icon icon-archives-fill"></i>
            
            <span class="menu-title">Archives</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-categories">
          <a href="/categories">
            
            <i class="icon icon-folder"></i>
            
            <span class="menu-title">Categories</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-tags">
          <a href="/tags">
            
            <i class="icon icon-tags"></i>
            
            <span class="menu-title">Tags</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-repository">
          <a href="/repository">
            
            <i class="icon icon-project"></i>
            
            <span class="menu-title">Repository</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-books">
          <a href="/books">
            
            <i class="icon icon-book-fill"></i>
            
            <span class="menu-title">Books</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-links">
          <a href="/links">
            
            <i class="icon icon-friendship"></i>
            
            <span class="menu-title">Links</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-about">
          <a href="/about">
            
            <i class="icon icon-cup-fill"></i>
            
            <span class="menu-title">About</span>
          </a>
        </li>
        
      </ul>
      
	
    <ul class="social-links">
    	
        <li><a href="https://github.com/tobefans" target="_blank" title="Github" data-toggle=tooltip data-placement=top><i class="icon icon-github"></i></a></li>
        
        <li><a href="https://github.com/tobefans" target="_blank" title="Weibo" data-toggle=tooltip data-placement=top><i class="icon icon-weibo"></i></a></li>
        
        <li><a href="https://github.com/tobefans" target="_blank" title="Twitter" data-toggle=tooltip data-placement=top><i class="icon icon-twitter"></i></a></li>
        
        <li><a href="https://github.com/tobefans" target="_blank" title="Behance" data-toggle=tooltip data-placement=top><i class="icon icon-behance"></i></a></li>
        
        <li><a href="/atom.xml" target="_blank" title="Rss" data-toggle=tooltip data-placement=top><i class="icon icon-rss"></i></a></li>
        
    </ul>

    </nav>
  </div>
</header>

  
    <aside class="sidebar" itemscope itemtype="http://schema.org/WPSideBar">
  <div class="slimContent">
    
      <div class="widget">
    <h3 class="widget-title">Board</h3>
    <div class="widget-body">
        <div id="board">
            <div class="content">
                <p>欢迎交流与分享经验!</p>
            </div>
        </div>
    </div>
</div>

    
      
  <div class="widget">
    <h3 class="widget-title">Categories</h3>
    <div class="widget-body">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习/">机器学习</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/深度学习模型/">深度学习模型</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/语音特征/">语音特征</a><span class="category-list-count">2</span></li></ul>
    </div>
  </div>


    
      
  <div class="widget">
    <h3 class="widget-title">Tags</h3>
    <div class="widget-body">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/BERT/">BERT</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GLUE/">GLUE</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Transformer/">Transformer</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/self-attention/">self-attention</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </div>


    
      
  <div class="widget">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget-body tagcloud">
      <a href="/tags/BERT/" style="font-size: 13px;">BERT</a> <a href="/tags/GLUE/" style="font-size: 13px;">GLUE</a> <a href="/tags/Transformer/" style="font-size: 14px;">Transformer</a> <a href="/tags/self-attention/" style="font-size: 13px;">self-attention</a>
    </div>
  </div>

    
      
  <div class="widget">
    <h3 class="widget-title">Archive</h3>
    <div class="widget-body">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">May 2020</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>


    
      
  <div class="widget">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget-body">
      <ul class="recent-post-list list-unstyled no-thumbnail">
        
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/深度学习模型/">深度学习模型</a>
              </p>
              <p class="item-title">
                <a href="/2020/05/07/BERT/" class="title">暴力即正义——讲讲横扫nlp任务的BERT模型</a>
              </p>
              <p class="item-date">
                <time datetime="2020-05-07T08:08:20.000Z" itemprop="datePublished">2020-05-07</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/深度学习模型/">深度学习模型</a>
              </p>
              <p class="item-title">
                <a href="/2020/05/05/transfomer/" class="title">Transfomer以及Self-Attention讲解</a>
              </p>
              <p class="item-date">
                <time datetime="2020-05-05T12:23:39.000Z" itemprop="datePublished">2020-05-05</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/机器学习/">机器学习</a>
              </p>
              <p class="item-title">
                <a href="/2020/05/04/gradient-descent/" class="title">常见的梯度下降算法原理</a>
              </p>
              <p class="item-date">
                <time datetime="2020-05-04T04:58:54.000Z" itemprop="datePublished">2020-05-04</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/语音特征/">语音特征</a>
              </p>
              <p class="item-title">
                <a href="/2020/05/02/opensmile/" class="title">opensmile 工具的使用和批处理</a>
              </p>
              <p class="item-date">
                <time datetime="2020-05-02T12:32:13.000Z" itemprop="datePublished">2020-05-02</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                <a class="category-link" href="/categories/语音特征/">语音特征</a>
              </p>
              <p class="item-title">
                <a href="/2020/05/02/specgram/" class="title">语谱图的matlab提取和python提取</a>
              </p>
              <p class="item-date">
                <time datetime="2020-05-02T08:43:28.000Z" itemprop="datePublished">2020-05-02</time>
              </p>
            </div>
          </li>
          
      </ul>
    </div>
  </div>
  

    
  </div>
</aside>

  
  
<main class="main" role="main">
  <div class="content">
  <article id="post-BERT" class="article article-type-post" itemscope itemtype="http://schema.org/BlogPosting">
    
    <div class="article-header">
      
        
  
    <h1 class="article-title" itemprop="name">
      暴力即正义——讲讲横扫nlp任务的BERT模型
    </h1>
  

      
      <div class="article-meta">
        <span class="article-date">
    <i class="icon icon-calendar-check"></i>
	<a href="/2020/05/07/BERT/" class="article-date">
	  <time datetime="2020-05-07T08:08:20.000Z" itemprop="datePublished">2020-05-07</time>
	</a>
</span>
        
  <span class="article-category">
    <i class="icon icon-folder"></i>
    <a class="article-category-link" href="/categories/深度学习模型/">深度学习模型</a>
  </span>

        
  <span class="article-tag">
    <i class="icon icon-tags"></i>
	<a class="article-tag-link" href="/tags/BERT/">BERT</a>, <a class="article-tag-link" href="/tags/GLUE/">GLUE</a>, <a class="article-tag-link" href="/tags/Transformer/">Transformer</a>
  </span>


        

	<span class="article-read hidden-xs">
    	<i class="icon icon-eye-fill" aria-hidden="true"></i>
    	<span id="/2020/05/07/BERT/" class="leancloud_visitors"  data-flag-title="暴力即正义——讲讲横扫nlp任务的BERT模型">0</span>
    </span>

        <span class="post-comment"><i class="icon icon-comment"></i> <a href="/2020/05/07/BERT/#comments" class="article-comment-link">Comments</a></span>
        
      </div>
    </div>
    <div class="article-entry marked-body" itemprop="articleBody">
      
        <h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本文讲解Google在2019年发表的论文<a href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank" rel="noopener">BERT: Pre-training of Deep Bidirectional Transformers for<br>Language Understanding</a> 。从标题可以看出，该论文基于Transformer模型，提出了一款用于语言理解的预训练模型，并在GLUE, SQuAD等nlp任务中都取得了很好的效果。该模型的创新点实际上不在于模型结构，而在于预训练的方法。以下围绕这两方面都进行一些讲解。</p>
<h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><h3 id="总体框图"><a href="#总体框图" class="headerlink" title="总体框图"></a>总体框图</h3><p>首先最好还是先理解一下<a href="http://weiquanfan.xyz/2020/05/05/transfomer/">Transfomer</a>，这是BERT模型的基础所在。简而言之，Transfomer包含N个编码器和解码器。编码器将输入序列编码成带有全局新的特征序列，解码器将编码器的特征序列解码成预测结果。BERT使用的正是其中的编码器，模型如下所示。</p>
<p><img src="https://pic1.zhimg.com/80/v2-d942b566bde7c44704b7d03a1b596c0c_1440w.jpg" alt="BERT模型"></p>
<p><strong>注意：并不是一个Tm代表一个Transformer，可以看成一行Tm表示一个Transformer的编码器，而编码器也不是两层可以有多层。那么，输入序列送入BERT后，最后一层Tm将对每一个输入token都生成一个新的特征序列，而T则表示任务，一般都是用全连接层来完成分类任务。</strong>另外用来对比的是，与GPT是单向的Transformer连接，ELMo是双向的LSTM连接。</p>
<h3 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h3><p>从框图上可以看到，输入序列是各token的embedding。在Transfomer中，这种嵌入由word embedding加上positional embedding而来。而在BERT中，还要额外加上一个segment embedding，用于指示各个token属于输入的第几个句子。这是因为有的nlp任务是输入一对句子的，需要借此加以区分。而且，positional embedding也不是沿用三角函数，三种embedding都是学习出来的。<br>另外，一开始的token，除了要在一开始添加一个起始标志[CLS]之外，还要在不同句子的过渡位置插一个[SEP]标志（如果有多句子的话）。</p>
<p><img src="https://pic2.zhimg.com/80/v2-11505b394299037e999d12997e9d1789_1440w.jpg" alt="embedding"></p>
<h3 id="迁移策略"><a href="#迁移策略" class="headerlink" title="迁移策略"></a>迁移策略</h3><p>BERT是一个预训练模型，所以我们要怎么拿过来用呢？官方为我们提供了各种任务的应用方法。<br>首先，NLP的下游任务可以分为4类：</p>
<ul>
<li>句子关系判断：识别蕴含(entailment)、识别语义相似等</li>
<li>分类任务：文本分类、情感计算等</li>
<li>序列标注：分词、实体识别、语义标注等</li>
<li>生成式任务：机器翻译、文本摘要等</li>
</ul>
<p><img src="https://pic2.zhimg.com/80/v2-b054e303cdafa0ce41ad761d5d0314e1_1440w.jpg" alt="迁移策略"></p>
<p>图中 (a) 解决的是句子关系判断问题，MultiNLI(识别蕴含，M推理出N，蕴含/矛盾/中立），QQP（识别语义相似），QNLI（识别是否回答了问题），STS-B（识别语义相似），MRPC（识别语义等价，微软）、RTE（识别蕴含，小数据），SWAG（识别回答问题，大数据)。<br>(b) 解决的是分类任务，SST-2（情感计算，斯坦福），CoLA（句子语言性判断，是否能成句）。<br>(c) 解决的是序列标注任务，SQuAD（判断回答的起始和结束时刻，斯坦福问答数据集，从phrase中选取answer）。<br>(d) 解决的是序列标注任务，NER（命名实体识别）。<br>总的来说 (a) 和 (b) 都是分类任务，差别在于输入的是一个句子还是一对句子。这类任务，只需通过在第一个token（即[CLS]标志）的特征序列送入全连接层即可获取识别结果。(c) 和 (d) 都是序列标注任务，这类则在多个token的特征序列送入全连接层，获取各自的标注结果。可以看到，目前只有生成式任务还没有被ko。</p>
<p>p.s. 大名鼎鼎的GLUE任务集则包含了MultiNLI、QQP、QNLI、STS-B、MRPC、RTE、WNLI(也是识别蕴含)、SST-2、CoLA。</p>
<h2 id="预训练方法"><a href="#预训练方法" class="headerlink" title="预训练方法"></a>预训练方法</h2><p>常见的预训练方法一般是给前面的序列去预测下一个token（像是GPT）。而BERT就提出了两个比较有意思的训练任务 —— Masked LM 和 Next Sentence Prediction。</p>
<h3 id="Masked-LM"><a href="#Masked-LM" class="headerlink" title="Masked LM"></a>Masked LM</h3><p>为了实现模型的双向，就不能一直给定前面预测后面，于是作者提出了一个trick。在训练过程中，随机mask掉15%的token，即把相应位置的token替换成一个[MASK]标识，而任务的目标就是要去恢复这个句子（包含MASK的词），而损失函数只考虑了MASK位置的预测值，忽视掉非masked的值。这样，MASK的词可前可后，就实现了模型的双向性。</p>
<p>由此也衍生出了一个问题，就是在实际预测的时候是不会碰到[MASK]的，用了太多[MASK]就容易影响到模型。所以作者又用了个小技巧，选中了要mask的token后，其中10%的token会被替代成其他token，10%的token不替换，剩下的80%才被替换为[MASK]。</p>
<h3 id="Next-Sentence-Prediction"><a href="#Next-Sentence-Prediction" class="headerlink" title="Next Sentence Prediction"></a>Next Sentence Prediction</h3><p>由于nlp中存在需要输入两个句子的句子关系判断任务，所以需要增设一个让模型理解句子之间关系的任务，于是Next Sentence Prediction应运而生。具体而言，就是输入两个句子，由模型来判断这两个句子是不是连续的上下句。其中，为了保持样本平衡性，选了50%的连续的正样本，再随机选50%的无关的负样本。其实这个任务和seq2seq的任务有点异曲同工之妙，只是从单词级别变到了句子级别。举个例子：<br>正样本：今天[MASK]（天气）真好，正好我们[MASK]（出去）吃饭吧。<br>负样本：今天[MASK]（天气）真好，[MASK]（我）吃饱了。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>其实正如标题，BERT模型除了提出这两种训练方法外，大量的数据肯定对这个预训练模型有很强的作用，不过一般人没这种计算资源…所以还是很感谢谷歌开源出来的预训练模型，可以很方便的使用并达到非常好的效果。在使用时，如果需要用到自己的数据库上，要么就是完全自己写然后导入BERT模型，要么可以直接使用官方的代码，如run_glue.py，只需要修改数据的预处理，定义好新的类，然后指定类别数等参数，就可以直接使用了。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://www.cnblogs.com/rucwxb/p/10277217.html" target="_blank" rel="noopener">https://www.cnblogs.com/rucwxb/p/10277217.html</a><br><a href="https://zhuanlan.zhihu.com/p/46652512" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/46652512</a></p>

      
    </div>
    <div class="article-footer">
      <blockquote class="mt-2x">
  <ul class="post-copyright list-unstyled">
    
    <li class="post-copyright-link hidden-xs">
      <strong>本文链接：</strong>
      <a href="http://weiquanfan.xyz/2020/05/07/BERT/" title="暴力即正义——讲讲横扫nlp任务的BERT模型" target="_blank" rel="external">http://weiquanfan.xyz/2020/05/07/BERT/</a>
    </li>
    
    <li class="post-copyright-license">
      <strong>版权声明： </strong> 本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by/4.0/deed.zh" target="_blank" rel="external">CC BY 4.0 CN协议</a> 许可协议。转载请注明出处！
    </li>
  </ul>
</blockquote>


<div class="panel panel-default panel-badger">
  <div class="panel-body">
    <figure class="media">
      <div class="media-left">
        <a href="https://github.com/tobefans" target="_blank" class="img-burn thumb-sm visible-lg">
          <img src="/images/photo.jpg" class="img-rounded w-full" alt="">
        </a>
      </div>
      <div class="media-body">
        <h3 class="media-heading"><a href="https://github.com/tobefans" target="_blank"><span class="text-dark">Weiquan Fan</span><small class="ml-1x">AI练习生</small></a></h3>
        <div>陌上花开，可缓缓归矣</div>
      </div>
    </figure>
  </div>
</div>


    </div>
  </article>
  
    
  <section id="comments">
  	
      <div id="vcomments"></div>
    
  </section>


  
</div>

  <nav class="bar bar-footer clearfix" data-stick-bottom>
  <div class="bar-inner">
  
  <ul class="pager pull-left">
    
    
    <li class="next">
      <a href="/2020/05/05/transfomer/" title="Transfomer以及Self-Attention讲解"><span>Older&nbsp;&nbsp;</span><i class="icon icon-angle-right" aria-hidden="true"></i></a>
    </li>
    
    
  </ul>
  
  
  <!-- Button trigger modal -->
  <button type="button" class="btn btn-fancy btn-donate pop-onhover bg-gradient-warning" data-toggle="modal" data-target="#donateModal"><span>$</span></button>
  <!-- <div class="wave-icon wave-icon-danger btn-donate" data-toggle="modal" data-target="#donateModal">
    <div class="wave-circle"><span class="icon"><i class="icon icon-bill"></i></span></div>
  </div> -->
  
  
  <div class="bar-right">
    
    <div class="share-component" data-sites="weibo,qq,wechat,facebook,twitter" data-mobile-sites="weibo,qq,qzone"></div>
    
  </div>
  </div>
</nav>
  
<!-- Modal -->
<div class="modal modal-center modal-small modal-xs-full fade" id="donateModal" tabindex="-1" role="dialog">
  <div class="modal-dialog" role="document">
    <div class="modal-content donate">
      <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
      <div class="modal-body">
        <div class="donate-box">
          <div class="donate-head">
            <p>Maybe you could buy me a cup of coffee.</p>
          </div>
          <div class="tab-content">
            <div role="tabpanel" class="tab-pane fade active in" id="alipay">
              <div class="donate-payimg">
                <img src="/images/donate/alipay.png" alt="Scan Qrcode" title="Scan" />
              </div>
              <p class="text-muted mv">Scan this qrcode</p>
              <p class="text-grey">Open alipay app scan this qrcode, buy me a coffee!</p>
            </div>
            <div role="tabpanel" class="tab-pane fade" id="wechatpay">
              <div class="donate-payimg">
                <img src="/images/donate/wachat.png" alt="Scan Qrcode" title="Scan" />
              </div>
              <p class="text-muted mv">Scan this qrcode</p>
              <p class="text-grey">Open wechat app scan this qrcode, buy me a coffee!</p>
            </div>
          </div>
          <div class="donate-footer">
            <ul class="nav nav-tabs nav-justified" role="tablist">
              <li role="presentation" class="active">
                <a href="#alipay" id="alipay-tab" role="tab" data-toggle="tab" aria-controls="alipay" aria-expanded="true"><i class="icon icon-alipay"></i> alipay</a>
              </li>
              <li role="presentation" class="">
                <a href="#wechatpay" role="tab" id="wechatpay-tab" data-toggle="tab" aria-controls="wechatpay" aria-expanded="false"><i class="icon icon-wepay"></i> wechat payment</a>
              </li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>



</main>

  <footer class="footer" itemscope itemtype="http://schema.org/WPFooter">
	
	
    <ul class="social-links">
    	
        <li><a href="https://github.com/tobefans" target="_blank" title="Github" data-toggle=tooltip data-placement=top><i class="icon icon-github"></i></a></li>
        
        <li><a href="https://github.com/tobefans" target="_blank" title="Weibo" data-toggle=tooltip data-placement=top><i class="icon icon-weibo"></i></a></li>
        
        <li><a href="https://github.com/tobefans" target="_blank" title="Twitter" data-toggle=tooltip data-placement=top><i class="icon icon-twitter"></i></a></li>
        
        <li><a href="https://github.com/tobefans" target="_blank" title="Behance" data-toggle=tooltip data-placement=top><i class="icon icon-behance"></i></a></li>
        
        <li><a href="/atom.xml" target="_blank" title="Rss" data-toggle=tooltip data-placement=top><i class="icon icon-rss"></i></a></li>
        
    </ul>

    <div class="copyright">
    	
        <div class="publishby">
        	Theme by <a href="https://github.com/cofess" target="_blank"> cofess </a>base on <a href="https://github.com/cofess/hexo-theme-pure" target="_blank">pure</a>.
        </div>
    </div>
</footer>
  <script src="//cdn.jsdelivr.net/npm/jquery@1.12.4/dist/jquery.min.js"></script>
<script>
window.jQuery || document.write('<script src="js/jquery.min.js"><\/script>')
</script>
<script src="/js/plugin.min.js"></script>
<script src="/js/application.js"></script>

    <script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>
<script src="/js/insight.js"></script>





   


<!-- custom analytics part create by xiamo -->
<script defer src="https://cdn1.lncld.net/static/js/av-min-1.2.1.js"></script>
<script defer>
AV.init({
  appId: 'G55DkRGHWai3lkkyvBotwMTl-gzGzoHsz',
  appKey: 'F0aXAudrJr6wsdAvrVvSp1T2'
});

function showTime(Counter) {
	var query = new AV.Query(Counter);
		var visitors= $('.leancloud_visitors');
		query.greaterThanOrEqualTo("time", 0);		
		query.find({
			success: function(results) {
				if (results.length == 0) {				
					return;
				}
				var data = results;
				visitors.each(function(){
					var url = $(this).attr('id').trim();					
					for (var i = 0; i < data.length; i++) {
						var object = data[i];
						var content = object.get('time');
						var _url = object.get('url')
						if(url == _url){
							$(this).text(content);
						}
					}
				})
				
			},
			error: function(object, error) {
				console.log("Error: " + error.code + " " + error.message);
			}
		});
}

function addCount(Counter) {
	var Counter = AV.Object.extend("Counter");
	url = $(".leancloud_visitors").attr('id').trim();
	title = $(".leancloud_visitors").attr('data-flag-title').trim();
	var query = new AV.Query(Counter);
	query.equalTo("url", url);
	query.find({
		success: function(results) {
			if (results.length > 0) {
				var counter = results[0];
				counter.fetchWhenSave(true);
				counter.increment("time");
				counter.save(null, {
					success: function(counter) {
						var content = counter.get('time');
						$(document.getElementById(url)).text(content);
					},
					error: function(counter, error) {
						console.log('Failed to save Visitor num, with error message: ' + error.message);
					}
				});
			} else {
				var newcounter = new Counter();
				newcounter.set("title", title);
				newcounter.set("url", url);
				newcounter.set("time", 1);
				newcounter.save(null, {
					success: function(newcounter) {
					    console.log("newcounter.get('time')="+newcounter.get('time'));
						var content = newcounter.get('time');
						$(document.getElementById(url)).text(content);
					},
					error: function(newcounter, error) {
						console.log('Failed to create');
					}
				});
			}
		},
		error: function(error) {
			console.log('Error:' + error.code + " " + error.message);
		}
	});
}
$(function() {
	var Counter = AV.Object.extend("Counter");
	if ($('.leancloud_visitors').length == 1) {
		addCount(Counter);
	} else {
		showTime(Counter);
	}
}); 
</script>



   
    
  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/valine"></script>
  <script type="text/javascript">
  var GUEST = ['nick', 'mail', 'link'];
  var meta = 'nick,mail,link';
  meta = meta.split(',').filter(function(item) {
    return GUEST.indexOf(item) > -1;
  });
  new Valine({
    el: '#vcomments',
    verify: false,
    notify: false,
    appId: 'G55DkRGHWai3lkkyvBotwMTl-gzGzoHsz',
    appKey: 'F0aXAudrJr6wsdAvrVvSp1T2',
    placeholder: '此时一位路人路过...',
    avatar: 'mm',
    meta: meta,
    pageSize: '10' || 10,
    visitor: false
  });
  </script>

     







<script src="/js/hexo_resize_image.js"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>
</html>
<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[了解一下Faster RCNN]]></title>
    <url>%2F2020%2F11%2F19%2Ffasterrcnn%2F</url>
    <content type="text"><![CDATA[1. 前言Faster RCNN 由 论文提出，是继R-CNN和Fast RCNN之后的目标检测上的又一力作。R-CNN提出selective search(SS)来搜索region proposal(RP)；Fast RCNN指出不必对每个RP各自提CNN特征，可以对原图提好CNN特征，再将SS找到的RP映射到CNN特征层上；Faster RCNN则提出了RPN层，将特征提取，proposal提取，bounding box整合在了一个网络中，极大地提高了检测速度。 2. 框架与流程Faster RCNN的模型框架如图。 可以分为4个主要内容： Conv layers。作为一种CNN网络目标检测方法，Faster RCNN首先使用一组基础的conv+relu+pooling层提取image的feature maps。该feature maps被共享用于后续RPN层和全连接层。 Region Proposal Networks。RPN网络用于生成region proposals。该层通过softmax判断anchors属于positive或者negative，再利用bounding box regression修正anchors获得精确的proposals。 Roi Pooling。该层收集输入的feature maps和proposals，综合这些信息后提取proposal feature maps，送入后续全连接层判定目标类别。 Classification。利用proposal feature maps计算proposal的类别，同时再次bounding box regression获得检测框最终的精确位置。 完整的网络图如下。 该网络对于一副任意大小PxQ的图像，首先缩放至固定大小MxN，然后将MxN图像送入网络； Conv layers中包含了13个conv层+13个relu层+4个pooling层； RPN网络首先经过3x3卷积，再分别生成positive anchors和对应bounding box regression偏移量，然后计算出proposals； Roi Pooling层则利用proposals从feature maps中提取proposal feature送入后续全连接和softmax网络作classification。 3. 模型细节3.1 Region Proposal Networks(RPN)从网络总图上可以看出，RPN 层可以分为上下两条支路，上面一条通过softmax分类anchors获得positive和negative分类，下面一条用于计算对于anchors的bounding box regression偏移量，以获得精确的proposal。而最后的Proposal层则负责综合positive anchors和对应bounding box regression偏移量获取proposals，同时剔除太小和超出边界的proposals。 3.1.1 anchorsanchors 是一组预设好的矩形。对于缩放至800×600的图，作者预设了9个anchors，坐标如下。 [[ -84. -40. 99. 55.] [-176. -88. 191. 103.] [-360. -184. 375. 199.] [ -56. -56. 71. 71.] [-120. -120. 135. 135.] [-248. -248. 263. 263.] [ -36. -80. 51. 95.] [ -80. -168. 95. 183.] [-168. -344. 183. 359.]] 其中每行的4个值表示矩形的左上和右下角点坐标。这9个矩形的长宽比为0.5、1或2，anchors中长宽1:2中最大为352x704，长宽2:1中最大736x384，这样就可以基本覆盖到整张图。 有了这些anchors，我们遍历Conv layers计算获得的feature maps，为每一个点都配备这9种anchors作为初始的检测框。这里如果有超出图像边缘的框，我们就对框进行裁剪，丢弃掉框外的部分。那么总共就有 (800//16) (600//16) 9 = 17100个anchor。 3.1.2 softmax分类一副MxN大小的矩阵送入Faster RCNN网络后，到RPN网络变为(M/16)x(N/16)，设为W×H。 在进入reshape与softmax之前，先做了1x1卷积，输出18（即2×9）层feature maps. 9表示九种anchor，2表示该anchor是否含有目标。 这里，为了进行softmax输出二分类结果，需要单独把‘2’这个维度孤立出来，因此在softmax前后各有一个reshape。数据的尺寸变化为：[1, 2x9, H, W] -&gt; [1, 2, Hx9, W], softmax -&gt; [1, 2x9, H, W]. 3.1.3 bounding box regression对于窗口一般使用四维向量(x,y,w,h)表示，分别表示窗口的中心点坐标和宽高。对于positive Anchors(设为A)，和groundtruth(设为G’)，我们的目标是寻找一种关系，使得输入原始的anchor A经过映射得到一个跟真实窗口G更接近的回归窗口G’。比较简单的思路就是: 注意，这里的平移dx和dy可以理解为相对于原宽长的平移因子，即相对于原宽长平移了多少倍的距离。缩放dw和dh可以理解为缩放了ln的dw和dh倍。 那么，对应于Faster RCNN原文，positive anchor与ground truth之间的平移量(tx, ty)与尺度因子(tw, th)如下： 训练bouding box regression网络回归分支时，标签是(tx,ty,tw,th)。 输入cnn feature，输出36（即4×9）层feature maps. 9表示九种anchor，4表示该anchor的平移量和缩放量。注意这里的平移缩放量是针对原M×N的尺寸的，而输入的feature是pooling后的尺寸。 3.1.4 Proposal LayerVGG输出 5038512 的特征，对应设置 5038k个anchors，而RPN输出： 大小为 50382k 的positive/negative softmax分类特征矩阵 大小为 50384k 的regression坐标回归特征矩阵 Proposal Layer负责综合所有平移缩放量和positive anchors，计算出精准的proposal，送入后续RoI Pooling Layer。 Proposal Layer forward 按照以下顺序依次处理： 按照输入的positive softmax scores由大到小排序anchors，提取前pre_nms_topN(e.g. 6000)个anchors，即提取修正位置后的positive anchors 限定超出图像边界的positive anchors为图像边界，防止后续roi pooling时proposal超出图像边界 剔除尺寸非常小的positive anchors 对剩余的positive anchors进行NMS（nonmaximum suppression），去掉大量重复框 Proposal Layer有3个输入：anchors是否有目标的分类器结果rpn_cls_prob_reshape，对应的bbox坐标(e.g. 300)，包含了缩放信息的im_info=[M, N, scale_factor]。然后输出300个 proposal=[x1, y1, x2, y2]。 3.2 RoI pooling由于RPN层输出的proposal尺寸不一，故提出了RoI pooling变换到统一的尺寸。Rol pooling层有2个输入： 原始的feature maps RPN输出的proposal boxes（大小各不相同） RoI Pooling layer forward过程： 由于proposal是对应MXN尺度的，所以首先使用spatial_scale=1/16将其映射回(M/16)X(N/16)大小的feature map尺度； 再将每个proposal对应的feature map区域水平分为 pooled_w × pooled_h 的网格； 对网格的每一份都进行max pooling处理。 这样处理后，即使大小不同的proposal输出结果都是 pooled_w × pooled_h 固定大小，实现了固定长度输出。 3.3 ClassificationClassification环节，利用已经获得的proposal feature maps，通过full connect层与softmax计算每个proposal具体属于那个类别（如人，车，电视等），输出cls_prob概率向量；同时再次利用bounding box regression获得每个proposal的位置偏移量bbox_pred，用于回归更加精确的目标检测框。 4. Faster RCNN的训练Faster RCNN训练过程分为6个步骤： 在已经训练好的model上，训练RPN网络，对应stage1_rpn_train.pt 利用步骤1中训练好的RPN网络，收集proposals，对应rpn_test.pt 第一次训练Fast RCNN网络，对应stage1_fast_rcnn_train.pt 第二训练RPN网络，对应stage2_rpn_train.pt 再次利用步骤4中训练好的RPN网络，收集proposals，对应rpn_test.pt 第二次训练Fast RCNN网络，对应stage2_fast_rcnn_train.pt 可以看到训练过程类似于一种“迭代”的过程，不过只循环了2次。至于只循环了2次的原因是应为作者提到：”A similar alternating training can be run for more iterations, but we have observed negligible improvements”，即循环更多次没有提升了。注意，在第二次训练时，RPN和Fast RCNN共享的网络层是冻结的。 5. 总结Faster RCNN是目标检测里two-stage的代表性杰作，在这之后还有一款用于目标检测和实例分割的Mask RCNN也为人称道。Mask RCNN类似于Faster RCNN的两个输出（预测框的坐标和类别），但多一条基于特征金字塔FCN网络的实例分割的mask通路，另外还将RoI pooling换成RoI align解决量化带来的边缘像素损失问题。后来，目标检测又有很多one-stage方法涌现，即一步直接生成预测框的坐标和类别，其中以YOLO和SSD为代表，它们最终输出k×(4+1+c)通道的特征图，其中4是坐标，1是前景、背景的置信度，c是类别数，c是anchor数。两者选择anchor框的策略不同，YOLO的anchor基于训练集所有框聚类得到宽和长，SSD由数学公式得到，且SSD在不同尺度的特征图上选取了不同的anchor数量(从而实现多尺度的检测)。两者的anchor框都只有宽度和高度，坐标x和y都默认在网格中心。 6. 参考文献https://zhuanlan.zhihu.com/p/31426458]]></content>
  </entry>
  <entry>
    <title><![CDATA[使用端点检测和百度语音识别技术实现视频的字幕生成]]></title>
    <url>%2F2020%2F06%2F26%2Fgen-srt%2F</url>
    <content type="text"><![CDATA[前言字幕文件中包含很多段信息，每一段表示了一句话的起始结束时间和内容，因此便涉及到了端点检测技术和语音识别技术。 端点检测：pydub.silence.detect_nonsilent 语音识别：aip.AipSpeech（百度接口）pip install pydub pip install baidu-aip 流程 视频提取音频 对音频进行端点检测，生成一句一句的音频 对各句音频进行语音识别 整合成字幕srt格式 代码#!/usr/bin/env python3 # -*- coding: utf-8 -*- from moviepy.editor import * from pydub import * from aip import AipSpeech video_file = r&#39;C:\Users\Lenovo\Desktop\video_sep\test.mp4&#39; audio_file = r&#39;C:\Users\Lenovo\Desktop\test.wav&#39; srt_file = r&#39;C:\Users\Lenovo\Desktop\srt\test.srt&#39; ## transform to audio video = VideoFileClip(video_file) video.audio.write_audiofile(audio_file, ffmpeg_params=[&#39;-ar&#39;,&#39;16000&#39;,&#39;-ac&#39;,&#39;1&#39;]) ## segment sound = AudioSegment.from_wav(audio_file) timestamp_list = silence.detect_nonsilent(sound, 700, sound.dBFS*1.3, 1) # look here for i in range(len(timestamp_list)): d = timestamp_list[i][1] - timestamp_list[i][0] print(&quot;Section is :&quot;, timestamp_list[i], &quot;duration is:&quot;, d) print(&#39;dBFS: {0}, max_dBFS: {1}, duration: {2}, split: {3}&#39;.format(round(sound.dBFS,2),round(sound.max_dBFS,2),sound.duration_seconds,len(timestamp_list))) def format_time(ms): hours = ms // 3600000 ms = ms % 3600000 minutes = ms // 60000 ms = ms % 60000 seconds = ms // 1000 mseconds = ms % 1000 return &#39;{:0&gt;2d}:{:0&gt;2d}:{:0&gt;2d},{:0&gt;3d}&#39;.format(hours, minutes, seconds, mseconds) ## 以下在百度AI开放平台申请获得 ## https://ai.baidu.com/tech/speech APP_ID = &#39;&#39; API_KEY = &#39;&#39; SECRET_KEY = &#39;&#39; client = AipSpeech(APP_ID, API_KEY, SECRET_KEY) idx = 0 text = [] for i in range(len(timestamp_list)): d = timestamp_list[i][1] - timestamp_list[i][0] data = sound[timestamp_list[i][0]:timestamp_list[i][1]].raw_data ## asr result = client.asr(data, &#39;pcm&#39;, 16000, {&#39;lan&#39;: &#39;zh&#39;,}) ## and look here if result[&#39;err_no&#39;] == 0: text.append(&#39;{0}\n{1} --&gt; {2}\n&#39;.format(idx, format_time(timestamp_list[i][0]), format_time(timestamp_list[i][1]))) text.append( result[&#39;result&#39;][0]) #.replace(&quot;，&quot;, &quot;&quot;) text.append(&#39;\n&#39;) idx = idx + 1 # print(format_time(timestamp_list[i][0]/ 1000), &quot;txt is &quot;, result[&#39;result&#39;][0]) with open(srt_file,&quot;w&quot;) as f: f.writelines(text) 字幕生成的其他方式通过双门限法进行端点检测双门限法的原理是浊音的能量高于清音，清音的过零率高于无声部分。因此，其核心在于：先利用能量，将浊音部分区分出来，再利用过零率，将清音也提取出来，就完成了端点检测。 通过 SpeechRcognition 进行语音识别SpeechRcognition 可以说是一款语音识别集合器，共包含了谷歌、必应、IBM等七个识别器： recognize_bing()：Microsoft Bing Speech recognize_google()： Google Web Speech API recognize_google_cloud()：Google Cloud Speech - requires installation of the google-cloud-speech package recognize_houndify()： Houndify by SoundHound recognize_ibm()：IBM Speech to Text recognize_sphinx()：CMU Sphinx - requires installing PocketSphinx recognize_wit()：Wit.ai 基本使用方法如下： import speech_recognition as sr r = sr.Recognizer() test = sr.AudioFile(r&#39;C:\Users\Lenovo\Desktop\test.wav&#39;) with test as source: audio = r.record(source) r.recognize_google(audio, language=&#39;zh-CN&#39;, show_all= True) 但好像需要翻墙才能用… 通过autosub包直接生成字幕文件autosub是一个直接可以生成字幕文件的python库，详细可看中文教程.html)基本用法如下： autosub -S zh-CN -D zh-CN [你的视频/音频文件名] 不过这种方法也需要翻墙，我尝试了更改proxy也没什么效果… 总结总体而言，字幕生成需要的两个技术块，各有多种实现方法，而我最终选取的pydub加baidu-aip是相对简单并且有效的一种。不过实测效果并没有达到我的期望，因为一开始端点检测就不是十分准确，导致在错误的句子里上下文关系也不太对，语音识别也会有偏差了。更进一步的端点检测方法还得综合考虑能量和过零率，最好还要自定义地加上各个句子长度不能相差太大的限制等等。]]></content>
      <tags>
        <tag>端点检测</tag>
        <tag>语音识别</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于keras的简易语音识别]]></title>
    <url>%2F2020%2F06%2F20%2FASR-with-keras%2F</url>
    <content type="text"><![CDATA[简介最近忽然看到不是基于kaldi的ASR代码，尝试了一下发现效果还不错，搬上来记录一下。 源码地址： https://pan.baidu.com/s/1tFlZkMJmrMTD05cd_zxmAg 提取码：ndrr 数据集需要自行下载。 数据集数据集使用的是清华大学的thchs30中文数据，data文件夹中包含（.wav文件和.trn文件；trn文件里存放的是.wav文件的文字描述:第一行为词，第二行为拼音，第三行为音素）. 模型预测先直接解释有了训好的模型后如何使用，代码如下： # -*- coding: utf-8 -*- from keras.models import load_model from keras import backend as K import numpy as np import librosa from python_speech_features import mfcc import pickle import glob wavs = glob.glob(&#39;A2_8.wav&#39;) print(wavs) with open(&#39;dictionary.pkl&#39;, &#39;rb&#39;) as fr: [char2id, id2char, mfcc_mean, mfcc_std] = pickle.load(fr) mfcc_dim = 13 model = load_model(&#39;asr.h5&#39;) index = np.random.randint(len(wavs)) print(wavs[index]) ## 读取数据，并去除掉没说话的起始结束时间 audio, sr = librosa.load(wavs[index]) energy = librosa.feature.rmse(audio) frames = np.nonzero(energy &gt;= np.max(energy) / 5) indices = librosa.core.frames_to_samples(frames)[1] audio = audio[indices[0]:indices[-1]] if indices.size else audio[0:0] X_data = mfcc(audio, sr, numcep=mfcc_dim, nfft=551) X_data = (X_data - mfcc_mean) / (mfcc_std + 1e-14) print(X_data.shape) pred = model.predict(np.expand_dims(X_data, axis=0)) pred_ids = K.eval(K.ctc_decode(pred, [X_data.shape[0]], greedy=False, beam_width=10, top_paths=1)[0][0]) pred_ids = pred_ids.flatten().tolist() print(&#39;&#39;.join([id2char[i] for i in pred_ids])) 模型训练模型采用了 TDNN 网络结构，并直接通过字符级别来预测，直接根据常见度将字符对应成数字标签。整个流程而言， 先将一个个语音样本变成MFCC特征，即一个样本的维度为time*num_MFCC，time维度将被补齐到batch里最长的time。 将批量样本送入网络，采用1d卷积，仅在时间轴上卷积，一个样本的输出维度为time*(num_words+1)，加的1代表预测了空状态。 通过CTC Loss计算损失 # -*- coding: utf-8 -*- #导入相关的库 from keras.models import Model from keras.layers import Input, Activation, Conv1D, Lambda, Add, Multiply, BatchNormalization from keras.optimizers import Adam, SGD from keras import backend as K from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau import numpy as np import matplotlib.pyplot as plt from mpl_toolkits.axes_grid1 import make_axes_locatable import random import pickle import glob from tqdm import tqdm import os from python_speech_features import mfcc import scipy.io.wavfile as wav import librosa from IPython.display import Audio #读取数据集文件 text_paths = glob.glob(&#39;data/*.trn&#39;) total = len(text_paths) print(total) with open(text_paths[0], &#39;r&#39;, encoding=&#39;utf8&#39;) as fr: lines = fr.readlines() print(lines) #数据集文件trn内容读取保存到数组中 texts = [] paths = [] for path in text_paths: with open(path, &#39;r&#39;, encoding=&#39;utf8&#39;) as fr: lines = fr.readlines() line = lines[0].strip(&#39;\n&#39;).replace(&#39; &#39;, &#39;&#39;) texts.append(line) paths.append(path.rstrip(&#39;.trn&#39;)) print(paths[0], texts[0]) #定义mfcc数 mfcc_dim = 13 #根据数据集标定的音素读入 def load_and_trim(path): audio, sr = librosa.load(path) energy = librosa.feature.rmse(audio) frames = np.nonzero(energy &gt;= np.max(energy) / 5) indices = librosa.core.frames_to_samples(frames)[1] audio = audio[indices[0]:indices[-1]] if indices.size else audio[0:0] return audio, sr #可视化，显示语音文件的MFCC图 def visualize(index): path = paths[index] text = texts[index] print(&#39;Audio Text:&#39;, text) audio, sr = load_and_trim(path) plt.figure(figsize=(12, 3)) plt.plot(np.arange(len(audio)), audio) plt.title(&#39;Raw Audio Signal&#39;) plt.xlabel(&#39;Time&#39;) plt.ylabel(&#39;Audio Amplitude&#39;) plt.show() feature = mfcc(audio, sr, numcep=mfcc_dim, nfft=551) print(&#39;Shape of MFCC:&#39;, feature.shape) fig = plt.figure(figsize=(12, 5)) ax = fig.add_subplot(111) im = ax.imshow(feature, cmap=plt.cm.jet, aspect=&#39;auto&#39;) plt.title(&#39;Normalized MFCC&#39;) plt.ylabel(&#39;Time&#39;) plt.xlabel(&#39;MFCC Coefficient&#39;) plt.colorbar(im, cax=make_axes_locatable(ax).append_axes(&#39;right&#39;, size=&#39;5%&#39;, pad=0.05)) ax.set_xticks(np.arange(0, 13, 2), minor=False); plt.show() return path Audio(visualize(0)) #提取音频特征并存储 features = [] for i in tqdm(range(total)): path = paths[i] audio, sr = load_and_trim(path) features.append(mfcc(audio, sr, numcep=mfcc_dim, nfft=551)) print(len(features), features[0].shape) #随机选择100个数据集 samples = random.sample(features, 100) samples = np.vstack(samples) #平均MFCC的值为了归一化处理 mfcc_mean = np.mean(samples, axis=0) #计算标准差为了归一化 mfcc_std = np.std(samples, axis=0) print(mfcc_mean) print(mfcc_std) #归一化特征 features = [(feature - mfcc_mean) / (mfcc_std + 1e-14) for feature in features] #将数据集读入的标签和对应id存储列表 chars = {} for text in texts: for c in text: chars[c] = chars.get(c, 0) + 1 chars = sorted(chars.items(), key=lambda x: x[1], reverse=True) chars = [char[0] for char in chars] print(len(chars), chars[:100]) char2id = {c: i for i, c in enumerate(chars)} id2char = {i: c for i, c in enumerate(chars)} data_index = np.arange(total) np.random.shuffle(data_index) train_size = int(0.9 * total) test_size = total - train_size train_index = data_index[:train_size] test_index = data_index[train_size:] #神经网络输入和输出X,Y的读入数据集特征 X_train = [features[i] for i in train_index] Y_train = [texts[i] for i in train_index] X_test = [features[i] for i in test_index] Y_test = [texts[i] for i in test_index] batch_size = 16 #定义训练批次的产生，一次训练16个 def batch_generator(x, y, batch_size=batch_size): offset = 0 while True: offset += batch_size if offset == batch_size or offset &gt;= len(x): data_index = np.arange(len(x)) np.random.shuffle(data_index) x = [x[i] for i in data_index] y = [y[i] for i in data_index] offset = batch_size X_data = x[offset - batch_size: offset] Y_data = y[offset - batch_size: offset] X_maxlen = max([X_data[i].shape[0] for i in range(batch_size)]) Y_maxlen = max([len(Y_data[i]) for i in range(batch_size)]) X_batch = np.zeros([batch_size, X_maxlen, mfcc_dim]) Y_batch = np.ones([batch_size, Y_maxlen]) * len(char2id) X_length = np.zeros([batch_size, 1], dtype=&#39;int32&#39;) Y_length = np.zeros([batch_size, 1], dtype=&#39;int32&#39;) for i in range(batch_size): X_length[i, 0] = X_data[i].shape[0] X_batch[i, :X_length[i, 0], :] = X_data[i] Y_length[i, 0] = len(Y_data[i]) Y_batch[i, :Y_length[i, 0]] = [char2id[c] for c in Y_data[i]] inputs = {&#39;X&#39;: X_batch, &#39;Y&#39;: Y_batch, &#39;X_length&#39;: X_length, &#39;Y_length&#39;: Y_length} outputs = {&#39;ctc&#39;: np.zeros([batch_size])} yield (inputs, outputs) epochs = 50 num_blocks = 3 filters = 128 X = Input(shape=(None, mfcc_dim,), dtype=&#39;float32&#39;, name=&#39;X&#39;) Y = Input(shape=(None,), dtype=&#39;float32&#39;, name=&#39;Y&#39;) X_length = Input(shape=(1,), dtype=&#39;int32&#39;, name=&#39;X_length&#39;) Y_length = Input(shape=(1,), dtype=&#39;int32&#39;, name=&#39;Y_length&#39;) #卷积1层 # 一维卷积，默认channels_last，即通道维(MFCC特征维)放最后，对时间维进行卷积 def conv1d(inputs, filters, kernel_size, dilation_rate): return Conv1D(filters=filters, kernel_size=kernel_size, strides=1, padding=&#39;causal&#39;, activation=None, dilation_rate=dilation_rate)(inputs) #标准化函数 def batchnorm(inputs): return BatchNormalization()(inputs) #激活层函数 def activation(inputs, activation): return Activation(activation)(inputs) #全连接层函数 def res_block(inputs, filters, kernel_size, dilation_rate): hf = activation(batchnorm(conv1d(inputs, filters, kernel_size, dilation_rate)), &#39;tanh&#39;) hg = activation(batchnorm(conv1d(inputs, filters, kernel_size, dilation_rate)), &#39;sigmoid&#39;) h0 = Multiply()([hf, hg]) ha = activation(batchnorm(conv1d(h0, filters, 1, 1)), &#39;tanh&#39;) hs = activation(batchnorm(conv1d(h0, filters, 1, 1)), &#39;tanh&#39;) return Add()([ha, inputs]), hs h0 = activation(batchnorm(conv1d(X, filters, 1, 1)), &#39;tanh&#39;) shortcut = [] for i in range(num_blocks): for r in [1, 2, 4, 8, 16]: h0, s = res_block(h0, filters, 7, r) shortcut.append(s) h1 = activation(Add()(shortcut), &#39;relu&#39;) h1 = activation(batchnorm(conv1d(h1, filters, 1, 1)), &#39;relu&#39;) #softmax损失函数输出结果 Y_pred = activation(batchnorm(conv1d(h1, len(char2id) + 1, 1, 1)), &#39;softmax&#39;) sub_model = Model(inputs=X, outputs=Y_pred) #计算损失函数 def calc_ctc_loss(args): y, yp, ypl, yl = args return K.ctc_batch_cost(y, yp, ypl, yl) ctc_loss = Lambda(calc_ctc_loss, output_shape=(1,), name=&#39;ctc&#39;)([Y, Y_pred, X_length, Y_length]) #加载模型训练 model = Model(inputs=[X, Y, X_length, Y_length], outputs=ctc_loss) #建立优化器 optimizer = SGD(lr=0.02, momentum=0.9, nesterov=True, clipnorm=5) #激活模型开始计算 model.compile(loss={&#39;ctc&#39;: lambda ctc_true, ctc_pred: ctc_pred}, optimizer=optimizer) checkpointer = ModelCheckpoint(filepath=&#39;asr.h5&#39;, verbose=0) lr_decay = ReduceLROnPlateau(monitor=&#39;loss&#39;, factor=0.2, patience=1, min_lr=0.000) #开始训练 history = model.fit_generator( generator=batch_generator(X_train, Y_train), steps_per_epoch=len(X_train) // batch_size, epochs=epochs, validation_data=batch_generator(X_test, Y_test), validation_steps=len(X_test) // batch_size, callbacks=[checkpointer, lr_decay]) #保存模型 sub_model.save(&#39;asr.h5&#39;) #将字保存在pl=pkl中 with open(&#39;dictionary.pkl&#39;, &#39;wb&#39;) as fw: pickle.dump([char2id, id2char, mfcc_mean, mfcc_std], fw) train_loss = history.history[&#39;loss&#39;] plt.plot(np.linspace(1, epochs, epochs), train_loss, label=&#39;train&#39;) plt.legend(loc=&#39;upper right&#39;) plt.xlabel(&#39;Epoch&#39;) plt.ylabel(&#39;Loss&#39;) plt.show() #下面是模型的预测效果，可见main.py from keras.models import load_model import pickle with open(&#39;dictionary.pkl&#39;, &#39;rb&#39;) as fr: [char2id, id2char, mfcc_mean, mfcc_std] = pickle.load(fr) sub_model = load_model(&#39;asr.h5&#39;) def random_predict(x, y): index = np.random.randint(len(x)) feature = x[index] text = y[index] pred = sub_model.predict(np.expand_dims(feature, axis=0)) pred_ids = K.eval(K.ctc_decode(pred, [feature.shape[0]], greedy=False, beam_width=10, top_paths=1)[0][0]) pred_ids = pred_ids.flatten().tolist() print(&#39;True transcription:\n-- &#39;, text, &#39;\n&#39;) print(&#39;Predicted transcription:\n-- &#39; + &#39;&#39;.join([id2char[i] for i in pred_ids]), &#39;\n&#39;) random_predict(X_train, Y_train) random_predict(X_test, Y_test) 总结对比其他的分类任务，语音识别多了个解码过程，这也导致了目前在常见的深度学习框架中还没有很好的ASR框架，目前而言，CTC的应用也导致出现了一些完全端到端的ASR系统，相信以后这也会是个大趋势。]]></content>
      <categories>
        <category>语音识别</category>
      </categories>
      <tags>
        <tag>语音识别</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用QT designer、python搭建界面程序]]></title>
    <url>%2F2020%2F06%2F10%2FQT%2F</url>
    <content type="text"><![CDATA[前言PyQt 是Python语言的GUI编程解决方案之一，是类似于 Tkinter 的一个高级库。 为了更好的辅助PyQt界面的搭建，可以通过Qt Designer完成GUI界面设计。 使用Qt Designer可以通过拖拽、点击完成GUI界面设计，并且设计完成后生成的.ui程序可以通过 pyuic5 命令直接转换成.py文件以供python程序调用。 搭建完界面并写好逻辑后，还可通过 pyinstaller 将.py文件封装成.exe文件，以供没有python解释器的用户使用。 本文以搭建标注工具界面程序为例。 预安装的软件与库 Qt Designer: pip install —pre pyqt5-tools~=5.11（位于\Python36\Lib\site-packages\pyqt5_tools\designer.exe，也可通过这里下载） PyQt5: pip install PyQt5 pip install pyinstaller Qt Designer 的界面设计 Qt Designer 的界面主要分为四大区：项目区、控件区、编辑区、属性区。 具体而言，就是在【控件区】里点击添加需要的控件，这些控件的效果会在【编辑区】里实时显示，并在【属性区】这些控件的属性，【项目区】用于显示控件间的层级关系。 在新建一个窗口后，一般需要通过 Container 确定外部轮廓，可选用常见的 Frame 控件，再在 Frame 里边选用 Layouts 来规范后续控件的排列样式，常用水平或垂直排列，最后再选用具体部件往里边填充。 常用的控件有各种Button（按钮）、Label（静态显示文本框）、Text Edit（输入输出文本框）、listWidget（列表显示框）、Check Box（选中框）、各种Slider（滑动条）等。 每一个组件都有可设置的属性，最重要的通用属性有 objectName （用于在后续逻辑编写时指明时哪个控件），text （用于在GUI里在控件上显示）， geometry （用于设置控件位置和尺寸，但控件位于Layer中时就不可设置了）。 设计好界面之后保存可以生成my_win.ui文件，它可以直接在python代码里被加载使用，但为了在代码里进一步调用修改等，更好的方法是将.ui文件转换成相应的.py文件。这需要借助 \Python36\Scripts\pyuic5.exe工具。 pyuic5 -o my_win.py my_win.ui Qt 逻辑编写很多控件都可以通过点击（或其他操作）触发事件，事件响应可自由编写，通过 connect 函数绑定。 #!/usr/bin/env python3 # -*- coding: utf-8 -*- &quot;&quot;&quot; Created on Sun June 12 12:06:21 2020 @author: weiquan fan &quot;&quot;&quot; import sys, os from PyQt5.QtWidgets import QApplication, QMainWindow, QMessageBox, QCompleter, QFileDialog from PyQt5.QtMultimedia import QMediaContent, QMediaPlayer from PyQt5.QtCore import pyqtSignal, QEvent from PyQt5.Qt import QUrl from my_win import Ui_MainWindow import csv root_path_metadata = &quot;./data/&quot; if not os.path.exists(root_path_metadata): os.makedirs(root_path_metadata) class mainWin(QMainWindow, Ui_MainWindow): doubleClicked_speaker = pyqtSignal() doubleClicked_dialog = pyqtSignal() def __init__(self, parent=None): super(mainWin, self).__init__(parent) self.setupUi(self) ## emotion self.refresh_1() self.radioButton.clicked.connect(self.showPos) self.radioButton_2.clicked.connect(self.showNeu) self.radioButton_3.clicked.connect(self.showNeg) ## DA self.refresh_2() ## dialog identity self.refresh_3() self.frame_9.setHidden(True) self.checkBox_5.stateChanged.connect(self.use_subsysdem3) self.radioButton_5.clicked.connect(self.personA) self.radioButton_4.clicked.connect(self.personB) ## save buttons self.refresh_save() self.btn_save.clicked.connect(self.save_data) self.btn_save_2.clicked.connect(self.save_dialog_data) ## history self.list_speaker = [] self.list_dialog = [] self.lineEdit_speaker.installEventFilter(self) self.lineEdit.installEventFilter(self) self.doubleClicked_speaker.connect(self.completer_name_speaker) self.doubleClicked_dialog.connect(self.completer_name_dialog) ## video player self.player = QMediaPlayer() self.player.setVideoOutput(self.wgt_player) self.btn_open.clicked.connect(self.openVideoFile) self.btn_play_pause.clicked.connect(self.playPause) self.player.durationChanged.connect(self.getDuration) self.player.positionChanged.connect(self.getPosition) self.sld_duration.sliderMoved.connect(self.updatePosition) ## for opening video def openVideoFile(self): name = QFileDialog.getOpenFileName()[0] self.lineEdit.setText(name.split(&#39;/&#39;)[-1]) self.player.setMedia(QMediaContent(QUrl.fromLocalFile(name))) # self.player.setMedia(QMediaContent(QFileDialog.getOpenFileUrl()[0])) self.player.play() def playPause(self): if self.player.state()==1: self.player.pause() else: self.player.play() def getDuration(self, d): self.sld_duration.setRange(0, d) self.sld_duration.setEnabled(True) self.displayTime(d) def getPosition(self, p): self.sld_duration.setValue(p) self.displayTime(p) def displayTime(self, ms): minutes = int(ms/60000) seconds = int((ms-minutes*60000)/1000) dur_ms = self.sld_duration.maximum() dur_min = int(dur_ms/60000) dur_sec = int((dur_ms-dur_min*60000)/1000) self.lab_duration.setText(&#39;{:0&gt;2d}:{:0&gt;2d} / {:0&gt;2d}:{:0&gt;2d}&#39;.format(minutes, seconds, dur_min, dur_sec)) def updatePosition(self, v): self.player.setPosition(v) self.displayTime(self.sld_duration.maximum()-v) ## for history def eventFilter(self, widget, event): if widget == self.lineEdit_speaker: if event.type() == QEvent.MouseButtonDblClick: self.doubleClicked_speaker.emit() elif widget == self.lineEdit: if event.type() == QEvent.MouseButtonDblClick: self.doubleClicked_dialog.emit() return super().eventFilter(widget, event) def completer_name_dialog(self): self.completer = QCompleter(self.list_dialog) self.lineEdit.setCompleter(self.completer) self.completer.setCompletionMode(QCompleter.UnfilteredPopupCompletion) self.completer.complete() self.completer.popup() def completer_name_speaker(self): self.completer = QCompleter(self.list_speaker) self.lineEdit_speaker.setCompleter(self.completer) self.completer.setCompletionMode(QCompleter.UnfilteredPopupCompletion) self.completer.complete() self.completer.popup() ## for label 1 def showPos(self): self.listWidget.clear() self.listWidget.addItem(&quot;高兴&quot;) self.listWidget.addItem(&quot;兴奋&quot;) self.listWidget.addItem(&quot;自豪&quot;) self.listWidget.addItem(&quot;满足&quot;) self.listWidget.addItem(&quot;感激&quot;) self.listWidget.addItem(&quot;自信&quot;) self.listWidget.addItem(&quot;轻松&quot;) self.listWidget.addItem(&quot;羡慕&quot;) def showNeg(self): self.listWidget.clear() self.listWidget.addItem(&quot;生气&quot;) self.listWidget.addItem(&quot;伤心&quot;) self.listWidget.addItem(&quot;害怕&quot;) self.listWidget.addItem(&quot;烦恼&quot;) self.listWidget.addItem(&quot;孤独&quot;) self.listWidget.addItem(&quot;羞愧&quot;) self.listWidget.addItem(&quot;恶心&quot;) self.listWidget.addItem(&quot;失望&quot;) self.listWidget.addItem(&quot;郁闷&quot;) self.listWidget.addItem(&quot;不安&quot;) self.listWidget.addItem(&quot;紧张&quot;) self.listWidget.addItem(&quot;无奈&quot;) self.listWidget.addItem(&quot;纠结&quot;) def showNeu(self): self.listWidget.clear() self.listWidget.addItem(&quot;共情&quot;) self.listWidget.addItem(&quot;平静&quot;) ## for label 3 def use_subsysdem3(self): if self.checkBox_5.isChecked(): self.frame_9.setHidden(False) else: self.refresh_3() self.frame_9.setHidden(True) def personA(self): self.frame_3.setHidden(False) self.frame_8.setHidden(True) def personB(self): self.frame_3.setHidden(True) self.frame_8.setHidden(False) def refresh_gui(self): self.refresh_1() self.refresh_2() self.refresh_3() self.refresh_save() def refresh_1(self): self.buttonGroup_2.setExclusive(False) self.radioButton.setChecked(False) self.radioButton_2.setChecked(False) self.radioButton_3.setChecked(False) self.buttonGroup_2.setExclusive(True) self.listWidget.clear() self.checkBox_3.setChecked(True) self.checkBox_2.setChecked(True) self.checkBox.setChecked(True) self.checkBox_4.setChecked(False) def refresh_2(self): self.listWidget_2.clear() self.listWidget_2.addItem(&quot;问候&quot;) self.listWidget_2.addItem(&quot;提问&quot;) self.listWidget_2.addItem(&quot;回答&quot;) self.listWidget_2.addItem(&quot;陈述观点&quot;) self.listWidget_2.addItem(&quot;陈述非观点&quot;) self.listWidget_2.addItem(&quot;道歉&quot;) self.listWidget_2.addItem(&quot;命令&quot;) self.listWidget_2.addItem(&quot;赞同&quot;) self.listWidget_2.addItem(&quot;反对&quot;) self.listWidget_2.addItem(&quot;表达知会&quot;) self.listWidget_2.addItem(&quot;欣赏&quot;) self.listWidget_2.addItem(&quot;叹词&quot;) self.listWidget_2.addItem(&quot;结束对话&quot;) self.listWidget_2.addItem(&quot;引用&quot;) self.listWidget_2.addItem(&quot;其他&quot;) def refresh_3(self): # self.checkBox_5.setChecked(False) self.buttonGroup.setExclusive(False) self.radioButton_4.setChecked(False) self.radioButton_5.setChecked(False) self.buttonGroup.setExclusive(True) self.buttonGroup_3.setExclusive(False) self.radioButton_6.setChecked(False) self.radioButton_7.setChecked(False) self.radioButton_8.setChecked(False) self.radioButton_9.setChecked(False) self.radioButton_10.setChecked(False) self.buttonGroup_3.setExclusive(True) # self.frame_9.setHidden(True) self.frame_3.setHidden(True) self.frame_8.setHidden(True) def refresh_save(self): self.lineEdit_2.setText(&#39;0&#39;) self.lineEdit_3.setText(&#39;0&#39;) self.lineEdit_4.setText(&#39;0&#39;) self.lineEdit_5.setText(&#39;0&#39;) self.lineEdit_6.setText(&#39;0&#39;) self.lineEdit_7.setText(&#39;0&#39;) self.lineEdit_speaker.setText(&#39;&#39;) def save_data(self): ## check many things try: self.label_val = self.buttonGroup_2.checkedButton().text() self.label_emotion = self.listWidget.selectedItems()[0].text() except: QMessageBox.information(self,&#39;提示&#39;,&#39;请选择具体情感后再重新保存&#39;, QMessageBox.Yes) return False try: self.label_da = self.listWidget_2.selectedItems()[0].text() except: QMessageBox.information(self,&#39;提示&#39;,&#39;请选择对话状态后再重新保存&#39;, QMessageBox.Yes) return False self.label_iden_isok = self.checkBox_5.isChecked() if self.label_iden_isok: if self.buttonGroup.checkedId() == -1: QMessageBox.information(self,&#39;提示&#39;,&#39;您已勾选该对话身份可标，请选择说话人身份后再重新保存&#39;, QMessageBox.Yes) return False else: self.label_iden = self.buttonGroup.checkedButton().text() if self.label_iden == &quot;倾诉者&quot;: self.label_reason = self.lineEdit_reason.text() self.label_result = self.lineEdit_result.text() self.label_reaction = &quot;空&quot; else: self.label_reason = &quot;空&quot; self.label_result = &quot;空&quot; try: self.label_reaction = self.buttonGroup_3.checkedButton().text() except: QMessageBox.information(self,&#39;提示&#39;,&#39;您已勾选该对话身份可标，请选择倾诉者反应后再重新保存&#39;, QMessageBox.Yes) return False else: self.label_iden = &quot;不可标&quot; self.label_reason = &quot;不可标&quot; self.label_result = &quot;不可标&quot; self.label_reaction = &quot;不可标&quot; if self.lineEdit_speaker.text() == &#39;&#39;: QMessageBox.information(self,&#39;提示&#39;,&#39;请输入说话人姓名&#39;, QMessageBox.Yes) return False else: self.name_speaker = self.lineEdit_speaker.text() try: self.start_time = &quot;{}:{}:{}&quot;.format(int(self.lineEdit_2.text()), int(self.lineEdit_3.text()), int(self.lineEdit_4.text())) self.end_time = &quot;{}:{}:{}&quot;.format(int(self.lineEdit_5.text()), int(self.lineEdit_6.text()), int(self.lineEdit_7.text())) except: QMessageBox.information(self,&#39;提示&#39;,&#39;时间应输入整数&#39;, QMessageBox.Yes) return False if self.lineEdit.text() == &#39;&#39;: QMessageBox.information(self,&#39;提示&#39;,&#39;请输入视频名字&#39;, QMessageBox.Yes) return False else: self.name_dialog = self.lineEdit.text() if not os.path.exists(root_path_metadata+self.name_dialog+&#39;.csv&#39;): with open(root_path_metadata+self.name_dialog+&#39;.csv&#39;,&quot;a&quot;,newline=&#39;&#39;,encoding=&#39;utf_8_sig&#39;) as csvfile: writer = csv.writer(csvfile, delimiter=&#39;,&#39;) writer.writerow([&#39;视频名字&#39;, &#39;说话者姓名&#39;, &#39;起始时间&#39;, &#39;结束时间&#39;, &#39;情绪（粗粒度）&#39;, &#39;情绪（细粒度）&#39;, &#39;是否基于音频&#39;, &#39;是否基于视频&#39;, &#39;是否基于文本&#39;, &#39;是否难以标注&#39;, &#39;对话状态&#39;, &#39;是否可标对话身份&#39;, &#39;说话人身份&#39;, &#39;起因&#39;, &#39;结果&#39;, &#39;倾诉者反应&#39;]) ## save self.label_emotion_audio_based = self.checkBox_3.isChecked() self.label_emotion_video_based = self.checkBox_2.isChecked() self.label_emotion_text_based = self.checkBox.isChecked() self.label_emotion_hard = self.checkBox_4.isChecked() onelist = [self.name_dialog, self.name_speaker, self.start_time, self.end_time, self.label_val, self.label_emotion, self.label_emotion_audio_based, self.label_emotion_video_based, self.label_emotion_text_based, self.label_emotion_hard, self.label_da, self.label_iden_isok, self.label_iden, self.label_reason, self.label_result, self.label_reaction] with open(root_path_metadata+self.name_dialog+&#39;.csv&#39;,&quot;a&quot;,newline=&#39;&#39;,encoding=&#39;utf_8_sig&#39;) as csvfile: writer = csv.writer(csvfile, delimiter=&#39;,&#39;) writer.writerow(onelist) self.refresh_gui() self.list_dialog.append(self.name_dialog) self.list_speaker.append(self.name_speaker) self.list_dialog = list(set(self.list_dialog)) self.list_speaker = list(set(self.list_speaker)) # self.lineEdit.setCompleter(QCompleter(self.list_dialog)) # self.lineEdit_speaker.setCompleter(QCompleter(self.list_speaker)) return True def save_dialog_data(self): flag_save_success = self.save_data() if flag_save_success == False: return 0 QMessageBox.about(self,&#39;提示&#39;,&#39;对话保存成功&#39;) self.refresh_gui() self.lineEdit.setText(&#39;&#39;) self.lineEdit_reason.setText(&#39;&#39;) self.lineEdit_result.setText(&#39;&#39;) self.checkBox_5.setChecked(False) self.frame_9.setHidden(True) def del_last_data(self): try: with open(root_path_metadata+self.name_dialog+&#39;.csv&#39;,&quot;r&quot;,newline=&#39;&#39;,encoding=&#39;utf_8_sig&#39;) as csvfile: data = csvfile.readlines() del data[-1] with open(root_path_metadata+self.name_dialog+&#39;.csv&#39;,&quot;w&quot;,newline=&#39;&#39;,encoding=&#39;utf_8_sig&#39;) as csvfile: writer = csv.writer(csvfile, delimiter=&#39;,&#39;) for row in data: writer.writerow(row.strip().split(&#39;,&#39;)) # writer.writerows(data) QMessageBox.about(self,&#39;提示&#39;,&#39;上一句的标注已删除&#39;) except: QMessageBox.information(self,&#39;提示&#39;,&#39;该视频尚未保存任何数据&#39;, QMessageBox.Yes) if __name__ == &#39;__main__&#39;: app = QApplication(sys.argv) main_win = mainWin() main_win.show() # main_win.showFullScreen() sys.exit(app.exec_()) 封装成可执行文件对于编写好的.py文件，若是需要更好的给没有python编辑器的人使用，则需要封装成.exe文件，这可以通过 pyinstaller 命令来完成。 pyinstaller -F biaozhu.py pyinstaller 有两种常见的模式：-F: 將程式打包成单一的执行文件(适合比较简单的代码)-D: 打包多個文件，exe及依赖的东西会一起放置在dist資料夾里(适合框架形式的程式) 在打包过程中，包含如下步骤 在路径下生成了biaozhu.spec: 包含打包时相关的设定 建立build 文件夹，放置了log记录和相关文件 建立dist 文件夹，放置了可执行文件，若是 —F 模式，则里边仅有一个.exe文件 另外，若是打包失败，可以通过改写.spec文件，再通过 pyinstaller -D XXX.spec 重新打包。 如果是库的 import 问题，可以通过 hiddenimport 里放置库名来hidden掉该错误。 总结以前只知道 Tkinter 可以来实现 python 的界面设计，但感觉并不那么友好。 而这次学习到的 PyQt 以及相应的 Qt designer则很好的解决了这一问题，可以通过拉拽进行布局，就像c++的SDL一样。完成界面设计后的逻辑编写才是更让人头疼的问题，容易产生各种bug，只能慢慢调。 最后完成程序之后还可以转成.exe文件，从而可以直接给别人使用，这个是意料之外的惊喜。]]></content>
      <categories>
        <category>界面</category>
      </categories>
      <tags>
        <tag>界面</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习主流框架的代码实例]]></title>
    <url>%2F2020%2F05%2F27%2FDL-Framework%2F</url>
    <content type="text"><![CDATA[前言深度学习框架从一开始的 Theano、TensorFlow，到后来封装程度更高的Pytorch、Keras等，层出不穷。此文通过一个简单的分类任务，综合进这些框架的代码。代码来源于莫烦python。 Theanofrom __future__ import print_function import numpy as np import theano import theano.tensor as T def compute_accuracy(y_target, y_predict): correct_prediction = np.equal(y_predict, y_target) accuracy = np.sum(correct_prediction)/len(correct_prediction) return accuracy rng = np.random N = 400 # training sample size feats = 784 # number of input variables # generate a dataset: D = (input_values, target_class) D = (rng.randn(N, feats), rng.randint(size=N, low=0, high=2)) # Declare Theano symbolic variables x = T.dmatrix(&quot;x&quot;) y = T.dvector(&quot;y&quot;) # initialize the weights and biases W = theano.shared(rng.randn(feats), name=&quot;w&quot;) b = theano.shared(0., name=&quot;b&quot;) # Construct Theano expression graph p_1 = T.nnet.sigmoid(T.dot(x, W) + b) # Logistic Probability that target = 1 (activation function) prediction = p_1 &gt; 0.5 # The prediction thresholded xent = -y * T.log(p_1) - (1-y) * T.log(1-p_1) # Cross-entropy loss function # or # xent = T.nnet.binary_crossentropy(p_1, y) # this is provided by theano cost = xent.mean() + 0.01 * (W ** 2).sum()# The cost to minimize (l2 regularization) gW, gb = T.grad(cost, [W, b]) # Compute the gradient of the cost # Compile learning_rate = 0.1 train = theano.function( inputs=[x, y], outputs=[prediction, xent.mean()], updates=((W, W - learning_rate * gW), (b, b - learning_rate * gb))) predict = theano.function(inputs=[x], outputs=prediction) # Training for i in range(500): pred, err = train(D[0], D[1]) if i % 50 == 0: print(&#39;cost:&#39;, err) print(&quot;accuracy:&quot;, compute_accuracy(D[1], predict(D[0]))) print(&quot;target values for D:&quot;) print(D[1]) print(&quot;prediction on D:&quot;) print(predict(D[0]) 先搭建计算图，再通过theano.function绑定好输入和输出，形成一个函数（如train，predict） TensorFlowfrom __future__ import print_function import tensorflow as tf from tensorflow.examples.tutorials.mnist import input_data # number 1 to 10 data mnist = input_data.read_data_sets(&#39;MNIST_data&#39;, one_hot=True) def compute_accuracy(v_xs, v_ys): global prediction y_pre = sess.run(prediction, feed_dict={xs: v_xs, keep_prob: 1}) correct_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(v_ys,1)) accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) result = sess.run(accuracy, feed_dict={xs: v_xs, ys: v_ys, keep_prob: 1}) return result def weight_variable(shape): initial = tf.truncated_normal(shape, stddev=0.1) return tf.Variable(initial) def bias_variable(shape): initial = tf.constant(0.1, shape=shape) return tf.Variable(initial) def conv2d(x, W): # stride [1, x_movement, y_movement, 1] # Must have strides[0] = strides[3] = 1 return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=&#39;SAME&#39;) def max_pool_2x2(x): # stride [1, x_movement, y_movement, 1] return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding=&#39;SAME&#39;) # define placeholder for inputs to network xs = tf.placeholder(tf.float32, [None, 784])/255. # 28x28 ys = tf.placeholder(tf.float32, [None, 10]) keep_prob = tf.placeholder(tf.float32) x_image = tf.reshape(xs, [-1, 28, 28, 1]) # print(x_image.shape) # [n_samples, 28,28,1] ## conv1 layer ## W_conv1 = weight_variable([5,5, 1,32]) # patch 5x5, in size 1, out size 32 b_conv1 = bias_variable([32]) h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1) # output size 28x28x32 h_pool1 = max_pool_2x2(h_conv1) # output size 14x14x32 ## conv2 layer ## W_conv2 = weight_variable([5,5, 32, 64]) # patch 5x5, in size 32, out size 64 b_conv2 = bias_variable([64]) h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2) # output size 14x14x64 h_pool2 = max_pool_2x2(h_conv2) # output size 7x7x64 ## fc1 layer ## W_fc1 = weight_variable([7*7*64, 1024]) b_fc1 = bias_variable([1024]) # [n_samples, 7, 7, 64] -&gt;&gt; [n_samples, 7*7*64] h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64]) h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1) h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob) ## fc2 layer ## W_fc2 = weight_variable([1024, 10]) b_fc2 = bias_variable([10]) prediction = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2) # the error between prediction and real data cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction), reduction_indices=[1])) # loss train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy) sess = tf.Session() # important step # tf.initialize_all_variables() no long valid from # 2017-03-02 if using tensorflow &gt;= 0.12 if int((tf.__version__).split(&#39;.&#39;)[1]) &lt; 12 and int((tf.__version__).split(&#39;.&#39;)[0]) &lt; 1: init = tf.initialize_all_variables() else: init = tf.global_variables_initializer() sess.run(init) for i in range(1000): batch_xs, batch_ys = mnist.train.next_batch(100) sess.run(train_step, feed_dict={xs: batch_xs, ys: batch_ys, keep_prob: 0.5}) if i % 50 == 0: print(compute_accuracy( mnist.test.images[:1000], mnist.test.labels[:1000])) 先搭建计算图，并创建一个sess会话，通过sess.run(train_step, feed_dict={xs: batch_xs, ys: batch_ys, keep_prob: 0.5})这样的形式进行实际训练 Pytorch# library # standard library import os # third-party library import torch import torch.nn as nn import torch.utils.data as Data import torchvision import matplotlib.pyplot as plt # torch.manual_seed(1) # reproducible # Hyper Parameters EPOCH = 1 # train the training data n times, to save time, we just train 1 epoch BATCH_SIZE = 50 LR = 0.001 # learning rate DOWNLOAD_MNIST = False # Mnist digits dataset if not(os.path.exists(&#39;./mnist/&#39;)) or not os.listdir(&#39;./mnist/&#39;): # not mnist dir or mnist is empyt dir DOWNLOAD_MNIST = True train_data = torchvision.datasets.MNIST( root=&#39;./mnist/&#39;, train=True, # this is training data transform=torchvision.transforms.ToTensor(), # Converts a PIL.Image or numpy.ndarray to # torch.FloatTensor of shape (C x H x W) and normalize in the range [0.0, 1.0] download=DOWNLOAD_MNIST, ) # plot one example print(train_data.train_data.size()) # (60000, 28, 28) print(train_data.train_labels.size()) # (60000) plt.imshow(train_data.train_data[0].numpy(), cmap=&#39;gray&#39;) plt.title(&#39;%i&#39; % train_data.train_labels[0]) plt.show() # Data Loader for easy mini-batch return in training, the image batch shape will be (50, 1, 28, 28) train_loader = Data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True) # pick 2000 samples to speed up testing test_data = torchvision.datasets.MNIST(root=&#39;./mnist/&#39;, train=False) test_x = torch.unsqueeze(test_data.test_data, dim=1).type(torch.FloatTensor)[:2000]/255. # shape from (2000, 28, 28) to (2000, 1, 28, 28), value in range(0,1) test_y = test_data.test_labels[:2000] class CNN(nn.Module): def __init__(self): super(CNN, self).__init__() self.conv1 = nn.Sequential( # input shape (1, 28, 28) nn.Conv2d( in_channels=1, # input height out_channels=16, # n_filters kernel_size=5, # filter size stride=1, # filter movement/step padding=2, # if want same width and length of this image after Conv2d, padding=(kernel_size-1)/2 if stride=1 ), # output shape (16, 28, 28) nn.ReLU(), # activation nn.MaxPool2d(kernel_size=2), # choose max value in 2x2 area, output shape (16, 14, 14) ) self.conv2 = nn.Sequential( # input shape (16, 14, 14) nn.Conv2d(16, 32, 5, 1, 2), # output shape (32, 14, 14) nn.ReLU(), # activation nn.MaxPool2d(2), # output shape (32, 7, 7) ) self.out = nn.Linear(32 * 7 * 7, 10) # fully connected layer, output 10 classes def forward(self, x): x = self.conv1(x) x = self.conv2(x) x = x.view(x.size(0), -1) # flatten the output of conv2 to (batch_size, 32 * 7 * 7) output = self.out(x) return output, x # return x for visualization cnn = CNN() print(cnn) # net architecture optimizer = torch.optim.Adam(cnn.parameters(), lr=LR) # optimize all cnn parameters loss_func = nn.CrossEntropyLoss() # the target label is not one-hotted # following function (plot_with_labels) is for visualization, can be ignored if not interested from matplotlib import cm try: from sklearn.manifold import TSNE; HAS_SK = True except: HAS_SK = False; print(&#39;Please install sklearn for layer visualization&#39;) def plot_with_labels(lowDWeights, labels): plt.cla() X, Y = lowDWeights[:, 0], lowDWeights[:, 1] for x, y, s in zip(X, Y, labels): c = cm.rainbow(int(255 * s / 9)); plt.text(x, y, s, backgroundcolor=c, fontsize=9) plt.xlim(X.min(), X.max()); plt.ylim(Y.min(), Y.max()); plt.title(&#39;Visualize last layer&#39;); plt.show(); plt.pause(0.01) plt.ion() # training and testing for epoch in range(EPOCH): for step, (b_x, b_y) in enumerate(train_loader): # gives batch data, normalize x when iterate train_loader output = cnn(b_x)[0] # cnn output loss = loss_func(output, b_y) # cross entropy loss optimizer.zero_grad() # clear gradients for this training step loss.backward() # backpropagation, compute gradients optimizer.step() # apply gradients if step % 50 == 0: test_output, last_layer = cnn(test_x) pred_y = torch.max(test_output, 1)[1].data.numpy() accuracy = float((pred_y == test_y.data.numpy()).astype(int).sum()) / float(test_y.size(0)) print(&#39;Epoch: &#39;, epoch, &#39;| train loss: %.4f&#39; % loss.data.numpy(), &#39;| test accuracy: %.2f&#39; % accuracy) if HAS_SK: # Visualization of trained flatten layer (T-SNE) tsne = TSNE(perplexity=30, n_components=2, init=&#39;pca&#39;, n_iter=5000) plot_only = 500 low_dim_embs = tsne.fit_transform(last_layer.data.numpy()[:plot_only, :]) labels = test_y.numpy()[:plot_only] plot_with_labels(low_dim_embs, labels) plt.ioff() # print 10 predictions from test data test_output, _ = cnn(test_x[:10]) pred_y = torch.max(test_output, 1)[1].data.numpy() print(pred_y, &#39;prediction number&#39;) print(test_y[:10].numpy(), &#39;real number&#39;) 动态搭建网络，一般数据导入，网络搭建 损失函数，训练都是用各自的模块完成。通过继承封装好的父类，如nn.Module进行网络搭建，torch.utils.data.Dataset导入数据等等 Keras# to try tensorflow, un-comment following two lines # import os # os.environ[&#39;KERAS_BACKEND&#39;]=&#39;tensorflow&#39; import numpy as np np.random.seed(1337) # for reproducibility from keras.datasets import mnist from keras.utils import np_utils from keras.models import Sequential from keras.layers import Dense, Activation, Convolution2D, MaxPooling2D, Flatten from keras.optimizers import Adam # download the mnist to the path &#39;~/.keras/datasets/&#39; if it is the first time to be called # training X shape (60000, 28x28), Y shape (60000, ). test X shape (10000, 28x28), Y shape (10000, ) (X_train, y_train), (X_test, y_test) = mnist.load_data() # data pre-processing X_train = X_train.reshape(-1, 1,28, 28)/255. X_test = X_test.reshape(-1, 1,28, 28)/255. y_train = np_utils.to_categorical(y_train, num_classes=10) y_test = np_utils.to_categorical(y_test, num_classes=10) # Another way to build your CNN model = Sequential() # Conv layer 1 output shape (32, 28, 28) model.add(Convolution2D( batch_input_shape=(None, 1, 28, 28), filters=32, kernel_size=5, strides=1, padding=&#39;same&#39;, # Padding method data_format=&#39;channels_first&#39;, )) model.add(Activation(&#39;relu&#39;)) # Pooling layer 1 (max pooling) output shape (32, 14, 14) model.add(MaxPooling2D( pool_size=2, strides=2, padding=&#39;same&#39;, # Padding method data_format=&#39;channels_first&#39;, )) # Conv layer 2 output shape (64, 14, 14) model.add(Convolution2D(64, 5, strides=1, padding=&#39;same&#39;, data_format=&#39;channels_first&#39;)) model.add(Activation(&#39;relu&#39;)) # Pooling layer 2 (max pooling) output shape (64, 7, 7) model.add(MaxPooling2D(2, 2, &#39;same&#39;, data_format=&#39;channels_first&#39;)) # Fully connected layer 1 input shape (64 * 7 * 7) = (3136), output shape (1024) model.add(Flatten()) model.add(Dense(1024)) model.add(Activation(&#39;relu&#39;)) # Fully connected layer 2 to shape (10) for 10 classes model.add(Dense(10)) model.add(Activation(&#39;softmax&#39;)) # Another way to define your optimizer adam = Adam(lr=1e-4) # We add metrics to get more results you want to see model.compile(optimizer=adam, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) print(&#39;Training ------------&#39;) # Another way to train the model model.fit(X_train, y_train, epochs=1, batch_size=64,) print(&#39;\nTesting ------------&#39;) # Evaluate the model with the metrics we defined earlier loss, accuracy = model.evaluate(X_test, y_test) print(&#39;\ntest loss: &#39;, loss) print(&#39;\ntest accuracy: &#39;, accuracy) Keras基于Theano或TensorFlow的内核，形成了更高层的封装，有点类似Pytorch。通过model.compile()绑定模型、损失函数和优化器，并通过model.fit即可进行训练 总结这四种框架各自都在不断地自我完善推陈出新，像是目前TensorFlow 2已经把Keras合并进来了，Keras用户迁移过来也十分简单，Pytorch也在不断地让自身更简洁，如去掉了Variable变量的使用等。目前而言，TensorFlow 和 Pytorch 是两大巨头，个人感觉企业用 TensorFlow 更多，高校用 Pytorch 更多吧。]]></content>
  </entry>
  <entry>
    <title><![CDATA[强化学习简介]]></title>
    <url>%2F2020%2F05%2F27%2FRL%2F</url>
    <content type="text"><![CDATA[前言强化学习是机器学习中的一大类，它可以让机器学着如何在环境中拿到高分, 表现出优秀的成绩. 而这些成绩背后却是他所付出的辛苦劳动, 不断的试错, 不断地尝试, 累积经验, 学习经验. 强化学习的方法可以分为理不理解所处环境。不理解环境，环境给什么就是什么，称为model-free，包含 Q learning, Sarsa, Policy Gradients 等方法。 理解环境，用多一个模型去表示环境，就是 model-based 方法。 OpenAI gym 环境库是一个编写好了多种交互环境的库，而自己编写环境是一个很耗时间的过程，以下均不涉及环境的编写。 Q learningQ learning 是一种model-free方法，它的核心在于构建一个Q表，这个表表示了处于每一种状态(state)时进行各个行动(action)的奖励值。 举例而言(莫烦python的例子)，下图就是一个强化学习的过程，有16个state(位置)，4个可选的action(上下左右)。让探索者(红框)学会走迷宫. 黄色的是天堂 (reward 1), 黑色的地狱 (reward -1)。 那么，Q learning 的流程如下。 包含了不断重复的三个步骤。 给定当前状态s和Q表， 使用贪婪算法采取一个行动a 给定当前状态s和行动a，由环境交互给出下一个状态s’和奖励r 由s、s’、a、Q表，更新得到新的Q表每次更新我们都用到了 Q 现实和 Q 估计, 而且 Q learning 的迷人之处就是 在 Q(s1, a2) 现实 中, 也包含了一个 Q(s2) 的最大估计值, 将对下一步的衰减的最大估计和当前所得到的奖励当成这一步的现实. 代码如下： import numpy as np import pandas as pd class QLearningTable: def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9): self.actions = actions # a list self.lr = learning_rate self.gamma = reward_decay self.epsilon = e_greedy self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64) def choose_action(self, observation): self.check_state_exist(observation) # action selection if np.random.uniform() &lt; self.epsilon: # choose best action state_action = self.q_table.loc[observation, :] # some actions may have the same value, randomly choose on in these actions action = np.random.choice(state_action[state_action == np.max(state_action)].index) else: # choose random action action = np.random.choice(self.actions) return action def learn(self, s, a, r, s_): self.check_state_exist(s_) q_predict = self.q_table.loc[s, a] if s_ != &#39;terminal&#39;: q_target = r + self.gamma * self.q_table.loc[s_, :].max() # next state is not terminal else: q_target = r # next state is terminal self.q_table.loc[s, a] += self.lr * (q_target - q_predict) # update def check_state_exist(self, state): if state not in self.q_table.index: # append new state to q table self.q_table = self.q_table.append( pd.Series( [0]*len(self.actions), index=self.q_table.columns, name=state, ) ) from maze_env import Maze from RL_brain import QLearningTable def update(): for episode in range(100): # initial observation observation = env.reset() while True: # fresh env env.render() # RL choose action based on observation action = RL.choose_action(str(observation)) # RL take action and get next observation and reward observation_, reward, done = env.step(action) # RL learn from this transition RL.learn(str(observation), action, reward, str(observation_)) # swap observation observation = observation_ # break while loop when end of this episode if done: break # end of game print(&#39;game over&#39;) env.destroy() if __name__ == &quot;__main__&quot;: env = Maze() RL = QLearningTable(actions=list(range(env.n_actions))) env.after(100, update) env.mainloop() SarsaSarsa 和 Q learning 很类似，差别在于Sarsa会更‘胆小’一点，不太敢尝试。它的流程如下。 可以看出，它和 Q learning 差别仅在于更新环节，具体来讲： 他在当前 state 已经想好了 state 对应的 action, 而且想好了 下一个 state_ 和下一个 action_ (Qlearning 还没有想好下一个 action_) 更新 Q(s,a) 的时候基于的是下一个贪婪算法的 Q(s_, a_) (Qlearning 是基于 maxQ(s_))这种不同之处使得 Sarsa 相对于 Qlearning, 更加的胆小. 因为 Qlearning 永远都是想着 maxQ 最大化, 因为这个 maxQ 而变得贪婪, 不考虑其他非 maxQ 的结果. 我们可以理解成 Qlearning 是一种贪婪, 大胆, 勇敢的算法, 对于错误, 死亡并不在乎. 而 Sarsa 是一种保守的算法, 他在乎每一步决策, 对于错误和死亡比较铭感. import numpy as np import pandas as pd class RL(object): def __init__(self, action_space, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9): self.actions = action_space # a list self.lr = learning_rate self.gamma = reward_decay self.epsilon = e_greedy self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64) def check_state_exist(self, state): if state not in self.q_table.index: # append new state to q table self.q_table = self.q_table.append( pd.Series( [0]*len(self.actions), index=self.q_table.columns, name=state, ) ) def choose_action(self, observation): self.check_state_exist(observation) # action selection if np.random.rand() &lt; self.epsilon: # choose best action state_action = self.q_table.loc[observation, :] # some actions may have the same value, randomly choose on in these actions action = np.random.choice(state_action[state_action == np.max(state_action)].index) else: # choose random action action = np.random.choice(self.actions) return action def learn(self, *args): pass # off-policy class QLearningTable(RL): def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9): super(QLearningTable, self).__init__(actions, learning_rate, reward_decay, e_greedy) def learn(self, s, a, r, s_): self.check_state_exist(s_) q_predict = self.q_table.loc[s, a] if s_ != &#39;terminal&#39;: q_target = r + self.gamma * self.q_table.loc[s_, :].max() # next state is not terminal else: q_target = r # next state is terminal self.q_table.loc[s, a] += self.lr * (q_target - q_predict) # update # on-policy class SarsaTable(RL): def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9): super(SarsaTable, self).__init__(actions, learning_rate, reward_decay, e_greedy) def learn(self, s, a, r, s_, a_): self.check_state_exist(s_) q_predict = self.q_table.loc[s, a] if s_ != &#39;terminal&#39;: q_target = r + self.gamma * self.q_table.loc[s_, a_] # next state is not terminal else: q_target = r # next state is terminal self.q_table.loc[s, a] += self.lr * (q_target - q_predict) # update from maze_env import Maze from RL_brain import SarsaTable def update(): for episode in range(100): # 初始化环境 observation = env.reset() # Sarsa 根据 state 观测选择行为 action = RL.choose_action(str(observation)) while True: # 刷新环境 env.render() # 在环境中采取行为, 获得下一个 state_ (obervation_), reward, 和是否终止 observation_, reward, done = env.step(action) # 根据下一个 state (obervation_) 选取下一个 action_ action_ = RL.choose_action(str(observation_)) # 从 (s, a, r, s, a) 中学习, 更新 Q_tabel 的参数 ==&gt; Sarsa RL.learn(str(observation), action, reward, str(observation_), action_) # 将下一个当成下一步的 state (observation) and action observation = observation_ action = action_ # 终止时跳出循环 if done: break # 大循环完毕 print(&#39;game over&#39;) env.destroy() if __name__ == &quot;__main__&quot;: env = Maze() RL = SarsaTable(actions=list(range(env.n_actions))) env.after(100, update) env.mainloop() Deep Q Network(DQN)DQN 是一种结合了神经网络的强化学习。普通的强化学习中需要生成一个Q表，而如果状态数太多的话Q表也极为耗内存，所以 DQN 提出了用神经网络来代替Q表的功能。网络输入一个状态，输出各个动作的Q值。网络通过对Q估计和Q现实使用RMSprop来更新参数。Q估计就是网络输出，而Q现实等于奖励+下一状态的前模型的Q估计。流程图如下： 整个算法乍看起来很复杂, 不过我们拆分一下, 就变简单了. 也就是个 Q learning 主框架上加了些装饰，包括: 记忆库 (用于重复学习) 神经网络计算 Q 值 暂时冻结 q_target 参数 (切断相关性) 具体而言，记忆库是通过存储一堆数据在一个不断更新的记忆库里，训练时随机抽取数据出来训练。神经网络用来针对输入的状态来输出采取各个行动的Q值。共用了两个网络，他们的结构一模一样，但 q_target 网络用的是主网络之前很多个step的参数，这是为了形成一种延迟，切断他们的相关性。 import numpy as np import pandas as pd import tensorflow as tf np.random.seed(1) tf.set_random_seed(1) # Deep Q Network off-policy class DeepQNetwork: def __init__( self, n_actions, n_features, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9, replace_target_iter=300, memory_size=500, batch_size=32, e_greedy_increment=None, output_graph=False, ): self.n_actions = n_actions self.n_features = n_features self.lr = learning_rate self.gamma = reward_decay self.epsilon_max = e_greedy self.replace_target_iter = replace_target_iter self.memory_size = memory_size self.batch_size = batch_size self.epsilon_increment = e_greedy_increment self.epsilon = 0 if e_greedy_increment is not None else self.epsilon_max # total learning step self.learn_step_counter = 0 # initialize zero memory [s, a, r, s_] self.memory = np.zeros((self.memory_size, n_features * 2 + 2)) # consist of [target_net, evaluate_net] self._build_net() t_params = tf.get_collection(&#39;target_net_params&#39;) e_params = tf.get_collection(&#39;eval_net_params&#39;) self.replace_target_op = [tf.assign(t, e) for t, e in zip(t_params, e_params)] self.sess = tf.Session() if output_graph: # $ tensorboard --logdir=logs # tf.train.SummaryWriter soon be deprecated, use following tf.summary.FileWriter(&quot;logs/&quot;, self.sess.graph) self.sess.run(tf.global_variables_initializer()) self.cost_his = [] def _build_net(self): # ------------------ build evaluate_net ------------------ self.s = tf.placeholder(tf.float32, [None, self.n_features], name=&#39;s&#39;) # input self.q_target = tf.placeholder(tf.float32, [None, self.n_actions], name=&#39;Q_target&#39;) # for calculating loss with tf.variable_scope(&#39;eval_net&#39;): # c_names(collections_names) are the collections to store variables c_names, n_l1, w_initializer, b_initializer = \ [&#39;eval_net_params&#39;, tf.GraphKeys.GLOBAL_VARIABLES], 10, \ tf.random_normal_initializer(0., 0.3), tf.constant_initializer(0.1) # config of layers # first layer. collections is used later when assign to target net with tf.variable_scope(&#39;l1&#39;): w1 = tf.get_variable(&#39;w1&#39;, [self.n_features, n_l1], initializer=w_initializer, collections=c_names) b1 = tf.get_variable(&#39;b1&#39;, [1, n_l1], initializer=b_initializer, collections=c_names) l1 = tf.nn.relu(tf.matmul(self.s, w1) + b1) # second layer. collections is used later when assign to target net with tf.variable_scope(&#39;l2&#39;): w2 = tf.get_variable(&#39;w2&#39;, [n_l1, self.n_actions], initializer=w_initializer, collections=c_names) b2 = tf.get_variable(&#39;b2&#39;, [1, self.n_actions], initializer=b_initializer, collections=c_names) self.q_eval = tf.matmul(l1, w2) + b2 with tf.variable_scope(&#39;loss&#39;): self.loss = tf.reduce_mean(tf.squared_difference(self.q_target, self.q_eval)) with tf.variable_scope(&#39;train&#39;): self._train_op = tf.train.RMSPropOptimizer(self.lr).minimize(self.loss) # ------------------ build target_net ------------------ self.s_ = tf.placeholder(tf.float32, [None, self.n_features], name=&#39;s_&#39;) # input with tf.variable_scope(&#39;target_net&#39;): # c_names(collections_names) are the collections to store variables c_names = [&#39;target_net_params&#39;, tf.GraphKeys.GLOBAL_VARIABLES] # first layer. collections is used later when assign to target net with tf.variable_scope(&#39;l1&#39;): w1 = tf.get_variable(&#39;w1&#39;, [self.n_features, n_l1], initializer=w_initializer, collections=c_names) b1 = tf.get_variable(&#39;b1&#39;, [1, n_l1], initializer=b_initializer, collections=c_names) l1 = tf.nn.relu(tf.matmul(self.s_, w1) + b1) # second layer. collections is used later when assign to target net with tf.variable_scope(&#39;l2&#39;): w2 = tf.get_variable(&#39;w2&#39;, [n_l1, self.n_actions], initializer=w_initializer, collections=c_names) b2 = tf.get_variable(&#39;b2&#39;, [1, self.n_actions], initializer=b_initializer, collections=c_names) self.q_next = tf.matmul(l1, w2) + b2 def store_transition(self, s, a, r, s_): if not hasattr(self, &#39;memory_counter&#39;): self.memory_counter = 0 transition = np.hstack((s, [a, r], s_)) # replace the old memory with new memory index = self.memory_counter % self.memory_size self.memory[index, :] = transition self.memory_counter += 1 def choose_action(self, observation): # to have batch dimension when feed into tf placeholder observation = observation[np.newaxis, :] if np.random.uniform() &lt; self.epsilon: # forward feed the observation and get q value for every actions actions_value = self.sess.run(self.q_eval, feed_dict={self.s: observation}) action = np.argmax(actions_value) else: action = np.random.randint(0, self.n_actions) return action def learn(self): # check to replace target parameters if self.learn_step_counter % self.replace_target_iter == 0: self.sess.run(self.replace_target_op) print(&#39;\ntarget_params_replaced\n&#39;) # sample batch memory from all memory if self.memory_counter &gt; self.memory_size: sample_index = np.random.choice(self.memory_size, size=self.batch_size) else: sample_index = np.random.choice(self.memory_counter, size=self.batch_size) batch_memory = self.memory[sample_index, :] q_next, q_eval = self.sess.run( [self.q_next, self.q_eval], feed_dict={ self.s_: batch_memory[:, -self.n_features:], # fixed params self.s: batch_memory[:, :self.n_features], # newest params }) # change q_target w.r.t q_eval&#39;s action q_target = q_eval.copy() batch_index = np.arange(self.batch_size, dtype=np.int32) eval_act_index = batch_memory[:, self.n_features].astype(int) reward = batch_memory[:, self.n_features + 1] q_target[batch_index, eval_act_index] = reward + self.gamma * np.max(q_next, axis=1) &quot;&quot;&quot; For example in this batch I have 2 samples and 3 actions: q_eval = [[1, 2, 3], [4, 5, 6]] q_target = q_eval = [[1, 2, 3], [4, 5, 6]] Then change q_target with the real q_target value w.r.t the q_eval&#39;s action. For example in: sample 0, I took action 0, and the max q_target value is -1; sample 1, I took action 2, and the max q_target value is -2: q_target = [[-1, 2, 3], [4, 5, -2]] So the (q_target - q_eval) becomes: [[(-1)-(1), 0, 0], [0, 0, (-2)-(6)]] We then backpropagate this error w.r.t the corresponding action to network, leave other action as error=0 cause we didn&#39;t choose it. &quot;&quot;&quot; # train eval network _, self.cost = self.sess.run([self._train_op, self.loss], feed_dict={self.s: batch_memory[:, :self.n_features], self.q_target: q_target}) self.cost_his.append(self.cost) # increasing epsilon self.epsilon = self.epsilon + self.epsilon_increment if self.epsilon &lt; self.epsilon_max else self.epsilon_max self.learn_step_counter += 1 def plot_cost(self): import matplotlib.pyplot as plt plt.plot(np.arange(len(self.cost_his)), self.cost_his) plt.ylabel(&#39;Cost&#39;) plt.xlabel(&#39;training steps&#39;) plt.show() from maze_env import Maze from RL_brain import DeepQNetwork def run_maze(): step = 0 # 用来控制什么时候学习 for episode in range(300): # 初始化环境 observation = env.reset() while True: # 刷新环境 env.render() # DQN 根据观测值选择行为 action = RL.choose_action(observation) # 环境根据行为给出下一个 state, reward, 是否终止 observation_, reward, done = env.step(action) # DQN 存储记忆 RL.store_transition(observation, action, reward, observation_) # 控制学习起始时间和频率 (先累积一些记忆再开始学习) if (step &gt; 200) and (step % 5 == 0): RL.learn() # 将下一个 state_ 变为 下次循环的 state observation = observation_ # 如果终止, 就跳出循环 if done: break step += 1 # 总步数 # end of game print(&#39;game over&#39;) env.destroy() if __name__ == &quot;__main__&quot;: env = Maze() RL = DeepQNetwork(env.n_actions, env.n_features, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9, replace_target_iter=200, # 每 200 步替换一次 target_net 的参数 memory_size=2000, # 记忆上限 # output_graph=True # 是否输出 tensorboard 文件 ) env.after(100, run_maze) env.mainloop() RL.plot_cost() # 观看神经网络的误差曲线 总结强化学习本身是不依赖于深度学习的，它更多的是一种思想，通过行为与环境的交互产生奖励值，从而来更新Q表(或相同功能的神经网络)。它没有一种固定的代码，只有一套模式，具体代码还得根据实际应用与交互环境来编写。]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>强化学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用BeautifulSoup、requests和you_get爬虫下载B站视频]]></title>
    <url>%2F2020%2F05%2F19%2FBeautifulSoup%2F</url>
    <content type="text"><![CDATA[前言BeautifulSoup 是一个可以从HTML或XML文件中提取数据并解析的Python库， Requests 是一常用的可以获取和发送http的请求库， you_get 则是方便的下载各大网站的视频的命令行工具。整体流程上是，先用 Requests 请求获得网站源代码，再用 BeautifulSoup 解析网站并筛选出自己要的信息（如视频的url），最后用 you_get 下载。 例子以下代码实现的是下载B站电影。 #!/usr/bin/env python3 # -*- coding: utf-8 -*- &quot;&quot;&quot; Created on Sun Mar 15 12:06:21 2020 @author: weiquan fan &quot;&quot;&quot; from bs4 import BeautifulSoup as bs import requests,re,os def download(url, filename): path_root = &#39;./Videos&#39; os.system(&#39;you-get -o {} -O {} {}&#39;.format(path_root, filename, url)) url_base = &#39;https://www.bilibili.com/movie/?spm_id_from=333.851.b_62696c695f7265706f72745f6d6f766965.2&#39; response = requests.get(url_base) page = response.text soup = bs(page, &#39;html.parser&#39;) vids = soup.findAll(&#39;li&#39;,attrs={&#39;class&#39;:re.compile(&#39;video-item-biref.*?&#39;)})# bilibili video_urls = [] counter=1 if(vids): for v in vids: #v_link = v.find(&#39;a&#39;)[&#39;href&#39;] #v_name = v.find(&#39;img&#39;)[&#39;alt&#39;] print(v) v_link = v.find(&#39;a&#39;)[&#39;href&#39;] v_name = v.find(&#39;img&#39;)[&#39;alt&#39;] video_urls.append([v_link, v_name]) print(v_link,v_name) try: download(v_link, v_name) except Exception: print(&#39;can\&#39;t download &#39;+v_name+&#39; in &#39;+v_link) counter -= 1 counter += 1 if(counter&gt;15): break]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[CNN的进击之路——讲讲ResNet, Inception, ResNeXt和Densenet等常见网络]]></title>
    <url>%2F2020%2F05%2F13%2Fresnet%2F</url>
    <content type="text"><![CDATA[前言本文是一篇大杂烩，按照发布时间总结了CNN的一些常见网络。 AlexNetAlexNet来源于ImageNet Classification with Deep Convolutional Neural Networks。在ImageNet LSVRC-2010上以远超第二的准确率夺得了冠军，拉开了深度学习热潮的大幕。 模型结构： 模型特点： 提出了非线性激活函数ReLU (之前普遍使用Sigmoid或者tanh) 提出Dropout（每次迭代训练时随机删除一些神经元） 重叠池化（池化的时候，每次移动的步长小于池化的窗口长度） 数据扩充（水平翻转图像，从原始图像中随机裁剪、平移变换，颜色、光照变换） LRN归一化层（利用临近的数据做归一化） 多GPU实现（受当时GPU限制，在每个GPU中放置一半神经元，将网络分布在两个GPU上进行并行计） VGGVGG来源于Oxford的Visual Geometry Group的组提出的Very Deep Convolutional Networks for Large-Scale Image Recognition，在ILSVRC 2014获得亚军。 模型结构： 其中D、E列就是著名的VGG-16、VGG-19。 模型特点：使用了3个3x3卷积核来代替7x7卷积核，使用了2个3x3卷积核来代替5x5卷积核。因此模型结构很统一简洁（卷积核尺寸3x3和最大池化尺寸2x2），并不断加深网络。 GoogLeNet V1GoogLeNet V1来源于Going deeper with convolutions，在ILSVRC 2014获得冠军。 该网络的核心在于提出了Inception Module。该模块有4个分支，初始版本如下图左，包含三个不同尺度的卷积核层和一个最大池化层，并在输出通道维度上合并。由于5×5的计算量大，就进一步先通过1×1卷积降低维度再通过大卷积核。这里的最大池化也是重叠池化的，经padding后不会缩小特征图尺寸。 模型结构： 模型特点： 多尺度卷积的思想让网络变宽 提出1×1卷积 GoogLeNet V2GoogLeNet V2来源于Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift。该网络基于V1版本，吸收了VGG的分解操作，使用了2个3x3卷积核来代替5x5卷积核。 模型特点： 提出了著名的BN层。 另外，为了适配BN层，增大学习速率并加快学习衰减速度以适用BN规范化后的数据；去除Dropout并减轻L2正则（因BN已起到正则化的作用）；去除LRN；更彻底地对训练样本进行shuffle；减少数据增强过程中对数据的光学畸变（因为BN训练更快，每个样本被训练的次数更少，因此更真实的样本对训练更有帮助）。 GoogLeNet V3GoogLeNet V3来源于Rethinking the Inception Architecture for Computer Vision。该网络基于V2版本，进一步改进了Inception，将3x3分解成1x3和3x1。同理，nxn可以分解成1xn和nx1。 ResNetResNet来源于大神何凯明的Deep Residual Learning for Image Recognition，在ILSVRC和COCO 2015上都夺得了冠军，有着里程碑的意义。 深度模型当深度到了几十层之后，由于梯度消失或者爆炸的原因，就容易发生退化问题：网络深度增加时，网络准确度出现饱和，甚至出现下降。现在假设我们有一个浅层网络，我们想通过向上堆积新层来建立深层网络，一个极端情况是这些增加的层什么也不学习，仅仅复制浅层网络的特征，即这样新层是恒等映射（Identity mapping）。在这种情况下，深层网络应该至少和浅层网络性能一样，也不应该出现退化现象。这引发了残差学习，即我们的目标是学习到残差F(x)=H(x)-x，则该层学习到的最终特征H(x)=F(x)+x。当残差为0时，此时堆积层仅仅做了恒等映射，至少网络性能不会下降，实际上残差不会为0，这也会使得堆积层在输入特征基础上学习到新的特征，从而拥有更好的性能。残差学习的结构下图所示。这有点类似与电路中的“短路”，所以是一种短路连接（shortcut connection）。 模型结构：ResNet网络参考VGG19网络，引入残差单元。如下图，第三列即是ResNet-34。 模型特点： 提出残差模块 模型开始变得很深，可以达到152层 卷积层由Conv+BN+ReLU变成BN+ReLU+Conv GoogLeNet V4GoogLeNet V4来源于Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning。该论文一方面沿袭v3版本，使用更多的Inception module得到GoogLeNet V4。另一方面吸收了ResNet的残差单元，提出了两种Inception-ResNet。 模型结构：下图为其中一种，Inception-ResNet-v1，具有如下特点： Inception module都是简化版，没有使用那么多的分支，因为identity部分（直接相连的线）本身包含丰富的特征信息； Inception module每个分支都没有使用pooling； 每个Inception module最后都使用了一个1x1的卷积（linear activation），作用是保证identity部分和Inception部分输出特征维度相同，这样才能保证两部分特征能够相加。 模型特点： 使得宽模型变得更深 DenseNetDenseNet来源于Densely Connected Convolutional Networks，斩获了CVPR 2017的最佳论文奖。 模型结构：DenseNet有点类似于ResNet，但本质上又有很大的不同。结构上，把以前所有层的特征图都沿着通道轴拼接起来（而不是相加）。这可以理解为充分利用产生过的特征。 如下为ResNet： 如下为DenseNet： 模型特点： 建立了不同层的连接关系，充分利用特征图 MobileNetMobileNet来源于Google提出的MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications，是一种小巧而高效的CNN模型。 模型结构：MobileNet的核心在于提出了深度可分离卷积，它把传统卷积分解成了深度卷积(depthwise convolution)和逐点卷积(pointwise convolution)，从而大量减少参数量。 对于输入特征图(DF,DF,M)，输出特征图(DG,DG,N)，传统卷积核的尺寸为(K,K,M,N)，如下图(a)。而对于深度可分离卷积，深度卷积的尺寸为(K,K,1,M)，它将这M个卷积核各自应用于输入特征图的各个通道（这与传统卷积不同，这里相乘后不需要沿着通道轴相加），输出特征为(DG,DG,M)，如(b)所示。逐点卷积的尺寸为(1,1,M,N)，这个就是普通的1×1卷积了，输出特征为(DG,DG,N)，如(c)所示。可以看到，参数量从（K×K×M×N）变成（K×K×1×M + 1×1×M×N），减小了 M(KKN - KK -N)。 模型特点： 轻型模型，可用于移动端 ResNeXtResNeXt来源于Aggregated Residual Transformations for Deep Neural Networks。它是基于ResNet，吸收了GoogLeNet的Inception，所以和谷歌的Inception-ResNet很像。 模型结构：如下图，左图是是ResNet，右图是新的ResNeXt。 该结构可以做如下等效，第三种就是等效的分组结构。 模型特点： ResNeXt的分支的拓扑结构是相同的，而Inception V4需要人工设计 提出了一种介于普通卷积核深度可分离卷积的这种策略：分组卷积 XceptionXception来源于Xception: Deep Learning with Depthwise Separable Convolutions。它是Inception-V3的另一种改进，吸收了深度可分离卷积，造就了一种参数量相对少一些的网络结构。 模型结构：Inception-V3可做如下简化，可以看到，如下图和深度可分离卷积是很像的，只是下图是先进行1×1的卷积，再进行channel-wise的spatial convolution，最后concat，而后者是先进行一个channel-wise的spatial convolution，然后是1×1的卷积。所以作者干脆把它换成深度可分离卷积。 最终整体结构如下，其中SeparalbeConv即是深度可分离卷积。 模型特点： 虽然使用了深度可分离卷积，但网络也加宽了，总体参数量和Inception-V3差不多，性能提升了。 提出时间和MobileNet相近，它们从不同的角度揭示了深度可分离卷积的强大作用，MobileNet的思路是通过将 3×3 卷积拆分的形式来减少参数数量，而Xception是通过对Inception的充分解耦来完成的。 ShuffleNetXception来源于ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices。这也是一款效率极高的轻型CNN模型，通过逐点群卷积(pointwise group convolution)和通道混洗(channel shuffle)大大降低计算量。 模型结构：如下图左是普通的分组卷积，但是经过多层分组卷积后某个输出channel仅仅来自输入channel的一小部分，学出来的特征也很局限，因此作者提出了通道混洗channel shuffle，过程如下图中，在进行GConv2之前，对其输入feature map做一个分配，也就是每个group分成几个subgroup，然后将不同group的subgroup作为GConv2的一个group的输入，使得GConv2的每一个group都能卷积输入的所有group的feature map，结果图下图右。 pointwise group convolution，其实就是带group的卷积核为1×1的卷积。下图左是一个深度可分离卷积，而中间的图则是一个使用了pointwise group convolution的ShuffleNet unit，它将1×1卷积变成分组卷积，并在第一组分组卷积后加上通道混洗而成。右边的图则是带有降采样的ShuffleNet unit，它一方面在辅分支加入步长为2的3×3平均池化，一方面将最后的相加变成了通道级联。 模型特征： 应用了1×1的通道卷积 提出了通道混洗 总结其实总的来说，创新性的应该包含了inception，残差学习，深度可分离卷积，分组卷积几种。inception有GoogLeNet V1-V4、Xception、ResNeXt。残差学习有ResNet、ResNeXt、DenseNet、GoogLeNet V4。深度可分离卷积有MobileNet、ShuffleNet、Xception。分组卷积有ResNeXt、ShuffleNet。]]></content>
      <categories>
        <category>深度学习模型</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[cv中Attention的奇妙旅途——讲讲Self-Attention, SENet和CBAM]]></title>
    <url>%2F2020%2F05%2F08%2FSENet%2F</url>
    <content type="text"><![CDATA[前言由于注意力机制的高速发展，我尝试着对attention形成一种比较系统化的理解，选了比较有代表性的Self-Attention, SENet和CBAM，整理成本文。 Self-Attention在谷歌发表的Attention Is All You Need之后，Self-Attention开始广为人知。正如我此前对这篇论文的讲解，最终的注意力可以表示为下图，其中Q为Query，K为Key，V为Value，三者都是由输入X经过不同的映射得来的。这个公式可以这么记，先通过相乘得到Query和Key的相似度，而后归一化加softmax成为注意力权重，该权重乘以Value值就是输出的新表达了。 然而，这个公式在这里的输入X的维度是time × embedding，那么怎么用于三维的图像呢？很自然的可以想到，时序信号中的时间可以类比到图像中的空间，那么只需要把长和宽两个维度拉成一个维度，就形成了一行地的空间信息。那么接下来，剩下的通道（单通道或三通道），就可以类比成时序信号中token的embedding。因此，Self-Attention公式在图像上的输入的维度是spatial × channel。那么，图像上的Self-Attention本质上是计算一种空间权重。 SENet2017年Squeeze-and-Excitation Networks获得了ILSVRC的冠军，使得SENet名声大噪。其实这篇论文的核心在于提出了一种很方便嵌入其他模型的模块————SE block。它的结构图如下。前边是传统的卷积操作，得到了特征图U(H x W x C)。而后看上边的支路，共有两个操作，一个是Squeeze一个是Excitation，可以得到一串通道上的注意力权重，再把它乘进各个通道，就得到了新的特征图。 Squeeze操作对U进行Global Average Pooling，得到一串(1 x 1 x C)的权重，这里就是注意力的雏形了，比起一开始Self注意力的QK矩阵真是简单粗暴了很多。 Excitation操作也很直接，就是把刚才得到的权重经过两层全连接层（后边带ReLU）再加一层Sigmoid。至于两层全连接的神经元数，第一层是输入C个而输出C/r个神经元，第二次则是输入C/r而输出C个神经元。可以看成一种压缩再恢复的过程，r表示压缩程度。作者的实验表明r取16时效果比较好。所以，图像上的SENet本质上是计算一种通道权重。 CBAM现在我们知道了有空间的注意力，有通道的注意力，那么也可以想到一种两者都有的注意力。CBAM: Convolutional Block Attention Module就干了这么一件事。它也是一种可嵌入的模块，结构图如下,包含通道注意力模块和空间注意力模块。 这里分别讲解这两种模块。 通道注意力模块 通道注意力模块其实基本就是SE block，不同点在于除了SE用的AvgPool之外还用了MaxPool，相当于有两种Squeeze方式，而后得到的两串注意力雏形各自同样经过两层带ReLU的全连接层（注意这里avg和max使用的全连接是共享的，有点奇怪，个人感觉不共享会好一些），相加起来再经过Sigmoid，就得到了通道上的注意力，然后乘回去得到了通道上更新了的特征图。 空间注意力模块 空间注意力模块可以说是用Self的思想和SENet的操作形成的。可以看到，刚才的通道注意力是在空间上进行Pool，那么，空间注意力是不是也可以在通道上进行Pool呢？这就形成了空间注意力模块。首先，它基于通道上进行global max pooling 和global average pooling，得到的两张空间图拼接一下形成2通道，再进行一下卷积(实验表明7 * 7卷积效果好些)降成单通道，类似的经过一个Sigmoid，就得到了空间注意力权重。乘回去就得到了空间上更新了的特征图。 所以，CBAM基于senet的通道注意力，引入空间注意力，本质上是计算了通道和空间的权重。 总结经过这么一个流程可以看到： 注意力就是计算通道(嵌入)和空间(时间)的注意力权重，Self是空间，SENet是通道，CBAM是空间加通道。甚至于空间上的注意力还可以拆分，只有横轴的注意力或只有纵轴的注意力，这些都视实际输入的图像需求来选择。 注意力的形式变得简单。一开始的Self需要经过QK矩阵算出一个三维（nlp中是二维，空间拉出长宽就变成了三维）的注意力权重，表示空间上每一个点与空间上所有点的注意力。CBAM中的空间注意力模块，只需经过pooling和一些简单的其它操作即得到了一个二维的注意力权重，表示一种整体上应该关注空间上的哪些位置。]]></content>
      <categories>
        <category>深度学习模型</category>
      </categories>
      <tags>
        <tag>attention</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[讲讲横扫nlp任务的BERT模型]]></title>
    <url>%2F2020%2F05%2F07%2FBERT%2F</url>
    <content type="text"><![CDATA[前言本文讲解Google在2019年发表的论文BERT: Pre-training of Deep Bidirectional Transformers forLanguage Understanding 。从标题可以看出，该论文基于Transformer模型，提出了一款用于语言理解的预训练模型，并在GLUE, SQuAD等nlp任务中都取得了很好的效果。该模型的创新点实际上不在于模型结构，而在于预训练的方法。以下围绕这两方面都进行一些讲解。 模型结构总体框图首先最好还是先理解一下Transfomer，这是BERT模型的基础所在。简而言之，Transfomer包含N个编码器和解码器。编码器将输入序列编码成带有全局新的特征序列，解码器将编码器的特征序列解码成预测结果。BERT使用的正是其中的编码器，模型如下所示。 注意：并不是一个Tm代表一个Transformer，可以看成一行Tm表示一个Transformer的编码器，而编码器也不是两层可以有多层。那么，输入序列送入BERT后，最后一层Tm将对每一个输入token都生成一个新的特征序列，而T则表示任务，一般都是用全连接层来完成分类任务。另外用来对比的是，与GPT是单向的Transformer连接，ELMo是双向的LSTM连接。 Embedding从框图上可以看到，输入序列是各token的embedding。在Transfomer中，这种嵌入由word embedding加上positional embedding而来。而在BERT中，还要额外加上一个segment embedding，用于指示各个token属于输入的第几个句子。这是因为有的nlp任务是输入一对句子的，需要借此加以区分。而且，positional embedding也不是沿用三角函数，三种embedding都是学习出来的。另外，一开始的token，除了要在一开始添加一个起始标志[CLS]之外，还要在不同句子的过渡位置插一个[SEP]标志（如果有多句子的话）。 迁移策略BERT是一个预训练模型，所以我们要怎么拿过来用呢？官方为我们提供了各种任务的应用方法。首先，NLP的下游任务可以分为4类： 句子关系判断：识别蕴含(entailment)、识别语义相似等 分类任务：文本分类、情感计算等 序列标注：分词、实体识别、语义标注等 生成式任务：机器翻译、文本摘要等 图中 (a) 解决的是句子关系判断问题，MultiNLI(识别蕴含，M推理出N，蕴含/矛盾/中立），QQP（识别语义相似），QNLI（识别是否回答了问题），STS-B（识别语义相似），MRPC（识别语义等价，微软）、RTE（识别蕴含，小数据），SWAG（识别回答问题，大数据)。(b) 解决的是分类任务，SST-2（情感计算，斯坦福），CoLA（句子语言性判断，是否能成句）。(c) 解决的是序列标注任务，SQuAD（判断回答的起始和结束时刻，斯坦福问答数据集，从phrase中选取answer）。(d) 解决的是序列标注任务，NER（命名实体识别）。总的来说 (a) 和 (b) 都是分类任务，差别在于输入的是一个句子还是一对句子。这类任务，只需通过在第一个token（即[CLS]标志）的特征序列送入全连接层即可获取识别结果。(c) 和 (d) 都是序列标注任务，这类则在多个token的特征序列送入全连接层，获取各自的标注结果。可以看到，目前只有生成式任务还没有被ko。 p.s. 大名鼎鼎的GLUE任务集则包含了MultiNLI、QQP、QNLI、STS-B、MRPC、RTE、WNLI(也是识别蕴含)、SST-2、CoLA。 预训练方法常见的预训练方法一般是给前面的序列去预测下一个token（像是GPT）。而BERT就提出了两个比较有意思的训练任务 —— Masked LM 和 Next Sentence Prediction。 Masked LM为了实现模型的双向，就不能一直给定前面预测后面，于是作者提出了一个trick。在训练过程中，随机mask掉15%的token，即把相应位置的token替换成一个[MASK]标识，而任务的目标就是要去恢复这个句子（包含MASK的词），而损失函数只考虑了MASK位置的预测值，忽视掉非masked的值。这样，MASK的词可前可后，就实现了模型的双向性。 由此也衍生出了一个问题，就是在实际预测的时候是不会碰到[MASK]的，用了太多[MASK]就容易影响到模型。所以作者又用了个小技巧，选中了要mask的token后，其中10%的token会被替代成其他token，10%的token不替换，剩下的80%才被替换为[MASK]。 Next Sentence Prediction由于nlp中存在需要输入两个句子的句子关系判断任务，所以需要增设一个让模型理解句子之间关系的任务，于是Next Sentence Prediction应运而生。具体而言，就是输入两个句子，由模型来判断这两个句子是不是连续的上下句。其中，为了保持样本平衡性，选了50%的连续的正样本，再随机选50%的无关的负样本。其实这个任务和seq2seq的任务有点异曲同工之妙，只是从单词级别变到了句子级别。举个例子：正样本：今天[MASK]（天气）真好，正好我们[MASK]（出去）吃饭吧。负样本：今天[MASK]（天气）真好，[MASK]（我）吃饱了。 总结其实BERT模型除了提出这两种训练方法外，大量的数据肯定对这个预训练模型有很强的作用，不过一般人没这种计算资源…所以还是很感谢谷歌开源出来的预训练模型，可以很方便的使用并达到非常好的效果。在使用时，如果需要用到自己的数据库上，要么就是完全自己写然后导入BERT模型，要么可以直接使用官方的代码，如run_glue.py，只需要修改数据的预处理，定义好新的类，然后指定类别数等参数，就可以直接使用了。 参考文献https://www.cnblogs.com/rucwxb/p/10277217.htmlhttps://zhuanlan.zhihu.com/p/46652512]]></content>
      <categories>
        <category>深度学习模型</category>
      </categories>
      <tags>
        <tag>BERT</tag>
        <tag>Transformer</tag>
        <tag>GLUE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Transfomer以及Self-Attention讲解]]></title>
    <url>%2F2020%2F05%2F05%2Ftransfomer%2F</url>
    <content type="text"><![CDATA[前言这一篇主要讲解谷歌发表的Attention Is All You Need。这篇论文提出了驰名的一种注意力机制 —— self-attention 模块，并进一步提出了 Transformer 架构，从而将以往用的计算代价较大的RNN替换掉了。目前，nlp任务中效果非常好的BERT模型就是大量应用了Transformer架构的Encoder。 下边是一个很好的使用Transformer进行机器翻译任务的例子。在预测过程中，编码阶段，输入的“I arrived at the”中的每个单词都会计算与所有单词的注意力权重，并加权求和得出新的自己的表示，逐层编码。解码阶段，输入由encoder出来的所有单词的表示和上一个位置输出的embedding，经过类似的注意力操作得到这一个位置的输出，是一种随着预测位置移动的迭代过程。戳我看例子 总体框架与流程框架对照着以上例子，看下边的Transfomer总体框架图。左边为编码器，右边为解码器。编码器和解码器中都包含了Positional Encoding模块，Multi-Head Attention模块，Feed-Forward模块。下一章节会对此着重讲解。 流程定义一下符号。 emb_dim：嵌入的尺寸 input_length：输入序列的长度 target_length：目标序列的长度+1。+1是因为要移位。 vocab_size：目标词汇表中的单词数量。 则Transformer的流程可表示为： 该模型将每个token表示为维度emb_dim的向量。然后，对于特定的输入序列，我们有了尺寸为（input_length）x（emb_dimb）的矩阵。 然后添加位置信息（位置编码）。与上一步一样，此步骤将返回尺寸为（input_length）x（emb_dim）的矩阵。 数据通过N=6个编码器块。之后，我们获得尺寸为（input_length）x（emb_dim）的矩阵。 目标序列经过等同于1和2的操作，并进行mask屏蔽。输出的尺寸为（target_length）x（emb_dim）。 4的结果经过N=6个解码器块。在每个迭代中，解码器都使用编码器的输出3）。这在总框图中由从编码器到解码器的箭头表示。输出的尺寸为（target_length）x（emb_dim）。 最后，逐行使用全连接层和softmax。输出的尺寸为（target_length）x（vocab_size）。 编码器对于训练阶段和测试阶段是一样的编码过程，而解码器的流程则有所不同，因此先讲解一下解码器的训练和测试。在测试阶段，由于没有groundtruth，所以我们需要从零开始不断迭代一个词一个词地生成。具体操作如下： 计算输入序列的嵌入表示。 使用起始token例如’‘，作为第一个目标序列。该模型将预测输出一个token。 将最后一个预测token添加到目标序列，并使用它生成新的预测。 重复执行步骤3，每次的输入token和输出token都增加，直到预测的token是表示序列结束的token，例如。 在训练阶段中，由于我们事先有roundtruth，因此我们将直接为模型提供整个已移位目标序列，并要求其预测未移位目标。举个例子，目标是将句子从英语翻译成西班牙语：X = [‘Hello’，’，’，’how’，’are’，’you’，’？’]（输入序列）Y = [‘Hola’，’，’，’como’，’estas’， ‘？’]（目标序列）在前面的示例之后，我们将给解码器输入：[‘‘，’Hola’，’，’，’como’，’estas’，’？’]预期的预测将是：[‘Hola’，’，’，’como’，’estas’，’？’，’‘] 因此可以看到，解码器在训练时直接从target_length-&gt;target_length，而测试时则是从1-&gt;1 2-&gt;2 3-&gt;3 … target_length-&gt;target_length的过程，最后预测的是每次迭代中最后一个预测的token串联起来。 Positional EncodingTransformer抛弃了RNN，而RNN最大的优点就是在时间序列上对数据的抽象，所以文章中作者提出两种Positional Encoding的方法，将encoding后的数据与embedding数据求和，加入了相对位置信息。 用不同频率的sine和cosine函数直接计算 学习出一份positional embedding实验后发现两者结果一样，所以用了第一种方法，优点是不需要训练参数，而且即使在训练集中没有出现过的句子长度上也能用 对于输入序列，经过word embedding后，加上positional embedding后即可得到该序列的 representation，序列中的每个token都转换成包含 word 的特征和 word 在句子中的位置信息的向量。 Multi-Head AttentionMulti-Head Attention其实就是多个Self-Attention结构的结合。因此，首先我们需要着重学习论文的重点Self-Attention。 Self-Attention从一个比较知名的例子讲起。假如我们要翻译一个词组Thinking Machines，其中Thinking的输入的embedding vector用x1表示，Machines的embedding vector用x2表示。当我们处理Thinking这个词时，我们需要计算句子中所有词与它的Attention Score，这就像将当前词作为搜索的query，去和句子中所有词（包含该词本身）的key去匹配（点乘），看看相关度有多高。相关度进行尺度缩放与softmax归一化可以得到注意力权重，注意力与相应的value加权求和就得到新的表达。 如果将输入的所有向量合并为矩阵形式，则所有query, key, value向量也可以合并为矩阵形式表示 则上述操作可简化为矩阵形式 这就是著名的注意力公式： Multi-Head Attention基于上边的Self-Attention， 我们进一步拓展，对输入序列使用不同的Q，K，V进行多次以上操作，而后拼接起来，再转换成最终的表示。这样每个head可以学习到在不同表示空间中的特征。 可视化如下： Masked Multi-Head Attention在训练过程的解码器中，需要对输入的注意力矩阵（即上边QK经过softmax的矩阵）进行masked操作，从而不给模型看见未来信息，解决了信息泄露问题。举例来说，对于目标序列（I have a dream），I作为第一个单词，只能有和自身的attention。have作为第二个单词，有和I, have 两个attention。 a 作为第三个单词，有和I,have,a 前面三个单词的attention。到了最后一个单词dream的时候，才有对整个句子4个单词的attention。 其它操作和上述的Multi-Head Attention一致。 Encoder-Decoder Multi-Head Attention在解码器的第二层attention里，需要整合encoder的输入序列和decoder的目标序列的信息，算出相互之间的注意力。与Multi-Head Attention的不同点在于，Encoder-Decoder Multi-Head Attention的Q矩阵来自decoder，而K和V来自encoder。其实也很好理解，就是注意力矩阵是由来自解码器的Query和来自编码器的Key之间计算得来，其它操作都相同。 Feed-Forward这个就很简单了，就是简单的映射层。 Produce Output Probabilities这个其实也是普通的映射层，它将每一个目标序列的token由emb_dim映射到vocab_size，因此就可得到各个token，串成目标序列了。 总结不得不说，这确实是一篇很经典的论文，将seq2seq模型推到了一个新高度，避免了RNN的大量计算代价，从此用CNN操作序列信号就有很好的效果了。另外，Self-Attention还跨界在cv行业也有了非常多的研究。可以说cv和nlp是同源的，只需要将图像的长宽拉成一列（空间信息）类比成序列信号的序列，图像的通道类比成序列信号的embedding即可。因此，Self-Attention模块的输入在nlp上是time × embedding，在cv上是spatial × channel。此外，当下横扫nlp的BERT模型也是基于Transfomer的encoder，这也表明这个模型的重要性了。 参考文献 https://medium.com/dissecting-bert/dissecting-bert-appendix-the-decoder-3b86f66b0e5f https://zhuanlan.zhihu.com/p/80986272 https://zhuanlan.zhihu.com/p/44121378 https://zhuanlan.zhihu.com/p/39034683 https://zhuanlan.zhihu.com/p/47282410]]></content>
      <categories>
        <category>深度学习模型</category>
      </categories>
      <tags>
        <tag>Transformer</tag>
        <tag>attention</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常见的梯度下降算法原理]]></title>
    <url>%2F2020%2F05%2F04%2Fgradient-descent%2F</url>
    <content type="text"><![CDATA[前言梯度下降算法（Gradient Descent Optimization）是神经网络模型训练最常用的优化算法。对于深度学习模型，基本都是采用梯度下降算法来进行优化训练的。梯度下降算法背后的原理：目标函数 $J(\theta)$ 关于参数 $\theta$ 的梯度将是损失函数（loss function）上升最快的方向。而我们要最小化loss，只需要将参数沿着梯度相反的方向前进一个步长，就可以实现目标函数（loss function）的下降。这个步长 $\eta$ 又称为学习速率。 原始的梯度下降Batch gradient descent 批梯度下降，对所有的样本计算梯度后求平均，并更新参数。 因为在执行每次更新时，我们需要在整个数据集上计算所有的梯度，所以批梯度下降法的速度会很慢，同时，批梯度下降法无法处理超出内存容量限制的数据集。批梯度下降法同样也不能在线更新模型，即在运行的过程中，不能增加新的样本。 对于凸误差函数，批梯度下降法能够保证收敛到全局最小值，对于非凸函数，则收敛到一个局部最小值。 SGD 随机梯度下降，对每个样本计算梯度，并更新一次参数。 SGD的运行速度更快 可以用于在线学习 SGD以高方差频繁地更新，导致目标函数出现剧烈波动。 与批梯度下降法的收敛会使得损失函数陷入局部最小相比，由于SGD的波动性，一方面，波动性使得SGD可以跳到新的和潜在更好的局部最优。另一方面，这使得最终收敛到特定最小值的过程变得复杂，因为SGD会一直持续波动。然而，已经证明当我们缓慢减小学习率，SGD与批梯度下降法具有相同的收敛行为，对于非凸优化和凸优化，可以分别收敛到局部最小值和全局最小值。 Mini-batch GD 小批量梯度下降法最终结合了上述两种方法的优点，在每次更新时使用个小批量训练样本 减少参数更新的方差，这样可以得到更加稳定的收敛结果 可以利用最新的深度学习库中高度优化的矩阵优化方法，高效地求解每个小批量数据的梯度。 小结原始的梯度下降方法有以下问题： 在梯度平缓的维度下降非常慢，在梯度险峻的维度容易抖动 容易陷入局部极小值或鞍点。Zero gradient,gradient descent gets stuck （在高维空间中，鞍点比局部极小值更容易出现）-选择一个合适的学习率可能是困难的。学习率太小会导致收敛的速度很慢，学习率太大会妨碍收敛，导致损失函数在最小值附近波动甚至偏离最小值-学习率调整试图在训练的过程中通过例如退火的方法调整学习率，即根据预定义的策略或者当相邻两代之间的下降值小于某个阈值时减小学习率。然而，策略和阈值需要预先设定好，因此无法适应数据集的特点-对所有的参数更新使用同样的学习率。如果数据是稀疏的，同时，特征的频率差异很大时，我们也许不想以同样的学习率更新所有的参数，对于出现次数较少的特征，我们对其执行更大的学习率 带冲量的梯度下降Momentum optimization冲量梯度下降算法是Boris Polyak在1964年提出的，其基于这样一个物理事实：将一个小球从山顶滚下，其初始速率很慢，但在加速度作用下速率很快增加，并最终由于阻力的存在达到一个稳定速率。对于冲量梯度下降算法，其更新方程如下： 可以看到，参数更新时不仅考虑当前梯度值，而且加上了一个积累项（冲量），但多了一个超参，一般取接近1的值如0.9。相比原始梯度下降算法，冲量梯度下降算法有助于加速收敛。当梯度与冲量方向一致时，冲量项会增加，而相反时，冲量项减少，因此冲量梯度下降算法可以减少训练的震荡过程。 Nesterov Accelerated Gradient (NAG)NAG算法是Yurii Nesterov在1983年提出的对冲量梯度下降算法的改进版本，其速度更快。其变化之处在于计算“超前梯度”更新冲量项，具体公式如下： 学习率自适应的梯度下降AdaGradAdaGrad是Duchi在2011年提出的一种学习速率自适应的梯度下降算法。在训练迭代过程，其学习速率是逐渐衰减的，经常更新的参数其学习速率衰减更快，这是一种自适应算法。 其更新过程如下： 把每一维度的梯度^2和记录下来，每次学习率都除以这个和 每一维度的学习率不一样，且都在不断减小 在梯度大的维度，减小下降速度；在梯度小的维度，加快下降速度 让学习率适应参数，对于出现次数较少的特征，我们对其采用更大的学习率，对于出现次数较多的特征，我们对其采用较小的学习率。因此，Adagrad非常适合处理稀疏数据。 Adagrad算法的一个主要优点是无需手动调整学习率 Adagrad的一个主要缺点是它在分母中累加梯度的平方：由于每增加一个正项，在整个训练过程中，累加的和会持续增长。这会导致学习率变小以至于最终变得无限小，在学习率无限小时，Adagrad算法将无法取得额外的信息。 RMSpropRMSprop是Hinton在他的课程上讲到的，其算是对Adagrad算法的改进，主要是解决学习速率过快衰减的问题。其实思路很简单，类似Momentum思想，引入一个超参数，在积累梯度平方项进行衰减： 此时可以看到s是梯度平方的指数加权移动平均值，其中\gamma一般取值0.9，此时s更平稳，减少了出现的爆炸情况，因此有助于避免学习速率很快下降的问题。同时Hinton也建议学习速率设置为0.001。 Adaptive moment estimation (Adam)Adam是Kingma等在2015年提出的一种新的优化算法，其结合了Momentum和RMSprop算法的思想。相比Momentum算法，其学习速率是自适应的，而相比RMSprop，其增加了冲量项。所以，Adam是两者的结合体： 可以看到前两项和Momentum和RMSprop是非常一致的， 由于和的初始值一般设置为0，在训练初期其可能较小，第三和第四项主要是为了放大它们。最后一项是参数更新。其中超参数的建议值是 总结本文沿着梯度下降的发展大致介绍了各种常用的梯度下降算法，目前比较常用的应该仍是 Adam ， 不过我感觉其实 SGD 加梯度衰减策略可能能取得更好的效果，当然这需要设置得比较合适。 彩蛋]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[opensmile 工具的使用和批处理]]></title>
    <url>%2F2020%2F05%2F02%2Fopensmile%2F</url>
    <content type="text"><![CDATA[前言openSMILE是一款以命令行形式运行的工具，通过配置config文件来提取音频特征。主要应用于语音识别、情感计算、音乐信息获取。2.0版本之后的openSMILE包括了openCV库，可以用于视频处理和视频特征提取。官网下载有linux和windows版本提供下载，windows可以不编译直接用，建议在命令行里指明 openSMILE 绝对路径。 openSMILE的输入输出格式文件输入格式 RIFF-WAVE (PCM) (for MP3, MP4, OGG, etc. a converter needs to be used) Comma Separated Value (CSV) HTK parameter files WEKA’s ARFF format.（由htk工具产生） Video streams via openCV.（opencv产生的视频流数据） 文件输出格式 RIFF-WAVE (PCM uncompressed audio) Comma Separated Value (CSV) HTK parameter file WEKA ARFF file LibSVM feature file format Binary float matrix format 分类器和其他组件openSMILE还提供了许多VAD算法，用于判断各时间点有没有说话。 Voice Activity Detection based on Fuzzy Logic Voice Activity Detection based on LSTM-RNN with pre-trained models Turn-/Speech-segment detector LibSVM (on-line) LSTM-RNN (Neural Network) classifier which can load RNNLIB and CURRENNT nets GMM (experimental implementation from eNTERFACE’12 project, to be release soon) SVM sink (for loading linear kernel WEKA SMO models) Speech Emotion recognition pre-trained models (openEAR) openSMILE使用流程简介 先切换到处理文件SMILExtract.exe所在的目录 通过如下语句提取：windows下：SMILExtract_Release -C “配置文件” -I “要处理的音频” -O “要保存特征向量的路径及文件名”linux下：SMILExtract -C “配置文件” -I “要处理的音频” -O “要保存特征向量的路径及文件名” 官方配置文件官方提供了许多常见特征集的配置文件，如MFCC，PLP，以及各大语音比赛中效果好的特征集。 MFCC特征为了提取MFCC特征（兼容HTK），提供了以下四个文件（它们是以它们所代表的相应的HTK参数类型命名的）：MFCC12_0_D_A.conf此配置从25毫秒的音频帧中提取梅尔频率倒谱系数（以10毫秒的速率采样）（汉明窗口）。 它由26个Mel频带计算13个MFCC（0-12）组，并应用了一个权重参数为22的倒谱提升滤波器。13个一阶和13个二阶系数被附加到MFCC后。MFCC12_E_D_A.conf此配置跟MFCC12_0_D_A.conf一样，但对数能量是只加在MFCC1-12上。MFCC12_0_D_A_Z.conf这个配置跟MFCC12_0_D_A.conf配置一样，除了所有特征是参考整个输入序列进行了标准化。MFCC12_E_D_A_Z.conf这个配置跟MFCC12_E_D_A.conf配置一样，除了所有特征是参考整个输入序列进行了标准化。帧长为25ms,帧移为10ms，使用的汉明窗，预增强参数为0.97。由26个通过FFT功率谱计算的mel-滤波器组计算MFCC 0/1-12。MEL频谱的频率范围为0-8kHz，同时这些配置文件提供了-I,-O选项。输出文件格式是HTK参数文件格式。如果需要输出其他文件格式，你必须在配置文件中更改‘cHtkSink’组件类型为你想要的类型。命令行示例如下： SMILExtract -C config/MFCC12_E_D_A.conf -I input.wav -O output.mfcc.htk PLP特征用于提取PLP倒谱系数（PLP-CC）（与HTK兼容）以下四个文件（它们是以它们所代表的相应的HTK参数类型命名的）：PLP_0_D_A.conf该配置从25 ms长音频（以10ms的速率采样）帧提取Mel频率倒谱系数（汉明窗口）。它从26个Mel频带，并使用预测阶数为5计算6个PLP（0-5），并应用了一个权重参数为22的倒谱提升滤波器。6个一阶和6个二阶系数被附加到PLP-CC后。PLP_E_D_A.conf该配置与PLP_0_D_A.conf相同，但对数能量是只加在PLP1-12上。PLP_0_D_A_Z.conf此配置与PLP_0_D_A.conf相同，除了所有特征是参考整个输入序列进行了标准化。PLP_E_D_Z.conf此配置与PLP_E_D_A.conf相同，除了所有特征是参考整个输入序列进行了标准化。帧长为25ms,帧移为10ms，使用的汉明窗，预增强参数为0.97。由26个通过FFT功率谱计算的听觉mel-滤波器组(压缩系数为0.33)计算PLP 0/1-5。线性预测器的预测阶数为5。MEL频谱的频率范围为0-8kHz，同时这些配置文件提供了-I,-O选项。输出文件格式是HTK参数文件格式。如果需要输出其他文件格式，你必须在配置文件中更改‘cHtkSink’组件类型为你想要的类型。命令行示例如下： SMILExtract -C config/PLP_E_D_A.conf -I input.wav -O output.plp.htk 情感特征集自openSMILE在openEAR的项目EWS09情感识别中被使用，openSMILE提供了各种情感识别的标准特征集。The INTERSPEECH 2009 Emotion Challenge feature set（参见[SSB09]）由配置文件config/emo IS09.conf提供。它包含对LLDs应用统计函数得到的384个特征。该特征被保存在Arff格式（针对WEKA），新的实例会被附加到一个已存在文件（这是用于批处理，其中openSMILE被反复调用从多个文件提取特征到单个特征文件）。 出现在Arff文件中16个低级描述符（LLDs）的名称，见下面的列表： pcm_RMSenergy 信号帧均方根能量 mfcc 梅尔频率倒谱系数1-12 Pcm_zcr 时间信号的过零率（基于帧） voiceProb 从ACF计算的发声概率。 F0 从倒谱计算的基频 附加到低级描述符名称的后缀_sma表示它们是通过窗口长度为3的移动平均滤波器进行平滑。附加到sma的后缀_de表示当前特征是低级描述符平滑后的一阶delta系数（微分）。 max 轮廓的最大值 min 轮廓的最小值 range = max- min maxPos 最大值的绝对位置（以帧为单位） minPos 最小值的绝对位置（以帧为单位） amean 轮廓的算术平均值 linregc1 轮廓线性逼近的斜率（m） linregc2 轮廓线性逼近的偏移量（t） linregerrQ 计算的二次误差作为线性近似值和实际轮廓的差值 stddev 轮廓上的值的标准偏差 skewness 偏度（3阶矩） kurtosis 峰度（4阶矩） The INTERSPEECH 2010 Paralinguistic Challenge feature set（见2010年INTERSPEECH会议论文集）由配置文件config/IS10_paraling.conf提供。该集包含的1582个特征是由34个低级描述符（LLDs）和34个相应的delta作为68个LLDs轮廓值，在此基础上应用21个函数得到1428个特征，另外，对4个基于音高的LLD及其4个delta系数应用了19个函数得到152个特征，最后附加音高（伪音节）的数量和总数输入的持续时间（2个特征）。该特征被保存在Arff格式（针对WEKA），新的实例会被附加到一个已存在文件（这是用于批处理，其中openSMILE被反复调用从多个文件提取特征到单个特征文件）。 出现在Arff文件中34个低级描述符（LLDs）的名称，见下面的列表： pcm_loudness 归一化强度提高到0.3的幂的响度 mfcc 美尔频率倒谱系数0-14 logMelFreqBand 梅尔频带的对数功率0-7（分布范围内从0到8 kHz） lspFreq 从8个LPC系数计算出的8个线谱对频率。 F0finEnv 平滑的基频轮廓线。 voicingFinalUnclipped 最终基频候选的发声概率。Unclipped的意思是，当其低于浊音阈值时，它不被设置为零。 附加到低级描述符名称的后缀_sma表示它们是通过窗口长度为3的移动平均滤波器进行平滑。附加到sma的后缀_de表示当前特征是低级描述符平滑后的一阶delta系数（微分）。出现在Arff文件中的21个函数的名字,均在以下列表中： maxPos 最大值的绝对位置（以帧为单位） minPos 最小值的绝对位置（以帧为单位） amean 轮廓的算术平均值 linregc1 轮廓线性逼近的斜率（m） linregc2 轮廓线性逼近的偏移量（t） linregerrA 把线性误差计算作为线性近似值和实际的轮廓的误差 linregerrQ 把二次误差计算作为线性近似值和实际的轮廓的误差 stddev 轮廓中的值的标准偏差 skewness 偏度（3阶矩）。 kurtosis 峰度（4阶矩）。 quartile1 第一四分位数（25％百分位数） quartile2 第一四分位数（50％百分位数） quartile3 第一四分位数（75％百分位数） iqr1-2 四分位数间距：quartile2- quartile1 iqr2-3 四分位数间距：quartile3- quartile2 iqr1-3 四分位数间距：quartile3- quartile1 percentile1.0 轮廓的离群值鲁棒最小值，按1％百分位数表示。 percentile99.0 轮廓的离群值鲁棒最大值，按99％百分位数表示。 pctlrange0-1 由1％和99％的百分点的范围表示的离群值鲁棒信号范围“max-min”。 upleveltime75 信号超过（75％*范围+min）的时间百分比。 upleveltime90 信号超过（90％*范围+min）的时间百分比。 四个音高相关的LLD（及相应的delta系数）如下（清音区域均为0，因此功能仅适用于这些轮廓的浊音区域）： F0final 平滑的基频频率 jitterLocal 本地（帧到帧）抖动（音调周期长度偏差） jitterDDP 差分帧间抖动（‘Jitter of the Jitter’） shimmerLocal 本地（帧到帧）闪烁（音调周期幅度偏差） 对这4 + 4个LLD应用了19个函数，即上述21个函数的集合没有最小值（1％百分位数）和范围。 The INTERSPEECH 2011 Speaker State Challenge feature set（见2011年INTERSPEECH会议论文集）由配置文件config/IS11_speake_state.conf提供。该集包含的4368个特征是由4个能量相关+50个频谱相关的低级描述符（LLDs）和54个相应的delta作为108个LLDs，在此基础上应用33个基本函数+平均值、最小值、最大值、标准差得到3996个特征；5个声音相关和5个对应的delta作为10个LLDs，在此基础上应用33个基本函数+二次平均、上升时长、下降时长得到360个特征；6个F0基本函数和对应的delta，12个特征。 The INTERSPEECH 2012 Speaker Trait Challenge feature set（见2012年INTERSPEECH会议论文集）由配置文件config/IS12_speake_trait.conf提供。该集包含的6125个特征。 The INTERSPEECH 2013 ComParE Challenge feature set （见2013年INTERSPEECH会议论文集）由配置文件config/IS13_ComParE.conf提供。该集包含的6373个特征，LLD包括能量，频谱，倒谱（MFCC）、声音、对数谐波噪声比（HNR），频谱谐度和心理声学频谱清晰度。 The MediaEval 2012 TUM feature set for violent video scenes detection 针对好莱坞流行电影的暴力进行检测的特征集在config/mediaeval2012_tum_affect/，里面有不同的设置，参考文章：Florian Eyben, Felix Weninger, Nicolas Lehment, Gerhard Rigoll, Björn Schuller: ”Violent Scenes Detection with Large, Brute-forced Acoustic and Visual Feature Sets”, Proc. MediaEval 2012 Workshop, Pisa, Italy, 04.-05.10.2012. MediaEval Audio IS12based subwin2.conf包含的是从2s的子窗中提取音频特征的配置。MediaEval Audio IS12based subwin2 step0.5.conf提取一样的特征，但是2s子窗的偏移为0.5s。MediaEval VideoFunctionals.conf用于视频特征提取，如文章使用方法，需要一个包含LLDs的CSV文件（由openCV提取）作为输入和输出，ARFF文件作为视频特征。 The openSMILE/openEAR ‘emobase’ set早期的基线集（参照”emobase2”集作为新的基线集），拥有情感识别的998个声学特征，包含以下低级描述符（LLDs）：强度，响度，12 MFCC，音高（F0），浊音概率，F0包络线，8 LSF（线频谱频率），过零率， 以及这些LLD的Delta回归系数。以下函数被应用于上述LLDs及其Delta系数。：Max./Min。输入的相对位置和范围，范围，算术平均值，2线性回归系数，线性和二次误差，标准差，偏度，峰度，四分位数1-3和三位四分位数范围。 The large openSMILE emotion feature set用于提取更多的LLDs和更多的函数(6552个特征)，配置文件为config/emo_large.conf。 The openSMILE ‘emobase2010’ reference set 是基于the INTERSPEECH 2010 Paralinguistic Challenge feature set，配置文件为config/emobase2010.conf。对持续时间和位置特征的规范化进行了一些调整。这个特性集包含了一套大大增强的低级描述符(LLDs)，以及一套“emobase”相比更加精细化选择的函数列表。建议使用此特征集作为比较新的情感识别特征集和方法的参考，因为它代表当前最先进的情感和语言识别功能。该集合包含1582个特征（与INTERSPEECH 2010 Paralinguistic 挑战集相同设置），其由34个低级描述符（LLDs）和34个相应的delta作为68个LLDs轮廓值，在此基础上应用21个函数得到1 428个特征，另外，对4个基于音高的LLD及其4个delta系数应用了19个函数得到152个特征，最后附加音高（伪音节）的数量和总数输入的持续时间（2个特征）。唯一的区别是INTERSPEECH 2010 paralinguistic挑战集标准化的是是“maxPos”和“minPos”特征，本配置被标准化为段长度。 python批处理提取openSMILE特征所有支持标准数据输出格式的配置文件都可以在WINDOWS的批特征提取GUI（使用VS10 C#编写，位于progsrc/openSMILEbatchGUI/）。这个工具允许openSMILE自动的执行文件夹中的若干文件。它可以在图形界面中选择音频文件和指定输出类型。openSMILE本身提供批处理GUI（使用VS10 C#编写，位于progsrc/openSMILEbatchGUI/），但若语音数据的目录结构较复杂，还可以利用python来进行批处理。示例代码如以下： import os from subprocess import call def excute_CMD(path_ExcuteFile, path_Config, path_Audio, path_Output): cmd = path_ExcuteFile + &quot; -C &quot; + path_Config + &quot; -I &quot; + path_Audio + &quot; -O &quot; + path_Output call(cmd, shell=True) def batch_extract_features(path_Config, path_Input_Root, path_Output): path_ExcuteFile = &quot;SMILExtract_Release&quot; filename = os.listdir(path_Input_Root) for i in range(len(filename)): print(&#39;Extracting features of %s&#39; % filename[i]) path_Input = path_Input_Root + &#39;/&#39; + filename[i] + &#39;.wav&#39; excute_CMD(path_ExcuteFile, path_Config, path_Input, path_Output) path_Config = &quot;./config/IS13_ComParE.conf&quot; path_Input_Root = &#39;root_path_to_audio/&#39; path_Output = &#39;features.csv&#39; batch_extract_features(path_Config, path_Input_Root, path_Output) 输出数据格式控制对于不包含统计函数的配置文件，选项定义在config/shared/standard_data_output_lldonly.conf.inc ==============================LLD only============================= ================================CSV================================ -csvoutput &lt;filename&gt; 默认输出选项. CSV格式，存放帧向LLD -appendcsv &lt;0/1&gt; 设为1代表添加到已有CSV文件文末，默认0 -timestampcsv &lt;0/1&gt; 设为0禁止把时间步输出到CSV第二列，默认为1 -headercsv &lt;0/1&gt; 设为0禁止把标题输入到CSV，默认为1 ================================HTK================================ -output &lt;filename&gt; 输出特征汇总（函数）到HTK格式文件 ================================ARFF=============================== -arffoutput &lt;filename&gt; 默认输出选项. ARFF格式，存放帧向LLD -appendarff &lt;0/1&gt; 设为0代表不添加到已有ARFF文件文末，默认1添加 -timestamparff &lt;0/1&gt; 设为0禁止把时间步输出到ARFF第二列，默认为1 arfftargetsfile &lt;file&gt;指定配置包含定义目标域（类）的文，默认为:shared/arff_targets_conf.inc 对于包含统计函数的配置文件，如全部的INTERSPEECH和AVEC挑战集，选项定义在config/shared/standard_data_output.conf.inc =============================LLD and func ========================= -instname &lt;string&gt; 通常是输入文件的名称保存在CSV和ARFF输出的首列。默认是&quot;unknow&quot; ================================ARFF=============================== -lldarffoutput, -D &lt;filename&gt; 启动LLD帧向输出到ARFF格式文件 -appendarfflld &lt;0/1&gt; 设为1代表添加到已有ARFF文件文末，默认0覆盖 -timestamparfflld &lt;0/1&gt; 设为0禁止把时间步输出到ARFF第二列，默认为1 -lldarfftargetsfile &lt;file&gt; 指定配置包含定义目标域（类）的文，默认为: shared/arff_targets_conf.inc ================================CSV================================ -lldcsvoutput, -D &lt;filename&gt; 启动LLD帧向输出到CSV格式文件 -appendcsvlld &lt;0/1&gt; 设为1代表添加到已有CSV文件文末，默认0覆盖 -timestampcsvlld &lt;0/1&gt; 设为0禁止把时间步输出到CSV第二列，默认为1 -headercsvlld &lt;0/1&gt; 设为0禁止把标题输入到CSV，默认为1 ================================HTK================================ -lldhtkoutput &lt;filename&gt; 启动LLD帧向输出到HTK格式文件 ================================ARFF=============================== -output, -O &lt;filename&gt; 默认输出选项. ARFF格式，存放特征汇总 -appendarff &lt;0/1&gt; 设为0代表不添加到已有ARFF文件文末，默认1添加 -timestamparff &lt;0/1&gt; 设为1把时间步输出到ARFF第二列，默认为0 -arfftargetsfile &lt;file&gt;指定配置包含定义目标域（类）的文，默认为: shared/arff_targets_conf.inc ================================CSV================================ -csvoutput &lt;filename&gt; 默认输出选项. CSV格式，存放特征汇总 -appendcsv &lt;0/1&gt; 设为0代表不添加到已有CSV文件文末，默认1 -timestampcsv &lt;0/1&gt; 设为0禁止把时间步输出到CSV第二列，默认为1 -headercsv &lt;0/1&gt; 设为0禁止把标题输入到CSV，默认为1 ================================HTK================================ -htkoutput &lt;filename&gt; 输出特征汇总（函数）到HTK格式文件 如下为lldcsvoutput的定义。注：从2.2版本起，可以指定一个“?”替代文件名。它会禁止相应的输出组件，即它不会产生输出文件，在标准输出接口界面，看到的所有的文件名默认都是”?” [lldsink:cCsvSink] reader.dmLevel = lld;lld_de filename=\cm[lldcsvoutput(D){?}:output csv file for LLD, disabled by default ?, only written if filename given] instanceName=\cm[instname(N){unknown}:instance name] append = \cm[appendcsvlld{0}:set to 1 to append to the LLD output csv file, default is not to append] timestamp = \cm[timestampcsvlld{1}:set to 0 to suppress timestamp column, default is 1, i.e. to show timestamp in second column] number = 0 printHeader = \cm[headercsvlld{1}:set to 0 to suppress header line with feature names, default is 1, i.e. to show header line] errorOnNoOutput = 1 那么，当需要同时输出lld和func时，可用如下命令 SMILExtract -C config/IS13_ComParE.conf -I input.wav -lldcsvoutput lld_output.csv -csvoutput func_output.csv 最后一点话其实如果只是用官方配置提特征那么只看批处理那里也够了。官方配置文件可以根据需求时再看需要哪个文件，也可自己按着这个格式自定义编写配置文件。另外输出格式控制感觉最好也是先看一下，我一开始都是直接用 -O 输出统计特征，但想输出lld时跑去源代码里一阵捣鼓，后来才发现它已经封装好了直接一个参数就可以了。 彩蛋]]></content>
      <categories>
        <category>语音特征</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[语谱图的matlab提取和python提取]]></title>
    <url>%2F2020%2F05%2F02%2Fspecgram%2F</url>
    <content type="text"><![CDATA[前言语谱图（spectrogram或specgram），也叫声谱图，可以简单看做一个二维矩阵，其纵轴表示频率，横轴表示时间，矩阵的值表示能量强弱。由于它拥有着频率和时间两个维度的信息，所以是比较综合地表示原语音信息的一种特征。另外，我将其看做语音和图像的一种连接，因为图像领域的模型发展得较快，所以通过这种方式把语音转换成一种特殊的图像再进一步处理。 语谱图流程简介1. 将语音可交叉地分成多帧（由于语音的短时平稳性） 2. 各帧加窗 3. 各帧通过快速傅里叶变化（fft）得到频谱向量 4. 沿着时间轴并联各频谱向量得到语谱图 语谱图的提取语谱图的matlab提取先看一段非官方代码，结合上述步骤进行理解。 [x,Fs,nBits]=wavread(&#39;audio.wav&#39;); s=length(x); % 信号长度 w=256; % 窗长 n=w; % nfft，表示做fft变换需要的点数，一般为刚大于w的2的幂。举例，w=250，则n一般设为256 ov=w/2; % 分帧的交叉程度，常见设为窗长的二分之一或四分之一 h=w-ov; % 不重叠点数 win=hamming(n)&#39;;% 选了常见的汉明窗，并设置nfft c=1; % 指向当前帧的指针 ncols=1+fix((s-n)/h); % 计算总共有多少帧 d=zeros((1+n/2),ncols); % 语谱图初始化 for b=0:h:(s-n) % 以下处理各帧 u=win.*x((b+1):(b+n)); % 各帧加窗 t=fft(u,n); % 各帧进行fft，内容为u，nfft=n。对于fft，输入n个时域点，输出n个频域点 d(:,c)=t(1:(1+n/2))&#39;; % 并联频谱向量，注意只取1+n/2，因为负频率无意义，只留下0和正频率 c=c+1; % 移动指针 end tt=[0:h:(s-n)]/Fs; % 时间轴 ff=[0:(n/2)]*Fs/n; % 频率轴 imagesc(tt/1000,ff/1000,20*log10(abs(d))); % 绘制 colormap(hot); axis xy xlabel(&#39;时间/s&#39;); ylabel(&#39;频率/kHz&#39;); 然而，matlab其实有封装好的函数可以直接调用。 [S,F,T]=specgram(x,nfft,Fs,windows_length,overlap_length) % x 为整段语音 % nfft 为fft变换点数，其实可以直接用默认的刚大于窗长的2的幂。也可自定义为大于窗长的整数，会对帧进行补零操作 % Fs 语音采样频率 % windows_length 窗长 % overlap_length 交叉长度 % S 语谱图 % F 频率值，尺度为1+n/2 % T 时间值，尺度为1+fix((s-n)/h) 语谱图的python提取有了刚才的基础，python的代码就容易理解啦。首先同样看一下不直接调用函数的写法。 import numpy as np from scipy.io import wavfile import matplotlib.pyplot as plt Fs, x = wavfile.read(&#39;audio.wav&#39;) wave = np.array(x[:,0], dtype = &quot;float&quot;) frame_len = 1000 frame_off = frame_len // 2 # 非重叠点数 specg_len = 1024 # 可以想象1是代表第一帧，然后第二帧结尾超出第一帧frame_off个点，第三帧再超出第二帧frame_off个点，总共第二帧到最后一帧共有(wave.size - frame_len) // frame_off 帧 frame_num = (wave.size - frame_len) // frame_off + 1 # 生成汉明窗 hamwindow = np.hamming(frame_len) specg = np.zeros((frame_num, specg_len // 2 + 1)) z = np.zeros(specg_len - frame_len) for idx in range(frame_num): base = idx * frame_off frame = wave[base: base + frame_len] # 分帧 frame = np.append(frame * hamwindow, z) # 加窗 specg[idx:] = np.log10(np.abs(np.fft.rfft(frame))) # FFT，返回幅度谱 specg = np.transpose(specg) io.savemat(&#39;specgram.mat&#39;, {&#39;specg&#39;:specg}) # aspect设为auto即可自动拉宽图 plt.imshow(specg, origin=&quot;lower&quot;, cmap = &quot;jet&quot;, aspect = &quot;auto&quot;, interpolation = &quot;none&quot;) plt.show() plt.xticks([]) plt.yticks([]) plt.savefig(&#39;specgram.png&#39;,bbox_inches=&#39;tight&#39;,pad_inches=0.0) plt.close() 再看看已经封装好的版本。 from scipy import io from scipy.io import wavfile import matplotlib.pyplot as plt Fs, x = wavfile.read(&#39;audio.wav&#39;) # 读取音频 specg = plt.specgram(x, Fs = Fs, pad_to = 256, NFFT = 256, noverlap = 128) # 提取语谱图，一键操作！ io.savemat(&#39;specgram.mat&#39;, {&#39;specg&#39;:specg[0]}) # 保存语谱图 ## 照例解释下参数 # x，Fs和上边一样 # pad_to为上边的nfft # NFFT为上边的windows_length（为什么nfft不设置为上边的nfft呢，迷惑） # noverlap为上边的overlap_length 补充一个librosa版本librosa提取的是梅尔频谱图，即在频谱图基础上再进一步将各帧通过梅尔滤波器(还可加对数操作)。另外若是在此基础上再进行倒谱即获得MFCC。还要注意到，梅尔频谱图的输出尺寸，频率等于梅尔滤波器的个数n_mels, 时间则只取决于窗移(非重叠数)hop_length(还没想明白，推测可能是进行了填充，所以尺寸上忽视了窗长的影响)。此外，还可通过设置power参数来确定要计算梅尔频谱图(设置为1)还是梅尔功率图(设置为2)。 from matplotlib import pyplot as plt import librosa import librosa.display # Load a wav file y, sr = librosa.load(&#39;./test.wav&#39;, sr=None) # plot a wavform plt.figure() librosa.display.waveplot(y, sr) # plt.plot(y) plt.title(&#39;wavform&#39;) plt.show() # extract mel spectrogram feature melspec = librosa.feature.melspectrogram(y, sr, n_fft=1024, win_length=1024, hop_length=512, n_mels=128, power=2.0) # convert to log scale logmelspec = librosa.power_to_db(melspec) # plot mel spectrogram plt.figure() librosa.display.specshow(logmelspec, sr=sr, x_axis=&#39;time&#39;, y_axis=&#39;mel&#39;) plt.title(&#39;spectrogram&#39;) plt.show() # aspect设为auto即可自动拉宽图 plt.imshow(logmelspec, origin=&quot;lower&quot;, cmap = &quot;jet&quot;, aspect = &quot;auto&quot;, interpolation = &quot;none&quot;) plt.show() plt.xticks([]) plt.yticks([]) plt.savefig(&#39;specgram.png&#39;,bbox_inches=&#39;tight&#39;,pad_inches=0.0) plt.close() 下图中，第一张是梅尔频谱图，第二张是梅尔功率图，功率图的声音和噪声区分更明显。而两者都比没有梅尔滤波器的频谱图有更独特明显的能量显示。 语谱图的一些可能有的小疑惑 关于nfftnfft既表示时域的点数也关联频域的点数。该数为2的幂数时更高效，但不是也没问题。nfft需要比窗长的值更大，然后加窗后的帧会被补零到nfft长度再进行fft。 关于频率分辨率频率轴上每一个点对应fs/nfft的频率。另外由于输出nfft/2+1个频率点，所以输出的频率范围为0到nfft/2×fs/nfft=fs/2。 关于自定义输出语谱图的尺寸问题时间轴尺寸为1+fix((s-n)/h)， 由windows_length和overlap_length决定。实际应用时由于各语音长度不同，时间尺寸一般都要进行截断或补零到一个固定值。截断的话可以截一段（起始信息，中间信息），也可以截多段（交叉不交叉都行）。频率轴尺寸为1+n/2，仅决定于nfft（python中的pad_to参数），所以可以通过设置该值控制频率轴尺寸。但是也不要比窗长大太多，否则补零太多可能就没什么信息了。nfft调大时，窗长可以跟着调大，为了防止导致的时间轴太短可以调高overlap_length。另外，其他参数不变时，仅变换nfft，可视化出来时可能肉眼看起来一样，但实际分辨率仍然是不同的。这也导致了一个问题，送入网络的是要用单通道的直接计算出来的语谱图，还是用可视化函数绘制出来的三通道的语谱图，这就根据实际情况去尝试了。 彩蛋 希望疫情早点过去]]></content>
      <categories>
        <category>语音特征</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[卷积当中的补零操作]]></title>
    <url>%2F2020%2F05%2F02%2Fpadding%2F</url>
    <content type="text"><![CDATA[卷积输入输出尺寸公式 这是一条比较完整的输出尺寸公式，考虑到了stride, padding, dilation, 这里的括号表示的是向下取整，这实际上是卷积图的边边剩下的部分比卷积核小，所以抛弃了这次卷积的结果。其实，若用另一种视角看，可以把空洞卷积当成更改了卷积核尺寸K的值，K -&gt; d × (K-1) +1，因此该公式可以更简洁的被表示为Out = floor((In + 2P − K)/S+1) tensorflow 版本tensorflow 版本的padding是通过直接选模式参数进行的，可选’SAME’,’VALID’.前者是通过padding在前后左右补零，使得输出尺寸保持不变（或以步长倍数缩小），非常常用，后者则是不进行padding，实际上应该是等价于上边总公式P=0的情况。SAME: Out = ceil(In/S)VALID: Out = ceil((In − K + 1)/S) pytorch 版本pytorch中padding是需要自己设置值的，因此输出尺寸就按照一开始的公式来即可。以下再看一些例子。 &gt;&gt;&gt; m = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3) &gt;&gt;&gt; input = torch.randn(20,3,24,24) &gt;&gt;&gt; m(input).shape torch.Size([20, 64, 12, 12]) &gt;&gt;&gt; input = torch.randn(20,3,25,25) &gt;&gt;&gt; m(input).shape torch.Size([20, 64, 13, 13]) &gt;&gt;&gt; input = torch.randn(20,3,24,24) &gt;&gt;&gt; m = nn.Conv2d(3, 64, kernel_size=6, stride=2, padding=3) &gt;&gt;&gt; m(input).shape torch.Size([20, 64, 13, 13]) &gt;&gt;&gt; m = nn.Conv2d(3, 64, kernel_size=6, stride=2, padding=2) &gt;&gt;&gt; m(input).shape torch.Size([20, 64, 12, 12]) &gt;&gt;&gt; input = torch.randn(20,3,25,25) &gt;&gt;&gt; m = nn.Conv2d(3, 64, kernel_size=6, stride=2, padding=3) &gt;&gt;&gt; m(input).shape torch.Size([20, 64, 13, 13]) &gt;&gt;&gt; m = nn.Conv2d(3, 64, kernel_size=6, stride=2, padding=2) &gt;&gt;&gt; m(input).shape torch.Size([20, 64, 12, 12]) 这里可以观察到： 若是需要保持 对于奇数卷积核，通过让padding=(k-1)/2，即可实现SAME的效果。即输出尺寸保持不变或以步长倍数缩小（对于奇数输入尺寸则向上取整） 对于偶数卷积核，若是偶数输入尺寸，则padding=floor((k-1)/2)可实现SAME的效果 对于偶数卷积核，若是奇数输入尺寸，则padding=ceil((k-1)/2)可实现SAME的效果 因此，建议不要用偶数卷积核。。。然后记住padding=(k-1)/2，即可实现tf中SAME的效果了。把padding带入一开始的总公式，可以得到Out = floor((In − 1)/S+1)，这实际上与SAME公式等效，可以看下边代码的暴力验证。 &gt;&gt;&gt; a = lambda x:np.ceil(x/5) &gt;&gt;&gt; b = lambda x:np.floor((x-1)/5+1) &gt;&gt;&gt; c = [np.random.randint(6,100) for i in range(10)] &gt;&gt;&gt; [a(rand)==b(rand) for rand in c] [True, True, True, True, True, True, True, True, True, True] 总结虽然想了半天这个公式，但实际使用时好像也就SAME功能用得比较多，因此记住核心： 用奇数卷积核 padding=(k-1)/2 若是空洞卷积，则代入K -&gt; d × (K-1) +1]]></content>
      <tags>
        <tag>卷积</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Github Pages和Hexo搭建自己的独立博客]]></title>
    <url>%2F2019%2F08%2F11%2FHow-to-use-Hexo-to-build-your-blog%2F</url>
    <content type="text"><![CDATA[前言 Github Pages: Github Pages可以被认为是用户编写的、托管在github上的静态网页。 Hexo: Hexo是一个快速、简洁且高效的博客框架。 安装Node.js点击此处访问官网，按需下载相应版本，默认安装可以了。 安装Hexo$ sudo npm install -g hexo-cli 若报错，尝试： $ sudo npm install --unsafe-perm --verbose -g hexo 初始化，建立博客项目选定博客网站项目程序文件的存放位置，如/Users/tobefans/Documents/Blog/，Bash中cd进入该目录下，执行命令： $ hexo init 执行完毕后，该命令将在当前目录下生成一套标准的Hexo博客项目模板 命令$ hexo g 生成静态网站文件 $ hexo s 启动本地服务器 $ hexo d 发布博客到GitHub 发布博客创建github.io仓库在自己的GitHub中，创建新仓库，标准命名为GitHub用户名.github.io，例如我的：tobefans.github.io 配置SSH密钥只有配置好 SSH 密钥后，我们才可以通过 git 操作实现本地代码库与 Github 代码库同步，在你第一次新建的文件夹里输入以下命令： $ ssh-keygen -t rsa -C &quot;your email@example.com&quot; //引号里面填写你的邮箱地址，比如我的是weiquan.fan96@gmail.com 之后复制~/.ssh/id_rsa.pub的公钥，在Github账号中添加进该公钥。 Git 会根据用户的名字和邮箱来记录提交，GitHub 也是用这些信息来做权限的处理，输入以下命令进行个人信息的设置，把名称和邮箱替换成你自己的，名字可以不是 GitHub 的昵称，但为了方便记忆，建议与 GitHub 一致。 $ git config --global user.name &quot;此处填你的用户名&quot; $ git config --global user.email &quot;此处填你的邮箱&quot; 可通过ssh -T git@github.com测试是否添加成功。 将本地的 Hexo 文件更新到 Github 的库中打开创建的 Hexo 文件夹下的 _config.yml，修改如下的代码段，repo为github项目的地址。 deploy: type: git repo: git@github.com:tobefans/tobefans.github.io.git branch: master 如果此时报以下错误，说明你的 deployer 没有安装成功 ERROR Deployer not found: git 需要执行以下命令再安装一次： $ npm install hexo-deployer-git --save 再执行 hexo g -d，你的博客就会部署到 Github 上了 访问博客在github该项目的setting中打开Github pages的设置。 你的博客地址：https://你的用户名.http://github.io，比如我的是：https://tobefans.github.io ，现在每个人都可以通过此链接访问你的博客了。 为博客更换自己喜欢的主题点击此处进入 Hexo 官网的主题专栏，我们可以看见有许多的主题供我们选择。我们要做的就是把主题克隆过来，在此我们以主题 Aero-Dual 为例，点进去我们就可以看见该主题作者的博客，鼠标滑到底，我们可以看见 Theme By Levblanc 的字样（其他主题类似），点击作者 Levblanc ，页面就会跳转到该主题所有的相关文件在 Github 上的地址，下载该项目，放至Hexo文件夹中的themes目录。 然后打开 Hexo 文件夹下的配置文件 _config.yml ，找到关键字 theme，修改参数为：theme：hexo-theme-aero-dual （其他主题修改成相应名称即可），再次注意冒号后面有一个空格。再通过$ hexo g更新。 另推荐一款主题：Next 在Hexo中渲染MathJax数学公式更换Hexo的markdown渲染引擎，hexo-renderer-kramed引擎是在默认的渲染引擎hexo-renderer-marked的基础上修改了一些bug，两者比较接近，也比较轻量级。在Hexo文件夹位置命令： npm uninstall hexo-renderer-marked --save npm install hexo-renderer-kramed --save 接下来到博客根目录下，找到node_modules\kramed\lib\rules\inline.js，把第11行的escape变量的值做相应的修改，这一步是在原基础上取消了对\,{,}的转义(escape)。同时把第20行的em变量也要做相应的修改。 // escape: /^\\([\\`*{}\[\]()#$+\-.!_&gt;])/, escape: /^\\([`*\[\]()#$+\-.!_&gt;])/ // em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/ 进入到主题目录，找到_config.yml配置问题，把mathjax默认的false修改为true，具体如下： # MathJax Support mathjax: enable: true per_page: true 在文章的Front-matter里打开mathjax开关，如下： --- title: 使用Github Pages和Hexo搭建自己的独立博客 date: 2019-08-11 19:10:47 tags: mathjax: true --- 这里有常见数学公式。 绑定域名通过阿里云之类的获得域名后，先解析域名如下。 记录类型 主机记录 记录值 CNAME www 你的github用户名.github.io A @ 192.30.252.153 A @ 192.30.252.154 在Hexo文件夹的source文件夹中，创建一个CNAME文件，存储预备使用的个人域名，如：weiquanfan.xyz也可通过github项目上setting里映射个人域名。 清理Hexo缓存并更新Hexo静态网站 $ hexo clean &amp;&amp; hexo g $ hexo clean $ hexo g -d Valine——评论系统Valine 诞生于 2017 年 8 月 7 日，是一款基于 Leancloud 的快速、简洁且高效的无后端评论系统。 注册帐号创建应用:请先登录或注册 LeanCloud, 进入控制台后点击左下角创建应用，选用免费的开发版应用即可。 应用设置:进入应用。在 设置 -&gt; 安全中心 ，把文件上传、短信服务、推送服务、实时通信这几个服务全部关闭，因为用不到。在 Web 安全域名 里填入想要打开评论系统的域名，如weiquanfan.xyz在 设置 -&gt; 应用 Key 找到APP ID和APP Key。 配置 Hexo 参数:打开 Hexo 主题的配置文件_config.yml，搜索一下 Valine，打开enable，并填写APP ID 和 APP Key。 更新网站:运行hexo g -d推送到博客。 此外，还需要注意，如果博客还有除正文内容之外的页面存在，例如关于、分类、标签，要把他们的 Markdown 文件的 comments 属性设置为 false，否则这些页面在展示的时候也会有评论的功能出现。 --- title: 使用Github Pages和Hexo搭建自己的独立博客 date: 2019-08-11 19:10:47 tags: mathjax: true comments: false --- 统计阅读量阅读量分两种：不蒜子统计站点的总访问量，即统计浏览了多少次；有多少人访问，在footer显示。LeanCloud统计单篇博文的阅读量，即统计单篇博文的阅读量是多少。 单篇博文的阅读量通过Valine实现，其实和评论系统差不多。在 存储 中新建Class，Class名称必须为Counter。并相应更改主题配置文件的leancloud_visitors，打开enable，并填写APP ID 和 APP Key。 站点的总访问量通过不蒜子实现。找到站点的themes/next/layout/_partials/footer.swig文件。插入代码如下。 {% if theme.footer.theme.enable %} {# #}{{ __('footer.theme') }} &mdash; {# #}{# #}NexT.{{ theme.scheme }}{# #}{% if theme.footer.theme.version %} v{{ theme.version }}{% endif %}&lt;/div&gt; # 此位置插入以下代码 &lt;div&gt; &lt;script async src=&quot;https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js&quot;&gt;&lt;/script&gt; 本站总访问量 &lt;span id=&quot;busuanzi_value_site_pv&quot;&gt;&lt;/span&gt; 次&amp;nbsp&amp;nbsp&amp;nbsp 本站访客数&lt;span id=&quot;busuanzi_value_site_uv&quot;&gt;&lt;/span&gt;人次 &lt;/div&gt; {% endif %} 彩蛋 喜大普奔，完成啦！！]]></content>
  </entry>
</search>

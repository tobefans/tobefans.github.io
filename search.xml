<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[使用BeautifulSoup、requests和you_get爬虫下载B站视频]]></title>
    <url>%2F2020%2F05%2F19%2FBeautifulSoup%2F</url>
    <content type="text"><![CDATA[前言BeautifulSoup 是一个可以从HTML或XML文件中提取数据并解析的Python库， Requests 是一常用的可以获取和发送http的请求库， you_get 则是方便的下载各大网站的视频的命令行工具。整体流程上是，先用 Requests 请求获得网站源代码，再用 BeautifulSoup 解析网站并筛选出自己要的信息（如视频的url），最后用 you_get 下载。 例子以下代码实现的是下载B站电影。 #!/usr/bin/env python3 # -*- coding: utf-8 -*- &quot;&quot;&quot; Created on Sun Mar 15 12:06:21 2020 @author: weiquan fan &quot;&quot;&quot; from bs4 import BeautifulSoup as bs import requests,re,os def download(url, filename): path_root = &#39;./Videos&#39; os.system(&#39;you-get -o {} -O {} {}&#39;.format(path_root, filename, url)) url_base = &#39;https://www.bilibili.com/movie/?spm_id_from=333.851.b_62696c695f7265706f72745f6d6f766965.2&#39; response = requests.get(url_base) page = response.text soup = bs(page, &#39;html.parser&#39;) vids = soup.findAll(&#39;li&#39;,attrs={&#39;class&#39;:re.compile(&#39;video-item-biref.*?&#39;)})# bilibili video_urls = [] counter=1 if(vids): for v in vids: #v_link = v.find(&#39;a&#39;)[&#39;href&#39;] #v_name = v.find(&#39;img&#39;)[&#39;alt&#39;] print(v) v_link = v.find(&#39;a&#39;)[&#39;href&#39;] v_name = v.find(&#39;img&#39;)[&#39;alt&#39;] video_urls.append([v_link, v_name]) print(v_link,v_name) try: download(v_link, v_name) except Exception: print(&#39;can\&#39;t download &#39;+v_name+&#39; in &#39;+v_link) counter -= 1 counter += 1 if(counter&gt;15): break]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[CNN的进击之路——讲讲ResNet, Inception, ResNeXt和Densenet等常见网络]]></title>
    <url>%2F2020%2F05%2F13%2Fresnet%2F</url>
    <content type="text"><![CDATA[前言本文是一篇大杂烩，按照发布时间总结了CNN的一些常见网络。 AlexNetAlexNet来源于ImageNet Classification with Deep Convolutional Neural Networks。在ImageNet LSVRC-2010上以远超第二的准确率夺得了冠军，拉开了深度学习热潮的大幕。 模型结构： 模型特点： 提出了非线性激活函数ReLU (之前普遍使用Sigmoid或者tanh) 提出Dropout（每次迭代训练时随机删除一些神经元） 重叠池化（池化的时候，每次移动的步长小于池化的窗口长度） 数据扩充（水平翻转图像，从原始图像中随机裁剪、平移变换，颜色、光照变换） LRN归一化层（利用临近的数据做归一化） 多GPU实现（受当时GPU限制，在每个GPU中放置一半神经元，将网络分布在两个GPU上进行并行计） VGGVGG来源于Oxford的Visual Geometry Group的组提出的Very Deep Convolutional Networks for Large-Scale Image Recognition，在ILSVRC 2014获得亚军。 模型结构： 其中D、E列就是著名的VGG-16、VGG-19。 模型特点：使用了3个3x3卷积核来代替7x7卷积核，使用了2个3x3卷积核来代替5x5卷积核。因此模型结构很统一简洁（卷积核尺寸3x3和最大池化尺寸2x2），并不断加深网络。 GoogLeNet V1GoogLeNet V1来源于Going deeper with convolutions，在ILSVRC 2014获得冠军。 该网络的核心在于提出了Inception Module。该模块有4个分支，初始版本如下图左，包含三个不同尺度的卷积核层和一个最大池化层，并在输出通道维度上合并。由于5×5的计算量大，就进一步先通过1×1卷积降低维度再通过大卷积核。这里的最大池化也是重叠池化的，经padding后不会缩小特征图尺寸。 模型结构： 模型特点： 多尺度卷积的思想让网络变宽 提出1×1卷积 GoogLeNet V2GoogLeNet V2来源于Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift。该网络基于V1版本，吸收了VGG的分解操作，使用了2个3x3卷积核来代替5x5卷积核。 模型特点： 提出了著名的BN层。 另外，为了适配BN层，增大学习速率并加快学习衰减速度以适用BN规范化后的数据；去除Dropout并减轻L2正则（因BN已起到正则化的作用）；去除LRN；更彻底地对训练样本进行shuffle；减少数据增强过程中对数据的光学畸变（因为BN训练更快，每个样本被训练的次数更少，因此更真实的样本对训练更有帮助）。 GoogLeNet V3GoogLeNet V3来源于Rethinking the Inception Architecture for Computer Vision。该网络基于V2版本，进一步改进了Inception，将3x3分解成1x3和3x1。同理，nxn可以分解成1xn和nx1。 ResNetResNet来源于大神何凯明的Deep Residual Learning for Image Recognition，在ILSVRC和COCO 2015上都夺得了冠军，有着里程碑的意义。 深度模型当深度到了几十层之后，由于梯度消失或者爆炸的原因，就容易发生退化问题：网络深度增加时，网络准确度出现饱和，甚至出现下降。现在假设我们有一个浅层网络，我们想通过向上堆积新层来建立深层网络，一个极端情况是这些增加的层什么也不学习，仅仅复制浅层网络的特征，即这样新层是恒等映射（Identity mapping）。在这种情况下，深层网络应该至少和浅层网络性能一样，也不应该出现退化现象。这引发了残差学习，即我们的目标是学习到残差F(x)=H(x)-x，则该层学习到的最终特征H(x)=F(x)+x。当残差为0时，此时堆积层仅仅做了恒等映射，至少网络性能不会下降，实际上残差不会为0，这也会使得堆积层在输入特征基础上学习到新的特征，从而拥有更好的性能。残差学习的结构下图所示。这有点类似与电路中的“短路”，所以是一种短路连接（shortcut connection）。 模型结构：ResNet网络参考VGG19网络，引入残差单元。如下图，第三列即是ResNet-34。 模型特点： 提出残差模块 模型开始变得很深，可以达到152层 卷积层由Conv+BN+ReLU变成BN+ReLU+Conv GoogLeNet V4GoogLeNet V4来源于Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning。该论文一方面沿袭v3版本，使用更多的Inception module得到GoogLeNet V4。另一方面吸收了ResNet的残差单元，提出了两种Inception-ResNet。 模型结构：下图为其中一种，Inception-ResNet-v1，具有如下特点： Inception module都是简化版，没有使用那么多的分支，因为identity部分（直接相连的线）本身包含丰富的特征信息； Inception module每个分支都没有使用pooling； 每个Inception module最后都使用了一个1x1的卷积（linear activation），作用是保证identity部分和Inception部分输出特征维度相同，这样才能保证两部分特征能够相加。 模型特点： 使得宽模型变得更深 DenseNetDenseNet来源于Densely Connected Convolutional Networks，斩获了CVPR 2017的最佳论文奖。 模型结构：DenseNet有点类似于ResNet，但本质上又有很大的不同。结构上，把以前所有层的特征图都沿着通道轴拼接起来（而不是相加）。这可以理解为充分利用产生过的特征。 如下为ResNet： 如下为DenseNet： 模型特点： 建立了不同层的连接关系，充分利用特征图 MobileNetMobileNet来源于Google提出的MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications，是一种小巧而高效的CNN模型。 模型结构：MobileNet的核心在于提出了深度可分离卷积，它把传统卷积分解成了深度卷积(depthwise convolution)和逐点卷积(pointwise convolution)，从而大量减少参数量。 对于输入特征图(DF,DF,M)，输出特征图(DG,DG,N)，传统卷积核的尺寸为(K,K,M,N)，如下图(a)。而对于深度可分离卷积，深度卷积的尺寸为(K,K,1,M)，它将这M个卷积核各自应用于输入特征图的各个通道（这与传统卷积不同，这里相乘后不需要沿着通道轴相加），输出特征为(DG,DG,M)，如(b)所示。逐点卷积的尺寸为(1,1,M,N)，这个就是普通的1×1卷积了，输出特征为(DG,DG,N)，如(c)所示。可以看到，参数量从（K×K×M×N）变成（K×K×1×M + 1×1×M×N），减小了 M(KKN - KK -N)。 模型特点： 轻型模型，可用于移动端 ResNeXtResNeXt来源于Aggregated Residual Transformations for Deep Neural Networks。它是基于ResNet，吸收了GoogLeNet的Inception，所以和谷歌的Inception-ResNet很像。 模型结构：如下图，左图是是ResNet，右图是新的ResNeXt。 该结构可以做如下等效，第三种就是等效的分组结构。 模型特点： ResNeXt的分支的拓扑结构是相同的，而Inception V4需要人工设计 提出了一种介于普通卷积核深度可分离卷积的这种策略：分组卷积 XceptionXception来源于Xception: Deep Learning with Depthwise Separable Convolutions。它是Inception-V3的另一种改进，吸收了深度可分离卷积，造就了一种参数量相对少一些的网络结构。 模型结构：Inception-V3可做如下简化，可以看到，如下图和深度可分离卷积是很像的，只是下图是先进行1×1的卷积，再进行channel-wise的spatial convolution，最后concat，而后者是先进行一个channel-wise的spatial convolution，然后是1×1的卷积。所以作者干脆把它换成深度可分离卷积。 最终整体结构如下，其中SeparalbeConv即是深度可分离卷积。 模型特点： 虽然使用了深度可分离卷积，但网络也加宽了，总体参数量和Inception-V3差不多，性能提升了。 提出时间和MobileNet相近，它们从不同的角度揭示了深度可分离卷积的强大作用，MobileNet的思路是通过将 3×3 卷积拆分的形式来减少参数数量，而Xception是通过对Inception的充分解耦来完成的。 ShuffleNetXception来源于ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices。这也是一款效率极高的轻型CNN模型，通过逐点群卷积(pointwise group convolution)和通道混洗(channel shuffle)大大降低计算量。 模型结构：如下图左是普通的分组卷积，但是经过多层分组卷积后某个输出channel仅仅来自输入channel的一小部分，学出来的特征也很局限，因此作者提出了通道混洗channel shuffle，过程如下图中，在进行GConv2之前，对其输入feature map做一个分配，也就是每个group分成几个subgroup，然后将不同group的subgroup作为GConv2的一个group的输入，使得GConv2的每一个group都能卷积输入的所有group的feature map，结果图下图右。 pointwise group convolution，其实就是带group的卷积核为1×1的卷积。下图左是一个深度可分离卷积，而中间的图则是一个使用了pointwise group convolution的ShuffleNet unit，它将1×1卷积变成分组卷积，并在第一组分组卷积后加上通道混洗而成。右边的图则是带有降采样的ShuffleNet unit，它一方面在辅分支加入步长为2的3×3平均池化，一方面将最后的相加变成了通道级联。 模型特征： 应用了1×1的通道卷积 提出了通道混洗 总结其实总的来说，创新性的应该包含了inception，残差学习，深度可分离卷积，分组卷积几种。inception有GoogLeNet V1-V4、Xception、ResNeXt。残差学习有ResNet、ResNeXt、DenseNet、GoogLeNet V4。深度可分离卷积有MobileNet、ShuffleNet、Xception。分组卷积有ResNeXt、ShuffleNet。]]></content>
      <categories>
        <category>深度学习模型</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[cv中Attention的奇妙旅途——讲讲Self-Attention, SENet和CBAM]]></title>
    <url>%2F2020%2F05%2F08%2FSENet%2F</url>
    <content type="text"><![CDATA[前言由于注意力机制的高速发展，我尝试着对attention形成一种比较系统化的理解，选了比较有代表性的Self-Attention, SENet和CBAM，整理成本文。 Self-Attention在谷歌发表的Attention Is All You Need之后，Self-Attention开始广为人知。正如我此前对这篇论文的讲解，最终的注意力可以表示为下图，其中Q为Query，K为Key，V为Value，三者都是由输入X经过不同的映射得来的。这个公式可以这么记，先通过相乘得到Query和Key的相似度，而后归一化加softmax成为注意力权重，该权重乘以Value值就是输出的新表达了。 然而，这个公式在这里的输入X的维度是time × embedding，那么怎么用于三维的图像呢？很自然的可以想到，时序信号中的时间可以类比到图像中的空间，那么只需要把长和宽两个维度拉成一个维度，就形成了一行地的空间信息。那么接下来，剩下的通道（单通道或三通道），就可以类比成时序信号中token的embedding。因此，Self-Attention公式在图像上的输入的维度是spatial × channel。那么，图像上的Self-Attention本质上是计算一种空间权重。 SENet2017年Squeeze-and-Excitation Networks获得了ILSVRC的冠军，使得SENet名声大噪。其实这篇论文的核心在于提出了一种很方便嵌入其他模型的模块————SE block。它的结构图如下。前边是传统的卷积操作，得到了特征图U(H x W x C)。而后看上边的支路，共有两个操作，一个是Squeeze一个是Excitation，可以得到一串通道上的注意力权重，再把它乘进各个通道，就得到了新的特征图。 Squeeze操作对U进行Global Average Pooling，得到一串(1 x 1 x C)的权重，这里就是注意力的雏形了，比起一开始Self注意力的QK矩阵真是简单粗暴了很多。 Excitation操作也很直接，就是把刚才得到的权重经过两层全连接层（后边带ReLU）再加一层Sigmoid。至于两层全连接的神经元数？第一层是输入C个而输出C/r个神经元，第二次则是输入C/r而输出C个神经元。可以看成一种压缩再恢复的过程，r表示压缩程度。作者的实验表明r取16时效果比较好。所以，图像上的SENet本质上是计算一种通道权重。 CBAM现在我们知道了有空间的注意力，有通道的注意力，那么也可以想到一种两者都有的注意力。CBAM: Convolutional Block Attention Module就干了这么一件事。它也是一种可嵌入的模块，结构图如下,包含通道注意力模块和空间注意力模块。 这里分别讲解这两种模块。 通道注意力模块 通道注意力模块其实基本就是SE block，不同点在于除了SE用的AvgPool之外还用了MaxPool，相当于有两种Squeeze方式，而后得到的两串注意力雏形各自同样经过两层带ReLU的全连接层（注意这里avg和max使用的全连接是共享的，有点奇怪，个人感觉不共享会好一些），相加起来再经过Sigmoid，就得到了通道上的注意力，然后乘回去得到了通道上更新了的特征图。 空间注意力模块 空间注意力模块可以说是用Self的思想和SENet的操作形成的。可以看到，刚才的通道注意力是在空间上进行Pool，那么，空间注意力是不是也可以在通道上进行Pool呢？这就形成了空间注意力模块。首先，它基于通道上进行global max pooling 和global average pooling，得到的两张空间图拼接一下形成2通道，再进行一下卷积(实验表明7 * 7卷积效果好些)降成单通道，类似的经过一个Sigmoid，就得到了空间注意力权重。乘回去就得到了空间上更新了的特征图。 所以，CBAM基于senet的通道注意力，引入空间注意力，本质上是计算了通道和空间的权重。 总结经过这么一个流程可以看到： 注意力就是计算通道(嵌入)和空间(时间)的注意力权重，Self是空间，SENet是通道，CBAM是空间加通道。甚至于空间上的注意力还可以拆分，只有横轴的注意力或只有纵轴的注意力，这些都视实际输入的图像需求来选择。 注意力的形式变得简单。一开始的Self需要经过QK矩阵算出一个三维（nlp中是二维，空间拉出长宽就变成了三维）的注意力权重，表示空间上每一个点与空间上所有点的注意力。CBAM中的空间注意力模块，只需经过pooling和一些简单的其它操作即得到了一个二维的注意力权重，表示一种整体上应该关注空间上的哪些位置。]]></content>
      <categories>
        <category>深度学习模型</category>
      </categories>
      <tags>
        <tag>attention</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[讲讲横扫nlp任务的BERT模型]]></title>
    <url>%2F2020%2F05%2F07%2FBERT%2F</url>
    <content type="text"><![CDATA[前言本文讲解Google在2019年发表的论文BERT: Pre-training of Deep Bidirectional Transformers forLanguage Understanding 。从标题可以看出，该论文基于Transformer模型，提出了一款用于语言理解的预训练模型，并在GLUE, SQuAD等nlp任务中都取得了很好的效果。该模型的创新点实际上不在于模型结构，而在于预训练的方法。以下围绕这两方面都进行一些讲解。 模型结构总体框图首先最好还是先理解一下Transfomer，这是BERT模型的基础所在。简而言之，Transfomer包含N个编码器和解码器。编码器将输入序列编码成带有全局新的特征序列，解码器将编码器的特征序列解码成预测结果。BERT使用的正是其中的编码器，模型如下所示。 注意：并不是一个Tm代表一个Transformer，可以看成一行Tm表示一个Transformer的编码器，而编码器也不是两层可以有多层。那么，输入序列送入BERT后，最后一层Tm将对每一个输入token都生成一个新的特征序列，而T则表示任务，一般都是用全连接层来完成分类任务。另外用来对比的是，与GPT是单向的Transformer连接，ELMo是双向的LSTM连接。 Embedding从框图上可以看到，输入序列是各token的embedding。在Transfomer中，这种嵌入由word embedding加上positional embedding而来。而在BERT中，还要额外加上一个segment embedding，用于指示各个token属于输入的第几个句子。这是因为有的nlp任务是输入一对句子的，需要借此加以区分。而且，positional embedding也不是沿用三角函数，三种embedding都是学习出来的。另外，一开始的token，除了要在一开始添加一个起始标志[CLS]之外，还要在不同句子的过渡位置插一个[SEP]标志（如果有多句子的话）。 迁移策略BERT是一个预训练模型，所以我们要怎么拿过来用呢？官方为我们提供了各种任务的应用方法。首先，NLP的下游任务可以分为4类： 句子关系判断：识别蕴含(entailment)、识别语义相似等 分类任务：文本分类、情感计算等 序列标注：分词、实体识别、语义标注等 生成式任务：机器翻译、文本摘要等 图中 (a) 解决的是句子关系判断问题，MultiNLI(识别蕴含，M推理出N，蕴含/矛盾/中立），QQP（识别语义相似），QNLI（识别是否回答了问题），STS-B（识别语义相似），MRPC（识别语义等价，微软）、RTE（识别蕴含，小数据），SWAG（识别回答问题，大数据)。(b) 解决的是分类任务，SST-2（情感计算，斯坦福），CoLA（句子语言性判断，是否能成句）。(c) 解决的是序列标注任务，SQuAD（判断回答的起始和结束时刻，斯坦福问答数据集，从phrase中选取answer）。(d) 解决的是序列标注任务，NER（命名实体识别）。总的来说 (a) 和 (b) 都是分类任务，差别在于输入的是一个句子还是一对句子。这类任务，只需通过在第一个token（即[CLS]标志）的特征序列送入全连接层即可获取识别结果。(c) 和 (d) 都是序列标注任务，这类则在多个token的特征序列送入全连接层，获取各自的标注结果。可以看到，目前只有生成式任务还没有被ko。 p.s. 大名鼎鼎的GLUE任务集则包含了MultiNLI、QQP、QNLI、STS-B、MRPC、RTE、WNLI(也是识别蕴含)、SST-2、CoLA。 预训练方法常见的预训练方法一般是给前面的序列去预测下一个token（像是GPT）。而BERT就提出了两个比较有意思的训练任务 —— Masked LM 和 Next Sentence Prediction。 Masked LM为了实现模型的双向，就不能一直给定前面预测后面，于是作者提出了一个trick。在训练过程中，随机mask掉15%的token，即把相应位置的token替换成一个[MASK]标识，而任务的目标就是要去恢复这个句子（包含MASK的词），而损失函数只考虑了MASK位置的预测值，忽视掉非masked的值。这样，MASK的词可前可后，就实现了模型的双向性。 由此也衍生出了一个问题，就是在实际预测的时候是不会碰到[MASK]的，用了太多[MASK]就容易影响到模型。所以作者又用了个小技巧，选中了要mask的token后，其中10%的token会被替代成其他token，10%的token不替换，剩下的80%才被替换为[MASK]。 Next Sentence Prediction由于nlp中存在需要输入两个句子的句子关系判断任务，所以需要增设一个让模型理解句子之间关系的任务，于是Next Sentence Prediction应运而生。具体而言，就是输入两个句子，由模型来判断这两个句子是不是连续的上下句。其中，为了保持样本平衡性，选了50%的连续的正样本，再随机选50%的无关的负样本。其实这个任务和seq2seq的任务有点异曲同工之妙，只是从单词级别变到了句子级别。举个例子：正样本：今天[MASK]（天气）真好，正好我们[MASK]（出去）吃饭吧。负样本：今天[MASK]（天气）真好，[MASK]（我）吃饱了。 总结其实BERT模型除了提出这两种训练方法外，大量的数据肯定对这个预训练模型有很强的作用，不过一般人没这种计算资源…所以还是很感谢谷歌开源出来的预训练模型，可以很方便的使用并达到非常好的效果。在使用时，如果需要用到自己的数据库上，要么就是完全自己写然后导入BERT模型，要么可以直接使用官方的代码，如run_glue.py，只需要修改数据的预处理，定义好新的类，然后指定类别数等参数，就可以直接使用了。 参考文献https://www.cnblogs.com/rucwxb/p/10277217.htmlhttps://zhuanlan.zhihu.com/p/46652512]]></content>
      <categories>
        <category>深度学习模型</category>
      </categories>
      <tags>
        <tag>BERT</tag>
        <tag>Transformer</tag>
        <tag>GLUE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Transfomer以及Self-Attention讲解]]></title>
    <url>%2F2020%2F05%2F05%2Ftransfomer%2F</url>
    <content type="text"><![CDATA[前言这一篇主要讲解谷歌发表的Attention Is All You Need。这篇论文提出了驰名的一种注意力机制 —— self-attention 模块，并进一步提出了 Transformer 架构，从而将以往用的计算代价较大的RNN替换掉了。目前，nlp任务中效果非常好的BERT模型就是大量应用了Transformer架构的Encoder。 下图是一个很好的使用Transformer进行机器翻译任务的例子。在预测过程中，编码阶段，输入的“I arrived at the”中的每个单词都会计算与所有单词的注意力权重，并加权求和得出新的自己的表示，逐层编码。解码阶段，输入由encoder出来的所有单词的表示和上一个位置输出的embedding，经过类似的注意力操作得到这一个位置的输出，是一种随着预测位置移动的迭代过程。 总体框架与流程框架对照着以上例子，看下边的Transfomer总体框架图。左边为编码器，右边为解码器。编码器和解码器中都包含了Positional Encoding模块，Multi-Head Attention模块，Feed-Forward模块。下一章节会对此着重讲解。 流程定义一下符号。 emb_dim：嵌入的尺寸 input_length：输入序列的长度 target_length：目标序列的长度+1。+1是因为要移位。 vocab_size：目标词汇表中的单词数量。 则Transformer的流程可表示为： 该模型将每个token表示为维度emb_dim的向量。然后，对于特定的输入序列，我们有了尺寸为（input_length）x（emb_dimb）的矩阵。 然后添加位置信息（位置编码）。与上一步一样，此步骤将返回尺寸为（input_length）x（emb_dim）的矩阵。 数据通过N=6个编码器块。之后，我们获得尺寸为（input_length）x（emb_dim）的矩阵。 目标序列经过等同于1和2的操作，并进行mask屏蔽。输出的尺寸为（target_length）x（emb_dim）。 4的结果经过N=6个解码器块。在每个迭代中，解码器都使用编码器的输出3）。这在总框图中由从编码器到解码器的箭头表示。输出的尺寸为（target_length）x（emb_dim）。 最后，逐行使用全连接层和softmax。输出的尺寸为（target_length）x（vocab_size）。 编码器对于训练阶段和测试阶段是一样的编码过程，而解码器的流程则有所不同，因此先讲解一下解码器的训练和测试。在测试阶段，由于没有groundtruth，所以我们需要从零开始不断迭代一个词一个词地生成。具体操作如下： 计算输入序列的嵌入表示。 使用起始token例如’‘，作为第一个目标序列。该模型将预测输出一个token。 将最后一个预测token添加到目标序列，并使用它生成新的预测。 重复执行步骤3，每次的输入token和输出token都增加，直到预测的token是表示序列结束的token，例如。 在训练阶段中，由于我们事先有roundtruth，因此我们将直接为模型提供整个已移位目标序列，并要求其预测未移位目标。举个例子，目标是将句子从英语翻译成西班牙语：X = [‘Hello’，’，’，’how’，’are’，’you’，’？’]（输入序列）Y = [‘Hola’，’，’，’como’，’estas’， ‘？’]（目标序列）在前面的示例之后，我们将给解码器输入：[‘‘，’Hola’，’，’，’como’，’estas’，’？’]预期的预测将是：[‘Hola’，’，’，’como’，’estas’，’？’，’‘] 因此可以看到，解码器在训练时直接从target_length-&gt;target_length，而测试时则是从1-&gt;1 2-&gt;2 3-&gt;3 … target_length-&gt;target_length的过程，最后预测的是每次迭代中最后一个预测的token串联起来。 Positional EncodingTransformer抛弃了RNN，而RNN最大的优点就是在时间序列上对数据的抽象，所以文章中作者提出两种Positional Encoding的方法，将encoding后的数据与embedding数据求和，加入了相对位置信息。 用不同频率的sine和cosine函数直接计算 学习出一份positional embedding实验后发现两者结果一样，所以用了第一种方法，优点是不需要训练参数，而且即使在训练集中没有出现过的句子长度上也能用 对于输入序列，经过word embedding后，加上positional embedding后即可得到该序列的 representation，序列中的每个token都转换成包含 word 的特征和 word 在句子中的位置信息的向量。 Multi-Head AttentionMulti-Head Attention其实就是多个Self-Attention结构的结合。因此，首先我们需要着重学习论文的重点Self-Attention。 Self-Attention从一个比较知名的例子讲起。假如我们要翻译一个词组Thinking Machines，其中Thinking的输入的embedding vector用x1表示，Machines的embedding vector用x2表示。当我们处理Thinking这个词时，我们需要计算句子中所有词与它的Attention Score，这就像将当前词作为搜索的query，去和句子中所有词（包含该词本身）的key去匹配（点乘），看看相关度有多高。相关度进行尺度缩放与softmax归一化可以得到注意力权重，注意力与相应的value加权求和就得到新的表达。 如果将输入的所有向量合并为矩阵形式，则所有query, key, value向量也可以合并为矩阵形式表示 则上述操作可简化为矩阵形式 这就是著名的注意力公式： Multi-Head Attention基于上边的Self-Attention， 我们进一步拓展，对输入序列使用不同的Q，K，V进行多次以上操作，而后拼接起来，再转换成最终的表示。这样每个head可以学习到在不同表示空间中的特征。 可视化如下： Masked Multi-Head Attention在训练过程的解码器中，需要对输入的注意力矩阵（即上边QK经过softmax的矩阵）进行masked操作，从而不给模型看见未来信息，解决了信息泄露问题。举例来说，对于目标序列（I have a dream），I作为第一个单词，只能有和自身的attention。have作为第二个单词，有和I, have 两个attention。 a 作为第三个单词，有和I,have,a 前面三个单词的attention。到了最后一个单词dream的时候，才有对整个句子4个单词的attention。 其它操作和上述的Multi-Head Attention一致。 Encoder-Decoder Multi-Head Attention在解码器的第二层attention里，需要整合encoder的输入序列和decoder的目标序列的信息，算出相互之间的注意力。与Multi-Head Attention的不同点在于，Encoder-Decoder Multi-Head Attention的Q矩阵来自decoder，而K和V来自encoder。其实也很好理解，就是注意力矩阵是由来自解码器的Query和来自编码器的Key之间计算得来，其它操作都相同。 Feed-Forward这个就很简单了，就是简单的映射层。 Produce Output Probabilities这个其实也是普通的映射层，它将每一个目标序列的token由emb_dim映射到vocab_size，因此就可得到各个token，串成目标序列了。 总结不得不说，这确实是一篇很经典的论文，将seq2seq模型推到了一个新高度，避免了RNN的大量计算代价，从此用CNN操作序列信号就有很好的效果了。另外，Self-Attention还跨界在cv行业也有了非常多的研究。可以说cv和nlp是同源的，只需要将图像的长宽拉成一列（空间信息）类比成序列信号的序列，图像的通道类比成序列信号的embedding即可。因此，Self-Attention模块的输入在nlp上是time × embedding，在cv上是spatial × channel。此外，当下横扫nlp的BERT模型也是基于Transfomer的encoder，这也表明这个模型的重要性了。 参考文献 https://medium.com/dissecting-bert/dissecting-bert-appendix-the-decoder-3b86f66b0e5f https://zhuanlan.zhihu.com/p/80986272 https://zhuanlan.zhihu.com/p/44121378 https://zhuanlan.zhihu.com/p/39034683 https://zhuanlan.zhihu.com/p/47282410]]></content>
      <categories>
        <category>深度学习模型</category>
      </categories>
      <tags>
        <tag>Transformer</tag>
        <tag>attention</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常见的梯度下降算法原理]]></title>
    <url>%2F2020%2F05%2F04%2Fgradient-descent%2F</url>
    <content type="text"><![CDATA[前言梯度下降算法（Gradient Descent Optimization）是神经网络模型训练最常用的优化算法。对于深度学习模型，基本都是采用梯度下降算法来进行优化训练的。梯度下降算法背后的原理：目标函数 $J(\theta)$ 关于参数 $\theta$ 的梯度将是损失函数（loss function）上升最快的方向。而我们要最小化loss，只需要将参数沿着梯度相反的方向前进一个步长，就可以实现目标函数（loss function）的下降。这个步长 $\eta$ 又称为学习速率。 原始的梯度下降Batch gradient descent 批梯度下降，对所有的样本计算梯度后求平均，并更新参数。 因为在执行每次更新时，我们需要在整个数据集上计算所有的梯度，所以批梯度下降法的速度会很慢，同时，批梯度下降法无法处理超出内存容量限制的数据集。批梯度下降法同样也不能在线更新模型，即在运行的过程中，不能增加新的样本。 对于凸误差函数，批梯度下降法能够保证收敛到全局最小值，对于非凸函数，则收敛到一个局部最小值。 SGD 随机梯度下降，对每个样本计算梯度，并更新一次参数。 SGD的运行速度更快 可以用于在线学习 SGD以高方差频繁地更新，导致目标函数出现剧烈波动。 与批梯度下降法的收敛会使得损失函数陷入局部最小相比，由于SGD的波动性，一方面，波动性使得SGD可以跳到新的和潜在更好的局部最优。另一方面，这使得最终收敛到特定最小值的过程变得复杂，因为SGD会一直持续波动。然而，已经证明当我们缓慢减小学习率，SGD与批梯度下降法具有相同的收敛行为，对于非凸优化和凸优化，可以分别收敛到局部最小值和全局最小值。 Mini-batch GD 小批量梯度下降法最终结合了上述两种方法的优点，在每次更新时使用个小批量训练样本 减少参数更新的方差，这样可以得到更加稳定的收敛结果 可以利用最新的深度学习库中高度优化的矩阵优化方法，高效地求解每个小批量数据的梯度。 小结原始的梯度下降方法有以下问题： 在梯度平缓的维度下降非常慢，在梯度险峻的维度容易抖动 容易陷入局部极小值或鞍点。Zero gradient,gradient descent gets stuck （在高维空间中，鞍点比局部极小值更容易出现）-选择一个合适的学习率可能是困难的。学习率太小会导致收敛的速度很慢，学习率太大会妨碍收敛，导致损失函数在最小值附近波动甚至偏离最小值-学习率调整试图在训练的过程中通过例如退火的方法调整学习率，即根据预定义的策略或者当相邻两代之间的下降值小于某个阈值时减小学习率。然而，策略和阈值需要预先设定好，因此无法适应数据集的特点-对所有的参数更新使用同样的学习率。如果数据是稀疏的，同时，特征的频率差异很大时，我们也许不想以同样的学习率更新所有的参数，对于出现次数较少的特征，我们对其执行更大的学习率 带冲量的梯度下降Momentum optimization冲量梯度下降算法是Boris Polyak在1964年提出的，其基于这样一个物理事实：将一个小球从山顶滚下，其初始速率很慢，但在加速度作用下速率很快增加，并最终由于阻力的存在达到一个稳定速率。对于冲量梯度下降算法，其更新方程如下： 可以看到，参数更新时不仅考虑当前梯度值，而且加上了一个积累项（冲量），但多了一个超参，一般取接近1的值如0.9。相比原始梯度下降算法，冲量梯度下降算法有助于加速收敛。当梯度与冲量方向一致时，冲量项会增加，而相反时，冲量项减少，因此冲量梯度下降算法可以减少训练的震荡过程。 Nesterov Accelerated Gradient (NAG)NAG算法是Yurii Nesterov在1983年提出的对冲量梯度下降算法的改进版本，其速度更快。其变化之处在于计算“超前梯度”更新冲量项，具体公式如下： 学习率自适应的梯度下降AdaGradAdaGrad是Duchi在2011年提出的一种学习速率自适应的梯度下降算法。在训练迭代过程，其学习速率是逐渐衰减的，经常更新的参数其学习速率衰减更快，这是一种自适应算法。 其更新过程如下： 把每一维度的梯度^2和记录下来，每次学习率都除以这个和 每一维度的学习率不一样，且都在不断减小 在梯度大的维度，减小下降速度；在梯度小的维度，加快下降速度 让学习率适应参数，对于出现次数较少的特征，我们对其采用更大的学习率，对于出现次数较多的特征，我们对其采用较小的学习率。因此，Adagrad非常适合处理稀疏数据。 Adagrad算法的一个主要优点是无需手动调整学习率 Adagrad的一个主要缺点是它在分母中累加梯度的平方：由于每增加一个正项，在整个训练过程中，累加的和会持续增长。这会导致学习率变小以至于最终变得无限小，在学习率无限小时，Adagrad算法将无法取得额外的信息。 RMSpropRMSprop是Hinton在他的课程上讲到的，其算是对Adagrad算法的改进，主要是解决学习速率过快衰减的问题。其实思路很简单，类似Momentum思想，引入一个超参数，在积累梯度平方项进行衰减： 此时可以看到s是梯度平方的指数加权移动平均值，其中\gamma一般取值0.9，此时s更平稳，减少了出现的爆炸情况，因此有助于避免学习速率很快下降的问题。同时Hinton也建议学习速率设置为0.001。 Adaptive moment estimation (Adam)Adam是Kingma等在2015年提出的一种新的优化算法，其结合了Momentum和RMSprop算法的思想。相比Momentum算法，其学习速率是自适应的，而相比RMSprop，其增加了冲量项。所以，Adam是两者的结合体： 可以看到前两项和Momentum和RMSprop是非常一致的， 由于和的初始值一般设置为0，在训练初期其可能较小，第三和第四项主要是为了放大它们。最后一项是参数更新。其中超参数的建议值是 总结本文沿着梯度下降的发展大致介绍了各种常用的梯度下降算法，目前比较常用的应该仍是 Adam ， 不过我感觉其实 SGD 加梯度衰减策略可能能取得更好的效果，当然这需要设置得比较合适。 彩蛋]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[opensmile 工具的使用和批处理]]></title>
    <url>%2F2020%2F05%2F02%2Fopensmile%2F</url>
    <content type="text"><![CDATA[前言openSMILE是一款以命令行形式运行的工具，通过配置config文件来提取音频特征。主要应用于语音识别、情感计算、音乐信息获取。2.0版本之后的openSMILE包括了openCV库，可以用于视频处理和视频特征提取。官网下载有linux和windows版本提供下载，windows可以不编译直接用，建议在命令行里指明 openSMILE 绝对路径。 openSMILE的输入输出格式文件输入格式 RIFF-WAVE (PCM) (for MP3, MP4, OGG, etc. a converter needs to be used) Comma Separated Value (CSV) HTK parameter files WEKA’s ARFF format.（由htk工具产生） Video streams via openCV.（opencv产生的视频流数据） 文件输出格式 RIFF-WAVE (PCM uncompressed audio) Comma Separated Value (CSV) HTK parameter file WEKA ARFF file LibSVM feature file format Binary float matrix format 分类器和其他组件openSMILE还提供了许多VAD算法，用于判断各时间点有没有说话。 Voice Activity Detection based on Fuzzy Logic Voice Activity Detection based on LSTM-RNN with pre-trained models Turn-/Speech-segment detector LibSVM (on-line) LSTM-RNN (Neural Network) classifier which can load RNNLIB and CURRENNT nets GMM (experimental implementation from eNTERFACE’12 project, to be release soon) SVM sink (for loading linear kernel WEKA SMO models) Speech Emotion recognition pre-trained models (openEAR) openSMILE使用流程简介 先切换到处理文件SMILExtract.exe所在的目录 通过如下语句提取：windows下：SMILExtract_Release -C “配置文件” -I “要处理的音频” -O “要保存特征向量的路径及文件名”linux下：SMILExtract -C “配置文件” -I “要处理的音频” -O “要保存特征向量的路径及文件名” 官方配置文件官方提供了许多常见特征集的配置文件，如MFCC，PLP，以及各大语音比赛中效果好的特征集。 MFCC特征为了提取MFCC特征（兼容HTK），提供了以下四个文件（它们是以它们所代表的相应的HTK参数类型命名的）：MFCC12_0_D_A.conf此配置从25毫秒的音频帧中提取梅尔频率倒谱系数（以10毫秒的速率采样）（汉明窗口）。 它由26个Mel频带计算13个MFCC（0-12）组，并应用了一个权重参数为22的倒谱提升滤波器。13个一阶和13个二阶系数被附加到MFCC后。MFCC12_E_D_A.conf此配置跟MFCC12_0_D_A.conf一样，但对数能量是只加在MFCC1-12上。MFCC12_0_D_A_Z.conf这个配置跟MFCC12_0_D_A.conf配置一样，除了所有特征是参考整个输入序列进行了标准化。MFCC12_E_D_A_Z.conf这个配置跟MFCC12_E_D_A.conf配置一样，除了所有特征是参考整个输入序列进行了标准化。帧长为25ms,帧移为10ms，使用的汉明窗，预增强参数为0.97。由26个通过FFT功率谱计算的mel-滤波器组计算MFCC 0/1-12。MEL频谱的频率范围为0-8kHz，同时这些配置文件提供了-I,-O选项。输出文件格式是HTK参数文件格式。如果需要输出其他文件格式，你必须在配置文件中更改‘cHtkSink’组件类型为你想要的类型。命令行示例如下： SMILExtract -C config/MFCC12_E_D_A.conf -I input.wav -O output.mfcc.htk PLP特征用于提取PLP倒谱系数（PLP-CC）（与HTK兼容）以下四个文件（它们是以它们所代表的相应的HTK参数类型命名的）：PLP_0_D_A.conf该配置从25 ms长音频（以10ms的速率采样）帧提取Mel频率倒谱系数（汉明窗口）。它从26个Mel频带，并使用预测阶数为5计算6个PLP（0-5），并应用了一个权重参数为22的倒谱提升滤波器。6个一阶和6个二阶系数被附加到PLP-CC后。PLP_E_D_A.conf该配置与PLP_0_D_A.conf相同，但对数能量是只加在PLP1-12上。PLP_0_D_A_Z.conf此配置与PLP_0_D_A.conf相同，除了所有特征是参考整个输入序列进行了标准化。PLP_E_D_Z.conf此配置与PLP_E_D_A.conf相同，除了所有特征是参考整个输入序列进行了标准化。帧长为25ms,帧移为10ms，使用的汉明窗，预增强参数为0.97。由26个通过FFT功率谱计算的听觉mel-滤波器组(压缩系数为0.33)计算PLP 0/1-5。线性预测器的预测阶数为5。MEL频谱的频率范围为0-8kHz，同时这些配置文件提供了-I,-O选项。输出文件格式是HTK参数文件格式。如果需要输出其他文件格式，你必须在配置文件中更改‘cHtkSink’组件类型为你想要的类型。命令行示例如下： SMILExtract -C config/PLP_E_D_A.conf -I input.wav -O output.plp.htk 情感特征集自openSMILE在openEAR的项目EWS09情感识别中被使用，openSMILE提供了各种情感识别的标准特征集。The INTERSPEECH 2009 Emotion Challenge feature set（参见[SSB09]）由配置文件config/emo IS09.conf提供。它包含对LLDs应用统计函数得到的384个特征。该特征被保存在Arff格式（针对WEKA），新的实例会被附加到一个已存在文件（这是用于批处理，其中openSMILE被反复调用从多个文件提取特征到单个特征文件）。 出现在Arff文件中16个低级描述符（LLDs）的名称，见下面的列表： pcm_RMSenergy 信号帧均方根能量 mfcc 梅尔频率倒谱系数1-12 Pcm_zcr 时间信号的过零率（基于帧） voiceProb 从ACF计算的发声概率。 F0 从倒谱计算的基频 附加到低级描述符名称的后缀_sma表示它们是通过窗口长度为3的移动平均滤波器进行平滑。附加到sma的后缀_de表示当前特征是低级描述符平滑后的一阶delta系数（微分）。 max 轮廓的最大值 min 轮廓的最小值 range = max- min maxPos 最大值的绝对位置（以帧为单位） minPos 最小值的绝对位置（以帧为单位） amean 轮廓的算术平均值 linregc1 轮廓线性逼近的斜率（m） linregc2 轮廓线性逼近的偏移量（t） linregerrQ 计算的二次误差作为线性近似值和实际轮廓的差值 stddev 轮廓上的值的标准偏差 skewness 偏度（3阶矩） kurtosis 峰度（4阶矩） The INTERSPEECH 2010 Paralinguistic Challenge feature set（见2010年INTERSPEECH会议论文集）由配置文件config/IS10_paraling.conf提供。该集包含的1582个特征是由34个低级描述符（LLDs）和34个相应的delta作为68个LLDs轮廓值，在此基础上应用21个函数得到1428个特征，另外，对4个基于音高的LLD及其4个delta系数应用了19个函数得到152个特征，最后附加音高（伪音节）的数量和总数输入的持续时间（2个特征）。该特征被保存在Arff格式（针对WEKA），新的实例会被附加到一个已存在文件（这是用于批处理，其中openSMILE被反复调用从多个文件提取特征到单个特征文件）。 出现在Arff文件中34个低级描述符（LLDs）的名称，见下面的列表： pcm_loudness 归一化强度提高到0.3的幂的响度 mfcc 美尔频率倒谱系数0-14 logMelFreqBand 梅尔频带的对数功率0-7（分布范围内从0到8 kHz） lspFreq 从8个LPC系数计算出的8个线谱对频率。 F0finEnv 平滑的基频轮廓线。 voicingFinalUnclipped 最终基频候选的发声概率。Unclipped的意思是，当其低于浊音阈值时，它不被设置为零。 附加到低级描述符名称的后缀_sma表示它们是通过窗口长度为3的移动平均滤波器进行平滑。附加到sma的后缀_de表示当前特征是低级描述符平滑后的一阶delta系数（微分）。出现在Arff文件中的21个函数的名字,均在以下列表中： maxPos 最大值的绝对位置（以帧为单位） minPos 最小值的绝对位置（以帧为单位） amean 轮廓的算术平均值 linregc1 轮廓线性逼近的斜率（m） linregc2 轮廓线性逼近的偏移量（t） linregerrA 把线性误差计算作为线性近似值和实际的轮廓的误差 linregerrQ 把二次误差计算作为线性近似值和实际的轮廓的误差 stddev 轮廓中的值的标准偏差 skewness 偏度（3阶矩）。 kurtosis 峰度（4阶矩）。 quartile1 第一四分位数（25％百分位数） quartile2 第一四分位数（50％百分位数） quartile3 第一四分位数（75％百分位数） iqr1-2 四分位数间距：quartile2- quartile1 iqr2-3 四分位数间距：quartile3- quartile2 iqr1-3 四分位数间距：quartile3- quartile1 percentile1.0 轮廓的离群值鲁棒最小值，按1％百分位数表示。 percentile99.0 轮廓的离群值鲁棒最大值，按99％百分位数表示。 pctlrange0-1 由1％和99％的百分点的范围表示的离群值鲁棒信号范围“max-min”。 upleveltime75 信号超过（75％*范围+min）的时间百分比。 upleveltime90 信号超过（90％*范围+min）的时间百分比。 四个音高相关的LLD（及相应的delta系数）如下（清音区域均为0，因此功能仅适用于这些轮廓的浊音区域）： F0final 平滑的基频频率 jitterLocal 本地（帧到帧）抖动（音调周期长度偏差） jitterDDP 差分帧间抖动（‘Jitter of the Jitter’） shimmerLocal 本地（帧到帧）闪烁（音调周期幅度偏差） 对这4 + 4个LLD应用了19个函数，即上述21个函数的集合没有最小值（1％百分位数）和范围。 The INTERSPEECH 2011 Speaker State Challenge feature set（见2011年INTERSPEECH会议论文集）由配置文件config/IS11_speake_state.conf提供。该集包含的4368个特征是由4个能量相关+50个频谱相关的低级描述符（LLDs）和54个相应的delta作为108个LLDs，在此基础上应用33个基本函数+平均值、最小值、最大值、标准差得到3996个特征；5个声音相关和5个对应的delta作为10个LLDs，在此基础上应用33个基本函数+二次平均、上升时长、下降时长得到360个特征；6个F0基本函数和对应的delta，12个特征。 The INTERSPEECH 2012 Speaker Trait Challenge feature set（见2012年INTERSPEECH会议论文集）由配置文件config/IS12_speake_trait.conf提供。该集包含的6125个特征。 The INTERSPEECH 2013 ComParE Challenge feature set （见2013年INTERSPEECH会议论文集）由配置文件config/IS13_ComParE.conf提供。该集包含的6373个特征，LLD包括能量，频谱，倒谱（MFCC）、声音、对数谐波噪声比（HNR），频谱谐度和心理声学频谱清晰度。 The MediaEval 2012 TUM feature set for violent video scenes detection 针对好莱坞流行电影的暴力进行检测的特征集在config/mediaeval2012_tum_affect/，里面有不同的设置，参考文章：Florian Eyben, Felix Weninger, Nicolas Lehment, Gerhard Rigoll, Björn Schuller: ”Violent Scenes Detection with Large, Brute-forced Acoustic and Visual Feature Sets”, Proc. MediaEval 2012 Workshop, Pisa, Italy, 04.-05.10.2012. MediaEval Audio IS12based subwin2.conf包含的是从2s的子窗中提取音频特征的配置。MediaEval Audio IS12based subwin2 step0.5.conf提取一样的特征，但是2s子窗的偏移为0.5s。MediaEval VideoFunctionals.conf用于视频特征提取，如文章使用方法，需要一个包含LLDs的CSV文件（由openCV提取）作为输入和输出，ARFF文件作为视频特征。 The openSMILE/openEAR ‘emobase’ set早期的基线集（参照”emobase2”集作为新的基线集），拥有情感识别的998个声学特征，包含以下低级描述符（LLDs）：强度，响度，12 MFCC，音高（F0），浊音概率，F0包络线，8 LSF（线频谱频率），过零率， 以及这些LLD的Delta回归系数。以下函数被应用于上述LLDs及其Delta系数。：Max./Min。输入的相对位置和范围，范围，算术平均值，2线性回归系数，线性和二次误差，标准差，偏度，峰度，四分位数1-3和三位四分位数范围。 The large openSMILE emotion feature set用于提取更多的LLDs和更多的函数(6552个特征)，配置文件为config/emo_large.conf。 The openSMILE ‘emobase2010’ reference set 是基于the INTERSPEECH 2010 Paralinguistic Challenge feature set，配置文件为config/emobase2010.conf。对持续时间和位置特征的规范化进行了一些调整。这个特性集包含了一套大大增强的低级描述符(LLDs)，以及一套“emobase”相比更加精细化选择的函数列表。建议使用此特征集作为比较新的情感识别特征集和方法的参考，因为它代表当前最先进的情感和语言识别功能。该集合包含1582个特征（与INTERSPEECH 2010 Paralinguistic 挑战集相同设置），其由34个低级描述符（LLDs）和34个相应的delta作为68个LLDs轮廓值，在此基础上应用21个函数得到1 428个特征，另外，对4个基于音高的LLD及其4个delta系数应用了19个函数得到152个特征，最后附加音高（伪音节）的数量和总数输入的持续时间（2个特征）。唯一的区别是INTERSPEECH 2010 paralinguistic挑战集标准化的是是“maxPos”和“minPos”特征，本配置被标准化为段长度。 python批处理提取openSMILE特征所有支持标准数据输出格式的配置文件都可以在WINDOWS的批特征提取GUI（使用VS10 C#编写，位于progsrc/openSMILEbatchGUI/）。这个工具允许openSMILE自动的执行文件夹中的若干文件。它可以在图形界面中选择音频文件和指定输出类型。openSMILE本身提供批处理GUI（使用VS10 C#编写，位于progsrc/openSMILEbatchGUI/），但若语音数据的目录结构较复杂，还可以利用python来进行批处理。示例代码如以下： import os from subprocess import call def excute_CMD(path_ExcuteFile, path_Config, path_Audio, path_Output): cmd = path_ExcuteFile + &quot; -C &quot; + path_Config + &quot; -I &quot; + path_Audio + &quot; -O &quot; + path_Output call(cmd, shell=True) def batch_extract_features(path_Config, path_Input_Root, path_Output): path_ExcuteFile = &quot;SMILExtract_Release&quot; filename = os.listdir(path_Input_Root) for i in range(len(filename)): print(&#39;Extracting features of %s&#39; % filename[i]) path_Input = path_Input_Root + &#39;/&#39; + filename[i] + &#39;.wav&#39; excute_CMD(path_ExcuteFile, path_Config, path_Input, path_Output) path_Config = &quot;./config/IS13_ComParE.conf&quot; path_Input_Root = &#39;root_path_to_audio/&#39; path_Output = &#39;features.csv&#39; batch_extract_features(path_Config, path_Input_Root, path_Output) 输出数据格式控制对于不包含统计函数的配置文件，选项定义在config/shared/standard_data_output_lldonly.conf.inc ==============================LLD only============================= ================================CSV================================ -csvoutput &lt;filename&gt; 默认输出选项. CSV格式，存放帧向LLD -appendcsv &lt;0/1&gt; 设为1代表添加到已有CSV文件文末，默认0 -timestampcsv &lt;0/1&gt; 设为0禁止把时间步输出到CSV第二列，默认为1 -headercsv &lt;0/1&gt; 设为0禁止把标题输入到CSV，默认为1 ================================HTK================================ -output &lt;filename&gt; 输出特征汇总（函数）到HTK格式文件 ================================ARFF=============================== -arffoutput &lt;filename&gt; 默认输出选项. ARFF格式，存放帧向LLD -appendarff &lt;0/1&gt; 设为0代表不添加到已有ARFF文件文末，默认1添加 -timestamparff &lt;0/1&gt; 设为0禁止把时间步输出到ARFF第二列，默认为1 arfftargetsfile &lt;file&gt;指定配置包含定义目标域（类）的文，默认为:shared/arff_targets_conf.inc 对于包含统计函数的配置文件，如全部的INTERSPEECH和AVEC挑战集，选项定义在config/shared/standard_data_output.conf.inc =============================LLD and func ========================= -instname &lt;string&gt; 通常是输入文件的名称保存在CSV和ARFF输出的首列。默认是&quot;unknow&quot; ================================ARFF=============================== -lldarffoutput, -D &lt;filename&gt; 启动LLD帧向输出到ARFF格式文件 -appendarfflld &lt;0/1&gt; 设为1代表添加到已有ARFF文件文末，默认0覆盖 -timestamparfflld &lt;0/1&gt; 设为0禁止把时间步输出到ARFF第二列，默认为1 -lldarfftargetsfile &lt;file&gt; 指定配置包含定义目标域（类）的文，默认为: shared/arff_targets_conf.inc ================================CSV================================ -lldcsvoutput, -D &lt;filename&gt; 启动LLD帧向输出到CSV格式文件 -appendcsvlld &lt;0/1&gt; 设为1代表添加到已有CSV文件文末，默认0覆盖 -timestampcsvlld &lt;0/1&gt; 设为0禁止把时间步输出到CSV第二列，默认为1 -headercsvlld &lt;0/1&gt; 设为0禁止把标题输入到CSV，默认为1 ================================HTK================================ -lldhtkoutput &lt;filename&gt; 启动LLD帧向输出到HTK格式文件 ================================ARFF=============================== -output, -O &lt;filename&gt; 默认输出选项. ARFF格式，存放特征汇总 -appendarff &lt;0/1&gt; 设为0代表不添加到已有ARFF文件文末，默认1添加 -timestamparff &lt;0/1&gt; 设为1把时间步输出到ARFF第二列，默认为0 -arfftargetsfile &lt;file&gt;指定配置包含定义目标域（类）的文，默认为: shared/arff_targets_conf.inc ================================CSV================================ -csvoutput &lt;filename&gt; 默认输出选项. CSV格式，存放特征汇总 -appendcsv &lt;0/1&gt; 设为0代表不添加到已有CSV文件文末，默认1 -timestampcsv &lt;0/1&gt; 设为0禁止把时间步输出到CSV第二列，默认为1 -headercsv &lt;0/1&gt; 设为0禁止把标题输入到CSV，默认为1 ================================HTK================================ -htkoutput &lt;filename&gt; 输出特征汇总（函数）到HTK格式文件 如下为lldcsvoutput的定义。注：从2.2版本起，可以指定一个“?”替代文件名。它会禁止相应的输出组件，即它不会产生输出文件，在标准输出接口界面，看到的所有的文件名默认都是”?” [lldsink:cCsvSink] reader.dmLevel = lld;lld_de filename=\cm[lldcsvoutput(D){?}:output csv file for LLD, disabled by default ?, only written if filename given] instanceName=\cm[instname(N){unknown}:instance name] append = \cm[appendcsvlld{0}:set to 1 to append to the LLD output csv file, default is not to append] timestamp = \cm[timestampcsvlld{1}:set to 0 to suppress timestamp column, default is 1, i.e. to show timestamp in second column] number = 0 printHeader = \cm[headercsvlld{1}:set to 0 to suppress header line with feature names, default is 1, i.e. to show header line] errorOnNoOutput = 1 那么，当需要同时输出lld和func时，可用如下命令 SMILExtract -C config/IS13_ComParE.conf -I input.wav -lldcsvoutput lld_output.csv -csvoutput func_output.csv 最后一点话其实如果只是用官方配置提特征那么只看批处理那里也够了。官方配置文件可以根据需求时再看需要哪个文件，也可自己按着这个格式自定义编写配置文件。另外输出格式控制感觉最好也是先看一下，我一开始都是直接用 -O 输出统计特征，但想输出lld时跑去源代码里一阵捣鼓，后来才发现它已经封装好了直接一个参数就可以了。 彩蛋]]></content>
      <categories>
        <category>语音特征</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[语谱图的matlab提取和python提取]]></title>
    <url>%2F2020%2F05%2F02%2Fspecgram%2F</url>
    <content type="text"><![CDATA[前言语谱图（spectrogram或specgram），也叫声谱图，可以简单看做一个二维矩阵，其纵轴表示频率，横轴表示时间，矩阵的值表示能量强弱。由于它拥有着频率和时间两个维度的信息，所以是比较综合地表示原语音信息的一种特征。另外，我将其看做语音和图像的一种连接，因为图像领域的模型发展得较快，所以通过这种方式把语音转换成一种特殊的图像再进一步处理。 语谱图流程简介1. 将语音可交叉地分成多帧（由于语音的短时平稳性） 2. 各帧加窗 3. 各帧通过快速傅里叶变化（fft）得到频谱向量 4. 沿着时间轴并联各频谱向量得到语谱图 语谱图的提取语谱图的matlab提取先看一段非官方代码，结合上述步骤进行理解。 [x,Fs,nBits]=wavread(&#39;audio.wav&#39;); s=length(x); % 信号长度 w=256; % 窗长 n=w; % nfft，表示做fft变换需要的点数，一般为刚大于w的2的幂。举例，w=250，则n一般设为256 ov=w/2; % 分帧的交叉程度，常见设为窗长的二分之一或四分之一 h=w-ov; % 不重叠点数 win=hamming(n)&#39;;% 选了常见的汉明窗，并设置nfft c=1; % 指向当前帧的指针 ncols=1+fix((s-n)/h); % 计算总共有多少帧 d=zeros((1+n/2),ncols); % 语谱图初始化 for b=0:h:(s-n) % 以下处理各帧 u=win.*x((b+1):(b+n)); % 各帧加窗 t=fft(u,n); % 各帧进行fft，内容为u，nfft=n。对于fft，输入n个时域点，输出n个频域点 d(:,c)=t(1:(1+n/2))&#39;; % 并联频谱向量，注意只取1+n/2，因为负频率无意义，只留下0和正频率 c=c+1; % 移动指针 end tt=[0:h:(s-n)]/Fs; % 时间轴 ff=[0:(n/2)]*Fs/n; % 频率轴 imagesc(tt/1000,ff/1000,20*log10(abs(d))); % 绘制 colormap(hot); axis xy xlabel(&#39;时间/s&#39;); ylabel(&#39;频率/kHz&#39;); 然而，matlab其实有封装好的函数可以直接调用。 [S,F,T]=specgram(x,nfft,Fs,windows_length,overlap_length) % x 为整段语音 % nfft 为fft变换点数，其实可以直接用默认的刚大于窗长的2的幂。也可自定义为大于窗长的整数，会对帧进行补零操作 % Fs 语音采样频率 % windows_length 窗长 % overlap_length 交叉长度 % S 语谱图 % F 频率值，尺度为1+n/2 % T 时间值，尺度为1+fix((s-n)/h) 语谱图的python提取有了刚才的基础，python的代码就容易理解啦。 from scipy import io from scipy.io import wavfile import matplotlib.pyplot as plt Fs, x = wavfile.read(&#39;audio.wav&#39;) # 读取音频 specg = plt.specgram(x, Fs = Fs, pad_to = 256, NFFT = 256, noverlap = 128) # 提取语谱图，一键操作！ io.savemat(&#39;specgram.mat&#39;, {&#39;specg&#39;:specg[0]}) # 保存语谱图 ## 照例解释下参数 # x，Fs和上边一样 # pad_to为上边的nfft # NFFT为上边的windows_length（为什么nfft不设置为上边的nfft呢，迷惑） # noverlap为上边的overlap_length 语谱图的一些可能有的小疑惑 关于nfftnfft既表示时域的点数也关联频域的点数。该数为2的幂数时更高效，但不是也没问题。nfft需要比窗长的值更大，然后加窗后的帧会被补零到nfft长度再进行fft。 关于频率分辨率频率轴上每一个点对应fs/nfft的频率。另外由于输出nfft/2+1个频率点，所以输出的频率范围为0到nfft/2×fs/nfft=fs/2。 关于自定义输出语谱图的尺寸问题时间轴尺寸为1+fix((s-n)/h)， 由windows_length和overlap_length决定。实际应用时由于各语音长度不同，时间尺寸一般都要进行截断或补零到一个固定值。截断的话可以截一段（起始信息，中间信息），也可以截多段（交叉不交叉都行）。频率轴尺寸为1+n/2，仅决定于nfft（python中的pad_to参数），所以可以通过设置该值控制频率轴尺寸。但是也不要比窗长大太多，否则补零太多可能就没什么信息了。nfft调大时，窗长可以跟着调大，为了防止导致的时间轴太短可以调高overlap_length。另外，其他参数不变时，仅变换nfft，可视化出来时可能肉眼看起来一样，但实际分辨率仍然是不同的。这也导致了一个问题，送入网络的是要用单通道的直接计算出来的语谱图，还是用可视化函数绘制出来的三通道的语谱图，这就根据实际情况去尝试了。 彩蛋 盆友们你们有在看吗！好无聊…]]></content>
      <categories>
        <category>语音特征</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[使用Github Pages和Hexo搭建自己的独立博客]]></title>
    <url>%2F2019%2F08%2F11%2FHow-to-use-Hexo-to-build-your-blog%2F</url>
    <content type="text"><![CDATA[前言 Github Pages: Github Pages可以被认为是用户编写的、托管在github上的静态网页。 Hexo: Hexo是一个快速、简洁且高效的博客框架。 安装Node.js点击此处访问官网，按需下载相应版本，默认安装可以了。 安装Hexo$ sudo npm install -g hexo-cli 若报错，尝试： $ sudo npm install --unsafe-perm --verbose -g hexo 初始化，建立博客项目选定博客网站项目程序文件的存放位置，如/Users/tobefans/Documents/Blog/，Bash中cd进入该目录下，执行命令： $ hexo init 执行完毕后，该命令将在当前目录下生成一套标准的Hexo博客项目模板 命令$ hexo g 生成静态网站文件 $ hexo s 启动本地服务器 $ hexo d 发布博客到GitHub 发布博客创建github.io仓库在自己的GitHub中，创建新仓库，标准命名为GitHub用户名.github.io，例如我的：tobefans.github.io 配置SSH密钥只有配置好 SSH 密钥后，我们才可以通过 git 操作实现本地代码库与 Github 代码库同步，在你第一次新建的文件夹里输入以下命令： $ ssh-keygen -t rsa -C &quot;your email@example.com&quot; //引号里面填写你的邮箱地址，比如我的是weiquan.fan96@gmail.com 之后复制~/.ssh/id_rsa.pub的公钥，在Github账号中添加进该公钥。 Git 会根据用户的名字和邮箱来记录提交，GitHub 也是用这些信息来做权限的处理，输入以下命令进行个人信息的设置，把名称和邮箱替换成你自己的，名字可以不是 GitHub 的昵称，但为了方便记忆，建议与 GitHub 一致。 $ git config --global user.name &quot;此处填你的用户名&quot; $ git config --global user.email &quot;此处填你的邮箱&quot; 可通过ssh -T git@github.com测试是否添加成功。 将本地的 Hexo 文件更新到 Github 的库中打开创建的 Hexo 文件夹下的 _config.yml，修改如下的代码段，repo为github项目的地址。 deploy: type: git repo: git@github.com:tobefans/tobefans.github.io.git branch: master 如果此时报以下错误，说明你的 deployer 没有安装成功 ERROR Deployer not found: git 需要执行以下命令再安装一次： $ npm install hexo-deployer-git --save 再执行 hexo g -d，你的博客就会部署到 Github 上了 访问博客在github该项目的setting中打开Github pages的设置。 你的博客地址：https://你的用户名.http://github.io，比如我的是：https://tobefans.github.io ，现在每个人都可以通过此链接访问你的博客了。 为博客更换自己喜欢的主题点击此处进入 Hexo 官网的主题专栏，我们可以看见有许多的主题供我们选择。我们要做的就是把主题克隆过来，在此我们以主题 Aero-Dual 为例，点进去我们就可以看见该主题作者的博客，鼠标滑到底，我们可以看见 Theme By Levblanc 的字样（其他主题类似），点击作者 Levblanc ，页面就会跳转到该主题所有的相关文件在 Github 上的地址，下载该项目，放至Hexo文件夹中的themes目录。 然后打开 Hexo 文件夹下的配置文件 _config.yml ，找到关键字 theme，修改参数为：theme：hexo-theme-aero-dual （其他主题修改成相应名称即可），再次注意冒号后面有一个空格。再通过$ hexo g更新。 另推荐一款主题：Next 在Hexo中渲染MathJax数学公式更换Hexo的markdown渲染引擎，hexo-renderer-kramed引擎是在默认的渲染引擎hexo-renderer-marked的基础上修改了一些bug，两者比较接近，也比较轻量级。在Hexo文件夹位置命令： npm uninstall hexo-renderer-marked --save npm install hexo-renderer-kramed --save 接下来到博客根目录下，找到node_modules\kramed\lib\rules\inline.js，把第11行的escape变量的值做相应的修改，这一步是在原基础上取消了对\,{,}的转义(escape)。同时把第20行的em变量也要做相应的修改。 // escape: /^\\([\\`*{}\[\]()#$+\-.!_&gt;])/, escape: /^\\([`*\[\]()#$+\-.!_&gt;])/ // em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/ 进入到主题目录，找到_config.yml配置问题，把mathjax默认的false修改为true，具体如下： # MathJax Support mathjax: enable: true per_page: true 在文章的Front-matter里打开mathjax开关，如下： --- title: 使用Github Pages和Hexo搭建自己的独立博客 date: 2019-08-11 19:10:47 tags: mathjax: true --- 这里有常见数学公式。 绑定域名通过阿里云之类的获得域名后，先解析域名如下。 记录类型 主机记录 记录值 CNAME www 你的github用户名.github.io A @ 192.30.252.153 A @ 192.30.252.154 在Hexo文件夹的source文件夹中，创建一个CNAME文件，存储预备使用的个人域名，如：weiquanfan.xyz也可通过github项目上setting里映射个人域名。 清理Hexo缓存并更新Hexo静态网站 $ hexo clean &amp;&amp; hexo g $ hexo clean $ hexo g -d Valine——评论系统Valine 诞生于 2017 年 8 月 7 日，是一款基于 Leancloud 的快速、简洁且高效的无后端评论系统。 注册帐号创建应用:请先登录或注册 LeanCloud, 进入控制台后点击左下角创建应用，选用免费的开发版应用即可。 应用设置:进入应用。在 设置 -&gt; 安全中心 ，把文件上传、短信服务、推送服务、实时通信这几个服务全部关闭，因为用不到。在 Web 安全域名 里填入想要打开评论系统的域名，如weiquanfan.xyz在 设置 -&gt; 应用 Key 找到APP ID和APP Key。 配置 Hexo 参数:打开 Hexo 主题的配置文件_config.yml，搜索一下 Valine，打开enable，并填写APP ID 和 APP Key。 更新网站:运行hexo g -d推送到博客。 此外，还需要注意，如果博客还有除正文内容之外的页面存在，例如关于、分类、标签，要把他们的 Markdown 文件的 comments 属性设置为 false，否则这些页面在展示的时候也会有评论的功能出现。 --- title: 使用Github Pages和Hexo搭建自己的独立博客 date: 2019-08-11 19:10:47 tags: mathjax: true comments: false --- 统计阅读量阅读量分两种：不蒜子统计站点的总访问量，即统计浏览了多少次；有多少人访问，在footer显示。LeanCloud统计单篇博文的阅读量，即统计单篇博文的阅读量是多少。 单篇博文的阅读量通过Valine实现，其实和评论系统差不多。在 存储 中新建Class，Class名称必须为Counter。并相应更改主题配置文件的leancloud_visitors，打开enable，并填写APP ID 和 APP Key。 站点的总访问量通过不蒜子实现。找到站点的themes/next/layout/_partials/footer.swig文件。插入代码如下。 {% if theme.footer.theme.enable %} {# #}{{ __('footer.theme') }} &mdash; {# #}{# #}NexT.{{ theme.scheme }}{# #}{% if theme.footer.theme.version %} v{{ theme.version }}{% endif %}&lt;/div&gt; # 此位置插入以下代码 &lt;div&gt; &lt;script async src=&quot;https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js&quot;&gt;&lt;/script&gt; 本站总访问量 &lt;span id=&quot;busuanzi_value_site_pv&quot;&gt;&lt;/span&gt; 次&amp;nbsp&amp;nbsp&amp;nbsp 本站访客数&lt;span id=&quot;busuanzi_value_site_uv&quot;&gt;&lt;/span&gt;人次 &lt;/div&gt; {% endif %} 彩蛋 喜大普奔，完成啦！！]]></content>
  </entry>
</search>

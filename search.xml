<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[常见的梯度下降算法原理]]></title>
    <url>%2F2020%2F05%2F04%2Fgradient-descent%2F</url>
    <content type="text"><![CDATA[前言梯度下降算法（Gradient Descent Optimization）是神经网络模型训练最常用的优化算法。对于深度学习模型，基本都是采用梯度下降算法来进行优化训练的。梯度下降算法背后的原理：目标函数 $J(\theta)$ 关于参数 $\theta$ 的梯度将是损失函数（loss function）上升最快的方向。而我们要最小化loss，只需要将参数沿着梯度相反的方向前进一个步长，就可以实现目标函数（loss function）的下降。这个步长 $\eta$ 又称为学习速率。 原始的梯度下降Batch gradient descent 批梯度下降，对所有的样本计算梯度后求平均，并更新参数。 因为在执行每次更新时，我们需要在整个数据集上计算所有的梯度，所以批梯度下降法的速度会很慢，同时，批梯度下降法无法处理超出内存容量限制的数据集。批梯度下降法同样也不能在线更新模型，即在运行的过程中，不能增加新的样本。 对于凸误差函数，批梯度下降法能够保证收敛到全局最小值，对于非凸函数，则收敛到一个局部最小值。 SGD 随机梯度下降，对每个样本计算梯度，并更新一次参数。 SGD的运行速度更快 可以用于在线学习 SGD以高方差频繁地更新，导致目标函数出现剧烈波动。 与批梯度下降法的收敛会使得损失函数陷入局部最小相比，由于SGD的波动性，一方面，波动性使得SGD可以跳到新的和潜在更好的局部最优。另一方面，这使得最终收敛到特定最小值的过程变得复杂，因为SGD会一直持续波动。然而，已经证明当我们缓慢减小学习率，SGD与批梯度下降法具有相同的收敛行为，对于非凸优化和凸优化，可以分别收敛到局部最小值和全局最小值。 Mini-batch GD 小批量梯度下降法最终结合了上述两种方法的优点，在每次更新时使用个小批量训练样本 减少参数更新的方差，这样可以得到更加稳定的收敛结果 可以利用最新的深度学习库中高度优化的矩阵优化方法，高效地求解每个小批量数据的梯度。 小结原始的梯度下降方法有以下问题： 在梯度平缓的维度下降非常慢，在梯度险峻的维度容易抖动 容易陷入局部极小值或鞍点。Zero gradient,gradient descent gets stuck （在高维空间中，鞍点比局部极小值更容易出现）-选择一个合适的学习率可能是困难的。学习率太小会导致收敛的速度很慢，学习率太大会妨碍收敛，导致损失函数在最小值附近波动甚至偏离最小值-学习率调整试图在训练的过程中通过例如退火的方法调整学习率，即根据预定义的策略或者当相邻两代之间的下降值小于某个阈值时减小学习率。然而，策略和阈值需要预先设定好，因此无法适应数据集的特点-对所有的参数更新使用同样的学习率。如果数据是稀疏的，同时，特征的频率差异很大时，我们也许不想以同样的学习率更新所有的参数，对于出现次数较少的特征，我们对其执行更大的学习率 带冲量的梯度下降Momentum optimization冲量梯度下降算法是Boris Polyak在1964年提出的，其基于这样一个物理事实：将一个小球从山顶滚下，其初始速率很慢，但在加速度作用下速率很快增加，并最终由于阻力的存在达到一个稳定速率。对于冲量梯度下降算法，其更新方程如下： 可以看到，参数更新时不仅考虑当前梯度值，而且加上了一个积累项（冲量），但多了一个超参，一般取接近1的值如0.9。相比原始梯度下降算法，冲量梯度下降算法有助于加速收敛。当梯度与冲量方向一致时，冲量项会增加，而相反时，冲量项减少，因此冲量梯度下降算法可以减少训练的震荡过程。 Nesterov Accelerated Gradient (NAG)NAG算法是Yurii Nesterov在1983年提出的对冲量梯度下降算法的改进版本，其速度更快。其变化之处在于计算“超前梯度”更新冲量项，具体公式如下： 学习率自适应的梯度下降AdaGradAdaGrad是Duchi在2011年提出的一种学习速率自适应的梯度下降算法。在训练迭代过程，其学习速率是逐渐衰减的，经常更新的参数其学习速率衰减更快，这是一种自适应算法。 其更新过程如下： 把每一维度的梯度^2和记录下来，每次学习率都除以这个和 每一维度的学习率不一样，且都在不断减小 在梯度大的维度，减小下降速度；在梯度小的维度，加快下降速度 让学习率适应参数，对于出现次数较少的特征，我们对其采用更大的学习率，对于出现次数较多的特征，我们对其采用较小的学习率。因此，Adagrad非常适合处理稀疏数据。 Adagrad算法的一个主要优点是无需手动调整学习率 Adagrad的一个主要缺点是它在分母中累加梯度的平方：由于每增加一个正项，在整个训练过程中，累加的和会持续增长。这会导致学习率变小以至于最终变得无限小，在学习率无限小时，Adagrad算法将无法取得额外的信息。 RMSpropRMSprop是Hinton在他的课程上讲到的，其算是对Adagrad算法的改进，主要是解决学习速率过快衰减的问题。其实思路很简单，类似Momentum思想，引入一个超参数，在积累梯度平方项进行衰减： 此时可以看到s是梯度平方的指数加权移动平均值，其中\gamma一般取值0.9，此时s更平稳，减少了出现的爆炸情况，因此有助于避免学习速率很快下降的问题。同时Hinton也建议学习速率设置为0.001。 Adaptive moment estimation (Adam)Adam是Kingma等在2015年提出的一种新的优化算法，其结合了Momentum和RMSprop算法的思想。相比Momentum算法，其学习速率是自适应的，而相比RMSprop，其增加了冲量项。所以，Adam是两者的结合体： 可以看到前两项和Momentum和RMSprop是非常一致的， 由于和的初始值一般设置为0，在训练初期其可能较小，第三和第四项主要是为了放大它们。最后一项是参数更新。其中超参数的建议值是 总结本文沿着梯度下降的发展大致介绍了各种常用的梯度下降算法，目前比较常用的应该仍是 Adam ， 不过我感觉其实 SGD 加梯度衰减策略可能能取得更好的效果，当然这需要设置得比较合适。 彩蛋]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[opensmile 工具的使用和批处理]]></title>
    <url>%2F2020%2F05%2F02%2Fopensmile%2F</url>
    <content type="text"><![CDATA[前言openSMILE是一款以命令行形式运行的工具，通过配置config文件来提取音频特征。主要应用于语音识别、情感计算、音乐信息获取。2.0版本之后的openSMILE包括了openCV库，可以用于视频处理和视频特征提取。官网下载有linux和windows版本提供下载，windows可以不编译直接用，建议在命令行里指明 openSMILE 绝对路径。 openSMILE的输入输出格式文件输入格式 RIFF-WAVE (PCM) (for MP3, MP4, OGG, etc. a converter needs to be used) Comma Separated Value (CSV) HTK parameter files WEKA’s ARFF format.（由htk工具产生） Video streams via openCV.（opencv产生的视频流数据） 文件输出格式 RIFF-WAVE (PCM uncompressed audio) Comma Separated Value (CSV) HTK parameter file WEKA ARFF file LibSVM feature file format Binary float matrix format 分类器和其他组件openSMILE还提供了许多VAD算法，用于判断各时间点有没有说话。 Voice Activity Detection based on Fuzzy Logic Voice Activity Detection based on LSTM-RNN with pre-trained models Turn-/Speech-segment detector LibSVM (on-line) LSTM-RNN (Neural Network) classifier which can load RNNLIB and CURRENNT nets GMM (experimental implementation from eNTERFACE’12 project, to be release soon) SVM sink (for loading linear kernel WEKA SMO models) Speech Emotion recognition pre-trained models (openEAR) openSMILE使用流程简介 先切换到处理文件SMILExtract.exe所在的目录 通过如下语句提取：windows下：SMILExtract_Release -C “配置文件” -I “要处理的音频” -O “要保存特征向量的路径及文件名”linux下：SMILExtract -C “配置文件” -I “要处理的音频” -O “要保存特征向量的路径及文件名” 官方配置文件官方提供了许多常见特征集的配置文件，如MFCC，PLP，以及各大语音比赛中效果好的特征集。 MFCC特征为了提取MFCC特征（兼容HTK），提供了以下四个文件（它们是以它们所代表的相应的HTK参数类型命名的）：MFCC12_0_D_A.conf此配置从25毫秒的音频帧中提取梅尔频率倒谱系数（以10毫秒的速率采样）（汉明窗口）。 它由26个Mel频带计算13个MFCC（0-12）组，并应用了一个权重参数为22的倒谱提升滤波器。13个一阶和13个二阶系数被附加到MFCC后。MFCC12_E_D_A.conf此配置跟MFCC12_0_D_A.conf一样，但对数能量是只加在MFCC1-12上。MFCC12_0_D_A_Z.conf这个配置跟MFCC12_0_D_A.conf配置一样，除了所有特征是参考整个输入序列进行了标准化。MFCC12_E_D_A_Z.conf这个配置跟MFCC12_E_D_A.conf配置一样，除了所有特征是参考整个输入序列进行了标准化。帧长为25ms,帧移为10ms，使用的汉明窗，预增强参数为0.97。由26个通过FFT功率谱计算的mel-滤波器组计算MFCC 0/1-12。MEL频谱的频率范围为0-8kHz，同时这些配置文件提供了-I,-O选项。输出文件格式是HTK参数文件格式。如果需要输出其他文件格式，你必须在配置文件中更改‘cHtkSink’组件类型为你想要的类型。命令行示例如下： SMILExtract -C config/MFCC12_E_D_A.conf -I input.wav -O output.mfcc.htk PLP特征用于提取PLP倒谱系数（PLP-CC）（与HTK兼容）以下四个文件（它们是以它们所代表的相应的HTK参数类型命名的）：PLP_0_D_A.conf该配置从25 ms长音频（以10ms的速率采样）帧提取Mel频率倒谱系数（汉明窗口）。它从26个Mel频带，并使用预测阶数为5计算6个PLP（0-5），并应用了一个权重参数为22的倒谱提升滤波器。6个一阶和6个二阶系数被附加到PLP-CC后。PLP_E_D_A.conf该配置与PLP_0_D_A.conf相同，但对数能量是只加在PLP1-12上。PLP_0_D_A_Z.conf此配置与PLP_0_D_A.conf相同，除了所有特征是参考整个输入序列进行了标准化。PLP_E_D_Z.conf此配置与PLP_E_D_A.conf相同，除了所有特征是参考整个输入序列进行了标准化。帧长为25ms,帧移为10ms，使用的汉明窗，预增强参数为0.97。由26个通过FFT功率谱计算的听觉mel-滤波器组(压缩系数为0.33)计算PLP 0/1-5。线性预测器的预测阶数为5。MEL频谱的频率范围为0-8kHz，同时这些配置文件提供了-I,-O选项。输出文件格式是HTK参数文件格式。如果需要输出其他文件格式，你必须在配置文件中更改‘cHtkSink’组件类型为你想要的类型。命令行示例如下： SMILExtract -C config/PLP_E_D_A.conf -I input.wav -O output.plp.htk 情感特征集自openSMILE在openEAR的项目EWS09情感识别中被使用，openSMILE提供了各种情感识别的标准特征集。The INTERSPEECH 2009 Emotion Challenge feature set（参见[SSB09]）由配置文件config/emo IS09.conf提供。它包含对LLDs应用统计函数得到的384个特征。该特征被保存在Arff格式（针对WEKA），新的实例会被附加到一个已存在文件（这是用于批处理，其中openSMILE被反复调用从多个文件提取特征到单个特征文件）。 出现在Arff文件中16个低级描述符（LLDs）的名称，见下面的列表： pcm_RMSenergy 信号帧均方根能量 mfcc 梅尔频率倒谱系数1-12 Pcm_zcr 时间信号的过零率（基于帧） voiceProb 从ACF计算的发声概率。 F0 从倒谱计算的基频 附加到低级描述符名称的后缀_sma表示它们是通过窗口长度为3的移动平均滤波器进行平滑。附加到sma的后缀_de表示当前特征是低级描述符平滑后的一阶delta系数（微分）。 max 轮廓的最大值 min 轮廓的最小值 range = max- min maxPos 最大值的绝对位置（以帧为单位） minPos 最小值的绝对位置（以帧为单位） amean 轮廓的算术平均值 linregc1 轮廓线性逼近的斜率（m） linregc2 轮廓线性逼近的偏移量（t） linregerrQ 计算的二次误差作为线性近似值和实际轮廓的差值 stddev 轮廓上的值的标准偏差 skewness 偏度（3阶矩） kurtosis 峰度（4阶矩） The INTERSPEECH 2010 Paralinguistic Challenge feature set（见2010年INTERSPEECH会议论文集）由配置文件config/IS10_paraling.conf提供。该集包含的1582个特征是由34个低级描述符（LLDs）和34个相应的delta作为68个LLDs轮廓值，在此基础上应用21个函数得到1428个特征，另外，对4个基于音高的LLD及其4个delta系数应用了19个函数得到152个特征，最后附加音高（伪音节）的数量和总数输入的持续时间（2个特征）。该特征被保存在Arff格式（针对WEKA），新的实例会被附加到一个已存在文件（这是用于批处理，其中openSMILE被反复调用从多个文件提取特征到单个特征文件）。 出现在Arff文件中34个低级描述符（LLDs）的名称，见下面的列表： pcm_loudness 归一化强度提高到0.3的幂的响度 mfcc 美尔频率倒谱系数0-14 logMelFreqBand 梅尔频带的对数功率0-7（分布范围内从0到8 kHz） lspFreq 从8个LPC系数计算出的8个线谱对频率。 F0finEnv 平滑的基频轮廓线。 voicingFinalUnclipped 最终基频候选的发声概率。Unclipped的意思是，当其低于浊音阈值时，它不被设置为零。 附加到低级描述符名称的后缀_sma表示它们是通过窗口长度为3的移动平均滤波器进行平滑。附加到sma的后缀_de表示当前特征是低级描述符平滑后的一阶delta系数（微分）。出现在Arff文件中的21个函数的名字,均在以下列表中： maxPos 最大值的绝对位置（以帧为单位） minPos 最小值的绝对位置（以帧为单位） amean 轮廓的算术平均值 linregc1 轮廓线性逼近的斜率（m） linregc2 轮廓线性逼近的偏移量（t） linregerrA 把线性误差计算作为线性近似值和实际的轮廓的误差 linregerrQ 把二次误差计算作为线性近似值和实际的轮廓的误差 stddev 轮廓中的值的标准偏差 skewness 偏度（3阶矩）。 kurtosis 峰度（4阶矩）。 quartile1 第一四分位数（25％百分位数） quartile2 第一四分位数（50％百分位数） quartile3 第一四分位数（75％百分位数） iqr1-2 四分位数间距：quartile2- quartile1 iqr2-3 四分位数间距：quartile3- quartile2 iqr1-3 四分位数间距：quartile3- quartile1 percentile1.0 轮廓的离群值鲁棒最小值，按1％百分位数表示。 percentile99.0 轮廓的离群值鲁棒最大值，按99％百分位数表示。 pctlrange0-1 由1％和99％的百分点的范围表示的离群值鲁棒信号范围“max-min”。 upleveltime75 信号超过（75％*范围+min）的时间百分比。 upleveltime90 信号超过（90％*范围+min）的时间百分比。 四个音高相关的LLD（及相应的delta系数）如下（清音区域均为0，因此功能仅适用于这些轮廓的浊音区域）： F0final 平滑的基频频率 jitterLocal 本地（帧到帧）抖动（音调周期长度偏差） jitterDDP 差分帧间抖动（‘Jitter of the Jitter’） shimmerLocal 本地（帧到帧）闪烁（音调周期幅度偏差） 对这4 + 4个LLD应用了19个函数，即上述21个函数的集合没有最小值（1％百分位数）和范围。 The INTERSPEECH 2011 Speaker State Challenge feature set（见2011年INTERSPEECH会议论文集）由配置文件config/IS11_speake_state.conf提供。该集包含的4368个特征是由4个能量相关+50个频谱相关的低级描述符（LLDs）和54个相应的delta作为108个LLDs，在此基础上应用33个基本函数+平均值、最小值、最大值、标准差得到3996个特征；5个声音相关和5个对应的delta作为10个LLDs，在此基础上应用33个基本函数+二次平均、上升时长、下降时长得到360个特征；6个F0基本函数和对应的delta，12个特征。 The INTERSPEECH 2012 Speaker Trait Challenge feature set（见2012年INTERSPEECH会议论文集）由配置文件config/IS12_speake_trait.conf提供。该集包含的6125个特征。 The INTERSPEECH 2013 ComParE Challenge feature set （见2013年INTERSPEECH会议论文集）由配置文件config/IS13_ComParE.conf提供。该集包含的6373个特征，LLD包括能量，频谱，倒谱（MFCC）、声音、对数谐波噪声比（HNR），频谱谐度和心理声学频谱清晰度。 The MediaEval 2012 TUM feature set for violent video scenes detection 针对好莱坞流行电影的暴力进行检测的特征集在config/mediaeval2012_tum_affect/，里面有不同的设置，参考文章：Florian Eyben, Felix Weninger, Nicolas Lehment, Gerhard Rigoll, Björn Schuller: ”Violent Scenes Detection with Large, Brute-forced Acoustic and Visual Feature Sets”, Proc. MediaEval 2012 Workshop, Pisa, Italy, 04.-05.10.2012. MediaEval Audio IS12based subwin2.conf包含的是从2s的子窗中提取音频特征的配置。MediaEval Audio IS12based subwin2 step0.5.conf提取一样的特征，但是2s子窗的偏移为0.5s。MediaEval VideoFunctionals.conf用于视频特征提取，如文章使用方法，需要一个包含LLDs的CSV文件（由openCV提取）作为输入和输出，ARFF文件作为视频特征。 The openSMILE/openEAR ‘emobase’ set早期的基线集（参照”emobase2”集作为新的基线集），拥有情感识别的998个声学特征，包含以下低级描述符（LLDs）：强度，响度，12 MFCC，音高（F0），浊音概率，F0包络线，8 LSF（线频谱频率），过零率， 以及这些LLD的Delta回归系数。以下函数被应用于上述LLDs及其Delta系数。：Max./Min。输入的相对位置和范围，范围，算术平均值，2线性回归系数，线性和二次误差，标准差，偏度，峰度，四分位数1-3和三位四分位数范围。 The large openSMILE emotion feature set用于提取更多的LLDs和更多的函数(6552个特征)，配置文件为config/emo_large.conf。 The openSMILE ‘emobase2010’ reference set 是基于the INTERSPEECH 2010 Paralinguistic Challenge feature set，配置文件为config/emobase2010.conf。对持续时间和位置特征的规范化进行了一些调整。这个特性集包含了一套大大增强的低级描述符(LLDs)，以及一套“emobase”相比更加精细化选择的函数列表。建议使用此特征集作为比较新的情感识别特征集和方法的参考，因为它代表当前最先进的情感和语言识别功能。该集合包含1582个特征（与INTERSPEECH 2010 Paralinguistic 挑战集相同设置），其由34个低级描述符（LLDs）和34个相应的delta作为68个LLDs轮廓值，在此基础上应用21个函数得到1 428个特征，另外，对4个基于音高的LLD及其4个delta系数应用了19个函数得到152个特征，最后附加音高（伪音节）的数量和总数输入的持续时间（2个特征）。唯一的区别是INTERSPEECH 2010 paralinguistic挑战集标准化的是是“maxPos”和“minPos”特征，本配置被标准化为段长度。 python批处理提取openSMILE特征所有支持标准数据输出格式的配置文件都可以在WINDOWS的批特征提取GUI（使用VS10 C#编写，位于progsrc/openSMILEbatchGUI/）。这个工具允许openSMILE自动的执行文件夹中的若干文件。它可以在图形界面中选择音频文件和指定输出类型。openSMILE本身提供批处理GUI（使用VS10 C#编写，位于progsrc/openSMILEbatchGUI/），但若语音数据的目录结构较复杂，还可以利用python来进行批处理。示例代码如以下： import os from subprocess import call def excute_CMD(path_ExcuteFile, path_Config, path_Audio, path_Output): cmd = path_ExcuteFile + &quot; -C &quot; + path_Config + &quot; -I &quot; + path_Audio + &quot; -O &quot; + path_Output call(cmd, shell=True) def batch_extract_features(path_Config, path_Input_Root, path_Output): path_ExcuteFile = &quot;SMILExtract_Release&quot; filename = os.listdir(path_Input_Root) for i in range(len(filename)): print(&#39;Extracting features of %s&#39; % filename[i]) path_Input = path_Input_Root + &#39;/&#39; + filename[i] + &#39;.wav&#39; excute_CMD(path_ExcuteFile, path_Config, path_Input, path_Output) path_Config = &quot;./config/IS13_ComParE.conf&quot; path_Input_Root = &#39;root_path_to_audio/&#39; path_Output = &#39;features.csv&#39; batch_extract_features(path_Config, path_Input_Root, path_Output) 输出数据格式控制对于不包含统计函数的配置文件，选项定义在config/shared/standard_data_output_lldonly.conf.inc ==============================LLD only============================= ================================CSV================================ -csvoutput &lt;filename&gt; 默认输出选项. CSV格式，存放帧向LLD -appendcsv &lt;0/1&gt; 设为1代表添加到已有CSV文件文末，默认0 -timestampcsv &lt;0/1&gt; 设为0禁止把时间步输出到CSV第二列，默认为1 -headercsv &lt;0/1&gt; 设为0禁止把标题输入到CSV，默认为1 ================================HTK================================ -output &lt;filename&gt; 输出特征汇总（函数）到HTK格式文件 ================================ARFF=============================== -arffoutput &lt;filename&gt; 默认输出选项. ARFF格式，存放帧向LLD -appendarff &lt;0/1&gt; 设为0代表不添加到已有ARFF文件文末，默认1添加 -timestamparff &lt;0/1&gt; 设为0禁止把时间步输出到ARFF第二列，默认为1 arfftargetsfile &lt;file&gt;指定配置包含定义目标域（类）的文，默认为:shared/arff_targets_conf.inc 对于包含统计函数的配置文件，如全部的INTERSPEECH和AVEC挑战集，选项定义在config/shared/standard_data_output.conf.inc =============================LLD and func ========================= -instname &lt;string&gt; 通常是输入文件的名称保存在CSV和ARFF输出的首列。默认是&quot;unknow&quot; ================================ARFF=============================== -lldarffoutput, -D &lt;filename&gt; 启动LLD帧向输出到ARFF格式文件 -appendarfflld &lt;0/1&gt; 设为1代表添加到已有ARFF文件文末，默认0覆盖 -timestamparfflld &lt;0/1&gt; 设为0禁止把时间步输出到ARFF第二列，默认为1 -lldarfftargetsfile &lt;file&gt; 指定配置包含定义目标域（类）的文，默认为: shared/arff_targets_conf.inc ================================CSV================================ -lldcsvoutput, -D &lt;filename&gt; 启动LLD帧向输出到CSV格式文件 -appendcsvlld &lt;0/1&gt; 设为1代表添加到已有CSV文件文末，默认0覆盖 -timestampcsvlld &lt;0/1&gt; 设为0禁止把时间步输出到CSV第二列，默认为1 -headercsvlld &lt;0/1&gt; 设为0禁止把标题输入到CSV，默认为1 ================================HTK================================ -lldhtkoutput &lt;filename&gt; 启动LLD帧向输出到HTK格式文件 ================================ARFF=============================== -output, -O &lt;filename&gt; 默认输出选项. ARFF格式，存放特征汇总 -appendarff &lt;0/1&gt; 设为0代表不添加到已有ARFF文件文末，默认1添加 -timestamparff &lt;0/1&gt; 设为1把时间步输出到ARFF第二列，默认为0 -arfftargetsfile &lt;file&gt;指定配置包含定义目标域（类）的文，默认为: shared/arff_targets_conf.inc ================================CSV================================ -csvoutput &lt;filename&gt; 默认输出选项. CSV格式，存放特征汇总 -appendcsv &lt;0/1&gt; 设为0代表不添加到已有CSV文件文末，默认1 -timestampcsv &lt;0/1&gt; 设为0禁止把时间步输出到CSV第二列，默认为1 -headercsv &lt;0/1&gt; 设为0禁止把标题输入到CSV，默认为1 ================================HTK================================ -htkoutput &lt;filename&gt; 输出特征汇总（函数）到HTK格式文件 如下为lldcsvoutput的定义。注：从2.2版本起，可以指定一个“?”替代文件名。它会禁止相应的输出组件，即它不会产生输出文件，在标准输出接口界面，看到的所有的文件名默认都是”?” [lldsink:cCsvSink] reader.dmLevel = lld;lld_de filename=\cm[lldcsvoutput(D){?}:output csv file for LLD, disabled by default ?, only written if filename given] instanceName=\cm[instname(N){unknown}:instance name] append = \cm[appendcsvlld{0}:set to 1 to append to the LLD output csv file, default is not to append] timestamp = \cm[timestampcsvlld{1}:set to 0 to suppress timestamp column, default is 1, i.e. to show timestamp in second column] number = 0 printHeader = \cm[headercsvlld{1}:set to 0 to suppress header line with feature names, default is 1, i.e. to show header line] errorOnNoOutput = 1 那么，当需要同时输出lld和func时，可用如下命令 SMILExtract -C config/IS13_ComParE.conf -I input.wav -lldcsvoutput lld_output.csv -csvoutput func_output.csv 最后一点话其实如果只是用官方配置提特征那么只看批处理那里也够了。官方配置文件可以根据需求时再看需要哪个文件，也可自己按着这个格式自定义编写配置文件。另外输出格式控制感觉最好也是先看一下，我一开始都是直接用 -O 输出统计特征，但想输出lld时跑去源代码里一阵捣鼓，后来才发现它已经封装好了直接一个参数就可以了。 彩蛋]]></content>
      <categories>
        <category>语音特征</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[语谱图的matlab提取和python提取]]></title>
    <url>%2F2020%2F05%2F02%2Fspecgram%2F</url>
    <content type="text"><![CDATA[前言语谱图（spectrogram或specgram），也叫声谱图，可以简单看做一个二维矩阵，其纵轴表示频率，横轴表示时间，矩阵的值表示能量强弱。由于它拥有着频率和时间两个维度的信息，所以是比较综合地表示原语音信息的一种特征。另外，我将其看做语音和图像的一种连接，因为图像领域的模型发展得较快，所以通过这种方式把语音转换成一种特殊的图像再进一步处理。 语谱图流程简介1. 将语音可交叉地分成多帧（由于语音的短时平稳性） 2. 各帧加窗 3. 各帧通过快速傅里叶变化（fft）得到频谱向量 4. 沿着时间轴并联各频谱向量得到语谱图 语谱图的提取语谱图的matlab提取先看一段非官方代码，结合上述步骤进行理解。 [x,Fs,nBits]=wavread(&#39;audio.wav&#39;); s=length(x); % 信号长度 w=256; % 窗长 n=w; % nfft，表示做fft变换需要的点数，一般为刚大于w的2的幂。举例，w=250，则n一般设为256 ov=w/2; % 分帧的交叉程度，常见设为窗长的二分之一或四分之一 h=w-ov; % 不重叠点数 win=hamming(n)&#39;;% 选了常见的汉明窗，并设置nfft c=1; % 指向当前帧的指针 ncols=1+fix((s-n)/h); % 计算总共有多少帧 d=zeros((1+n/2),ncols); % 语谱图初始化 for b=0:h:(s-n) % 以下处理各帧 u=win.*x((b+1):(b+n)); % 各帧加窗 t=fft(u,n); % 各帧进行fft，内容为u，nfft=n。对于fft，输入n个时域点，输出n个频域点 d(:,c)=t(1:(1+n/2))&#39;; % 并联频谱向量，注意只取1+n/2，因为负频率无意义，只留下0和正频率 c=c+1; % 移动指针 end tt=[0:h:(s-n)]/Fs; % 时间轴 ff=[0:(n/2)]*Fs/n; % 频率轴 imagesc(tt/1000,ff/1000,20*log10(abs(d))); % 绘制 colormap(hot); axis xy xlabel(&#39;时间/s&#39;); ylabel(&#39;频率/kHz&#39;); 然而，matlab其实有封装好的函数可以直接调用。 [S,F,T]=specgram(x,nfft,Fs,windows_length,overlap_length) % x 为整段语音 % nfft 为fft变换点数，其实可以直接用默认的刚大于窗长的2的幂。也可自定义为大于窗长的整数，会对帧进行补零操作 % Fs 语音采样频率 % windows_length 窗长 % overlap_length 交叉长度 % S 语谱图 % F 频率值，尺度为1+n/2 % T 时间值，尺度为1+fix((s-n)/h) 语谱图的python提取有了刚才的基础，python的代码就容易理解啦。 from scipy import io from scipy.io import wavfile import matplotlib.pyplot as plt Fs, x = wavfile.read(&#39;audio.wav&#39;) # 读取音频 specg = plt.specgram(x, Fs = Fs, pad_to = 256, NFFT = 256, noverlap = 128) # 提取语谱图，一键操作！ io.savemat(&#39;specgram.mat&#39;, {&#39;specg&#39;:specg[0]}) # 保存语谱图 ## 照例解释下参数 # x，Fs和上边一样 # pad_to为上边的nfft # NFFT为上边的windows_length（为什么nfft不设置为上边的nfft呢，迷惑） # noverlap为上边的overlap_length 语谱图的一些可能有的小疑惑 关于nfftnfft既表示时域的点数也关联频域的点数。该数为2的幂数时更高效，但不是也没问题。nfft需要比窗长的值更大，然后加窗后的帧会被补零到nfft长度再进行fft。 关于频率分辨率频率轴上每一个点对应fs/nfft的频率。另外由于输出nfft/2+1个频率点，所以输出的频率范围为0到nfft/2×fs/nfft=fs/2。 关于自定义输出语谱图的尺寸问题时间轴尺寸为1+fix((s-n)/h)， 由windows_length和overlap_length决定。实际应用时由于各语音长度不同，时间尺寸一般都要进行截断或补零到一个固定值。截断的话可以截一段（起始信息，中间信息），也可以截多段（交叉不交叉都行）。频率轴尺寸为1+n/2，仅决定于nfft（python中的pad_to参数），所以可以通过设置该值控制频率轴尺寸。但是也不要比窗长大太多，否则补零太多可能就没什么信息了。nfft调大时，窗长可以跟着调大，为了防止导致的时间轴太短可以调高overlap_length。另外，其他参数不变时，仅变换nfft，可视化出来时可能肉眼看起来一样，但实际分辨率仍然是不同的。这也导致了一个问题，送入网络的是要用单通道的直接计算出来的语谱图，还是用可视化函数绘制出来的三通道的语谱图，这就根据实际情况去尝试了。 彩蛋 盆友们你们有在看吗！好无聊…]]></content>
      <categories>
        <category>语音特征</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[使用Github Pages和Hexo搭建自己的独立博客]]></title>
    <url>%2F2019%2F08%2F11%2FHow-to-use-Hexo-to-build-your-blog%2F</url>
    <content type="text"><![CDATA[前言 Github Pages: Github Pages可以被认为是用户编写的、托管在github上的静态网页。 Hexo: Hexo是一个快速、简洁且高效的博客框架。 安装Node.js点击此处访问官网，按需下载相应版本，默认安装可以了。 安装Hexo$ sudo npm install -g hexo-cli 若报错，尝试： $ sudo npm install --unsafe-perm --verbose -g hexo 初始化，建立博客项目选定博客网站项目程序文件的存放位置，如/Users/tobefans/Documents/Blog/，Bash中cd进入该目录下，执行命令： $ hexo init 执行完毕后，该命令将在当前目录下生成一套标准的Hexo博客项目模板 命令$ hexo g 生成静态网站文件 $ hexo s 启动本地服务器 $ hexo d 发布博客到GitHub 发布博客创建github.io仓库在自己的GitHub中，创建新仓库，标准命名为GitHub用户名.github.io，例如我的：tobefans.github.io 配置SSH密钥只有配置好 SSH 密钥后，我们才可以通过 git 操作实现本地代码库与 Github 代码库同步，在你第一次新建的文件夹里输入以下命令： $ ssh-keygen -t rsa -C &quot;your email@example.com&quot; //引号里面填写你的邮箱地址，比如我的是weiquan.fan96@gmail.com 之后复制~/.ssh/id_rsa.pub的公钥，在Github账号中添加进该公钥。 Git 会根据用户的名字和邮箱来记录提交，GitHub 也是用这些信息来做权限的处理，输入以下命令进行个人信息的设置，把名称和邮箱替换成你自己的，名字可以不是 GitHub 的昵称，但为了方便记忆，建议与 GitHub 一致。 $ git config --global user.name &quot;此处填你的用户名&quot; $ git config --global user.email &quot;此处填你的邮箱&quot; 可通过ssh -T git@github.com测试是否添加成功。 将本地的 Hexo 文件更新到 Github 的库中打开创建的 Hexo 文件夹下的 _config.yml，修改如下的代码段，repo为github项目的地址。 deploy: type: git repo: git@github.com:tobefans/tobefans.github.io.git branch: master 如果此时报以下错误，说明你的 deployer 没有安装成功 ERROR Deployer not found: git 需要执行以下命令再安装一次： $ npm install hexo-deployer-git --save 再执行 hexo g -d，你的博客就会部署到 Github 上了 访问博客在github该项目的setting中打开Github pages的设置。 你的博客地址：https://你的用户名.http://github.io，比如我的是：https://tobefans.github.io ，现在每个人都可以通过此链接访问你的博客了。 为博客更换自己喜欢的主题点击此处进入 Hexo 官网的主题专栏，我们可以看见有许多的主题供我们选择。我们要做的就是把主题克隆过来，在此我们以主题 Aero-Dual 为例，点进去我们就可以看见该主题作者的博客，鼠标滑到底，我们可以看见 Theme By Levblanc 的字样（其他主题类似），点击作者 Levblanc ，页面就会跳转到该主题所有的相关文件在 Github 上的地址，下载该项目，放至Hexo文件夹中的themes目录。 然后打开 Hexo 文件夹下的配置文件 _config.yml ，找到关键字 theme，修改参数为：theme：hexo-theme-aero-dual （其他主题修改成相应名称即可），再次注意冒号后面有一个空格。再通过$ hexo g更新。 另推荐一款主题：Next 在Hexo中渲染MathJax数学公式更换Hexo的markdown渲染引擎，hexo-renderer-kramed引擎是在默认的渲染引擎hexo-renderer-marked的基础上修改了一些bug，两者比较接近，也比较轻量级。在Hexo文件夹位置命令： npm uninstall hexo-renderer-marked --save npm install hexo-renderer-kramed --save 接下来到博客根目录下，找到node_modules\kramed\lib\rules\inline.js，把第11行的escape变量的值做相应的修改，这一步是在原基础上取消了对\,{,}的转义(escape)。同时把第20行的em变量也要做相应的修改。 // escape: /^\\([\\`*{}\[\]()#$+\-.!_&gt;])/, escape: /^\\([`*\[\]()#$+\-.!_&gt;])/ // em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/ 进入到主题目录，找到_config.yml配置问题，把mathjax默认的false修改为true，具体如下： # MathJax Support mathjax: enable: true per_page: true 在文章的Front-matter里打开mathjax开关，如下： --- title: 使用Github Pages和Hexo搭建自己的独立博客 date: 2019-08-11 19:10:47 tags: mathjax: true --- 这里有常见数学公式。 绑定域名通过阿里云之类的获得域名后，先解析域名如下。 记录类型 主机记录 记录值 CNAME www 你的github用户名.github.io A @ 192.30.252.153 A @ 192.30.252.154 在Hexo文件夹的source文件夹中，创建一个CNAME文件，存储预备使用的个人域名，如：weiquanfan.xyz也可通过github项目上setting里映射个人域名。 清理Hexo缓存并更新Hexo静态网站 $ hexo clean &amp;&amp; hexo g $ hexo clean $ hexo g -d Valine——评论系统Valine 诞生于 2017 年 8 月 7 日，是一款基于 Leancloud 的快速、简洁且高效的无后端评论系统。 注册帐号创建应用:请先登录或注册 LeanCloud, 进入控制台后点击左下角创建应用，选用免费的开发版应用即可。 应用设置:进入应用。在 设置 -&gt; 安全中心 ，把文件上传、短信服务、推送服务、实时通信这几个服务全部关闭，因为用不到。在 Web 安全域名 里填入想要打开评论系统的域名，如weiquanfan.xyz在 设置 -&gt; 应用 Key 找到APP ID和APP Key。 配置 Hexo 参数:打开 Hexo 主题的配置文件_config.yml，搜索一下 Valine，打开enable，并填写APP ID 和 APP Key。 更新网站:运行hexo g -d推送到博客。 此外，还需要注意，如果博客还有除正文内容之外的页面存在，例如关于、分类、标签，要把他们的 Markdown 文件的 comments 属性设置为 false，否则这些页面在展示的时候也会有评论的功能出现。 --- title: 使用Github Pages和Hexo搭建自己的独立博客 date: 2019-08-11 19:10:47 tags: mathjax: true comments: false --- 统计阅读量阅读量分两种：不蒜子统计站点的总访问量，即统计浏览了多少次；有多少人访问，在footer显示。LeanCloud统计单篇博文的阅读量，即统计单篇博文的阅读量是多少。 单篇博文的阅读量通过Valine实现，其实和评论系统差不多。在 存储 中新建Class，Class名称必须为Counter。并相应更改主题配置文件的leancloud_visitors，打开enable，并填写APP ID 和 APP Key。 站点的总访问量通过不蒜子实现。找到站点的themes/next/layout/_partials/footer.swig文件。插入代码如下。 {% if theme.footer.theme.enable %} {# #}{{ __('footer.theme') }} &mdash; {# #}{# #}NexT.{{ theme.scheme }}{# #}{% if theme.footer.theme.version %} v{{ theme.version }}{% endif %}&lt;/div&gt; # 此位置插入以下代码 &lt;div&gt; &lt;script async src=&quot;https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js&quot;&gt;&lt;/script&gt; 本站总访问量 &lt;span id=&quot;busuanzi_value_site_pv&quot;&gt;&lt;/span&gt; 次&amp;nbsp&amp;nbsp&amp;nbsp 本站访客数&lt;span id=&quot;busuanzi_value_site_uv&quot;&gt;&lt;/span&gt;人次 &lt;/div&gt; {% endif %} 彩蛋 喜大普奔，完成啦！！]]></content>
  </entry>
</search>

{"meta":{"title":"vetch的小小世界","subtitle":null,"description":"陌上花开，可缓缓归矣","author":"Weiquan Fan","url":"http://weiquanfan.xyz","root":"/"},"pages":[{"title":"404 Not Found：该页无法显示","date":"2020-05-03T16:06:52.229Z","updated":"2019-08-02T02:17:12.000Z","comments":false,"path":"/404.html","permalink":"http://weiquanfan.xyz//404.html","excerpt":"","text":""},{"title":"Books","date":"2020-05-03T16:35:07.806Z","updated":"2020-05-03T16:35:07.806Z","comments":false,"path":"books/index.html","permalink":"http://weiquanfan.xyz/books/index.html","excerpt":"","text":""},{"title":"About","date":"2020-05-03T16:34:43.608Z","updated":"2020-05-03T16:34:43.608Z","comments":false,"path":"about/index.html","permalink":"http://weiquanfan.xyz/about/index.html","excerpt":"","text":"我是谁我在哪我要去哪里"},{"title":"Links","date":"2020-05-03T16:35:31.293Z","updated":"2020-05-03T16:35:31.293Z","comments":false,"path":"links/index.html","permalink":"http://weiquanfan.xyz/links/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2020-05-03T16:06:52.302Z","updated":"2019-08-02T02:17:12.000Z","comments":false,"path":"repository/index.html","permalink":"http://weiquanfan.xyz/repository/index.html","excerpt":"","text":""},{"title":"Tags","date":"2020-05-03T16:35:25.551Z","updated":"2020-05-03T16:35:25.551Z","comments":false,"path":"tags/index.html","permalink":"http://weiquanfan.xyz/tags/index.html","excerpt":"","text":""},{"title":"Categories","date":"2020-05-03T16:35:38.847Z","updated":"2020-05-03T16:35:38.847Z","comments":false,"path":"categories/index.html","permalink":"http://weiquanfan.xyz/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"nvidia驱动重装","slug":"nvidia-driver","date":"2021-08-22T14:10:15.000Z","updated":"2021-08-22T14:11:42.146Z","comments":true,"path":"2021/08/22/nvidia-driver/","link":"","permalink":"http://weiquanfan.xyz/2021/08/22/nvidia-driver/","excerpt":"","text":"在重启服务器的时候，发现nvidia驱动自动升级，使用 nvidia-smi 命令会报错如下，故重装nvidia驱动，参考Ubuntu18.04的驱动nvidia驱动升级为450版本后，ssh速度很慢的解决方案 NVIDIA-SMI has failed because it couldn&#39;t communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running. 卸载旧驱动sudo apt-get --purge remove nvidia* sudo apt autoremove 重启，使得所有nvidia残余进程被杀掉，否则无法安装 sudo shutdown -r now 下载安装驱动实验证明440.100版本驱动对18.04十分适合，要版本请自行查询。 wget https://cn.download.nvidia.com/XFree86/Linux-x86_64/440.100/NVIDIA-Linux-x86_64-440.100.run sudo chmod a+x NVIDIA-Linux-x86_64-440.100.run sudo ./NVIDIA-Linux-x86_64-440.100.run -no-x-check -no-nouveau-check -no-opengl-files 安装过程The distribution-provided pre-install script failed! Are you sure you want to continue? 选择 yes 继续。Would you like to register the kernel module souces with DKMS? This will allow DKMS to automatically build a new module, if you install a different kernel later? 选择 No 继续。Would you like to run the nvidia-xconfigutility to automatically update your x configuration so that the NVIDIA x driver will be used when you restart x? Any pre-existing x confile will be backed up. 选择No 重启sudo shutdown -r now 提升nvidia-smi运行速度方法1： sudo /usr/bin/nvidia-persistenced --verbose 也可直接把该命令放到开机自动运行 echo &quot;/usr/bin/nvidia-persistenced --verbose&quot; | sudo tee -a /etc/init.d/rc.local 方法2：设置持久模式：0/DISABLED,1/ENABLED sudo nvidia-smi -pm 1 完成","categories":[],"tags":[]},{"title":"语音特征小结","slug":"audio-features","date":"2021-08-22T12:36:20.000Z","updated":"2021-08-22T14:03:23.565Z","comments":true,"path":"2021/08/22/audio-features/","link":"","permalink":"http://weiquanfan.xyz/2021/08/22/audio-features/","excerpt":"","text":"前言本文汇总了一些常见或不常见的语音特征，持续更新中。 特征汇总韵律特征（prosodic feature）包含语音中音高、语调、能量、节奏变化等重要信息，表现为人昕觉系统感知到的“抑扬顿挫”，在语音信号处理的许多领域都有应用。基音频率、语速、能量等都是常用的韵律学特征。 基音频率（fundamental frequency, F0）是指发浊音时声带振动的频率，简称基频。人发声过程中来自肺部的气流冲击声门，形成一系列准周期的气流脉冲，经过声道的谐振及唇齿辐射最终形成语音信号，故浊音波形呈现一定的准周期性，这个周期就是基音周期，它和基频成倒数关系。基频变化范围很大，受性别、年龄、情绪等多种因素的影响。一般而言，男性的基频范围是135-185Hz，女性在260-350Hz之间。 基频检测方法主要有三类：1、时域：基于过零率，自相关等，最好的是YIN/PYIN算法。2、频域：倒谱，谐波，最佳梳妆滤波器等。3、统计方法：最大似然，rnn，HMM等都有。 语速（speaking rate） 特征表达了讲话速度的快慢，可以定义为单位时间内发音的词汇（或者音节）个数。语速受文化、环境、思维和表达能力多种因素的影响。和语速密切相关的因素还有停顿，是否考虑语段中的停顿对语速的计算数值有明显影响。 能量（energy）是与语音音量（或者说幅度）相关的声学特征。能量特征包含丰富的情感信息，比如人在悲伤时语音的能量通常会比较低。很早以前 vad（voice active detection) 中有一种检测语音方法：能量大的是语音，能量小的是噪声。当然，这种vad局限性非常大，用途很窄。 过零率 (zero-crossing rate) 核心点是计算信号跨越零点的次数，早期用于vad，判别语音和噪声，局限性也较大。 谱特征（spectral feature）含义相对宽泛，通常包含了语音信号的频谱、功率谱、倒频谱、频谱包络等特征。由于语音是短时平稳信号，所以通常用短时傅里叶变化对语音做分析，这样产生的特征能反映语音的短时特性。 梅尔倒谱系数（Mel-frequency Cepstral Coefficients, MFCC） 原理：根据人耳听觉机理的研究发现，人耳对不同频率的声波有不同的听觉敏感度。从200HZ到5000HZ对语音的清晰度影响最大。两个响度不等的声音作用于人耳时，则响度较高的频率成分的存在会影响到对响度较低的频率成分的成分，使其变得不易察觉，这种现象称为掩蔽效应。由于频率较低的声音在内耳蜗基底膜上行波传递的距离大于频率较高的声音，故一般来说，低音容易掩蔽高音，而高音掩蔽低音较困难。在低频处的声音掩蔽临界带宽较高频要小。所以从低频到高频这一频带内按临界带宽的大小由密到疏安排一组带通滤波器，对输入信号进行滤波。它是Mel标度频率域提取出来的倒谱系数，常用在语音识别和说话人识别领域。MFCC使用一组从低频到高频由密到疏交叠排列的三角形带通滤波器构建特征，这和人耳的听觉特性相符。出于计算复杂度的考虑，实际使用中MFCC系数通常取12-16阶。 Bark谱 (Bark Spectrogram)Bark谱与MFCC，Mel谱非常相似，都是将线性谱映射到非线性谱上的表征，而且都是低频带宽低，高频带宽高。 上世纪，研究者发现人耳结构对24个频点产生共振，根据这一理论，Eberhard Zwicker在1961年针对人耳特殊结构提出：信号在频带上也呈现出24个临界频带，分别从1到24。这就是Bark域。 CQT (Constant Q Transform)恒Q变换，也是一种非线性映射。指中心频率按指数规律分布，滤波带宽不同、但中心频率与带宽比为常量Q的滤波器组。与傅立叶变换不同，它频谱的横轴频率不是线性的，而是基于log2为底的，并且可以根据谱线频率的不同该改变滤波窗长度，以获得更好的性能。由于符合乐理常用在音乐中。 乐理里，所有的音都是由若干八度的12平均律共同组成的，12个半音等于一个八度，一个八度的跨度等于频率翻倍，所以一个半音等于2^(1/12)倍频。因此，音乐中的音调呈指数型跨度的，而CQT就很好的模拟了这种非线性度，以 log2 为底的非线性频谱。 共振峰（formant）是基频对应的整数次频率成分，用来反映人的声道物理特性。声道可以看成具有非均匀截面的声管，当准周期脉冲激励进入声道时会带动空气共振，产生一组共振频率，称为共振峰频率或简称共振峰。共振峰特征包括共振峰频率和频带宽度等。共振峰信息包含在语音频谱包络之中，在语音信号合成、语音识别中都有广泛应用。研究中常用的通常是前三个共振峰。共振峰的包络，位置，相邻的距离共同形成了音色特征。共振峰之间距离近听起来则偏厚粗，之间距离远听起来偏清澈。在男声变女声的时候，除了基频的移动，还需要调整共振峰，包括包络，距离等。否则将会丢失音色信息。 一个浊音用三个共振峰来表示，一个共振峰的频谱特性其实可以用一个GMM来建模拟合的，也就是说一个浊音是需要三个GMM来建模拟合，所以HMM需要三个状态也是这样来的；而非文本音素要用五个HMM状态，也是因为清音比较复杂，需要五个共振峰才能较好表示。 线性预测系数（Linear Predictive Coefficients, LPC） 是指对语音信号值进行线性预测的一组系数。具体而言，LPC是用若干个过去值的加权线性组合来逼近当前值，其中的权值即为线性预测系数。LPC能精确、便捷地表征语音短时能量的谱包络，并且能有效地估计基频、共振峰等语音参数。 Teager能量算子（Teager Energy Operation, TEO）是H.M.Teager提出的用以跟踪信号瞬时能量的非线性算子 。Teager指出，一个信号的能量不仅与它的幅度相关，而且与它的振动频率相关。TEO计算简单且时间分辨率高，对于解调信号效果很好。 声门波（glottal wave）反应了声门特性，是一种源特征。声门波可以获得比较准确的声道响应，它蕴含情感信息，对情感识别有一定作用。也有文章显示，从声门波的幅值提取出的特征，如NormalisedAmplitude Quotient (NAQ), the Quasi Open-Quotient (QOQ）等。 特征集汇总GeMAPS特征集GeMAPS特征集总共62个特征，这62个都是HSF特征，是由18个LLD特征计算得到。下面先介绍18个LLD特征，然后介绍62个HSF特征。这里只简单介绍每个特征的概念，不涉及具体计算细节。 18个LLD特征包括6个频率相关特征，3个能量/振幅相关特征，9个谱特征。6个频率相关特征包括：Pitch（log F0，在半音频率尺度上计算，从27.5Hz开始）；Jitter（单个连续基音周期内的偏差，偏差衡量的是观测变量与特定值的差，如果没有指明特定值通常使用的是变量的均值）；前三个共振峰的中心频率，第一个共振峰的带宽。3个能量/振幅的特征包括：Shimmer（相邻基音周期间振幅峰值之差），Loudness（从频谱中得到的声音强度的估计，可以根据能量来计算），HNR（Harmonics-to-noise）信噪比。9个谱特征包括，Alpha Ratio（50-1000Hz的能量和除以1-5kHz的能量和），Hammarberg Index（0-2kHz的最强能量峰除以2-5kHz的最强能量峰），Spectral Slope 0-500 Hz and 500-1500 Hz（对线性功率谱的两个区域0-500 Hz和500-1500 Hz做线性回归得到的两个斜率），Formant 1, 2, and 3 relative energy（前三个共振峰的中心频率除以基音的谱峰能量），Harmonic difference H1-H2（第一个基音谐波H1的能量除以第二个基音谐波的能量），Harmonic difference H1-A3（第一个基音谐波H1的能量除以第三个共振峰范围内的最高谐波能量）。 对18个LLD做统计，计算的时候是对3帧语音做symmetric moving average。首先计算算术平均和coefficient of variation（计算标准差然后用算术平均规范化），得到36个统计特征。然后对loudness和pitch运算8个函数，20百分位，50百分位，80百分位，20到80百分位之间的range，上升/下降语音信号的斜率的均值和标准差。这样就得到16个统计特征。上面的函数都是对voiced regions（非零的F0）做的。对Alpha Ratio，Hammarberg Index，Spectral Slope 0-500 Hz and 500-1500 Hz做算术平均得到4个统计特征。另外还有6个时间特征，每秒loudness峰的个数，连续voiced regions（F0&gt;0）的平均长度和标准差，unvoiced regions（F0=0）的平均长度和标准差，每秒voiced regions的个数。36+16+4+6得到62个特征。 eGeMAPS特征集（1）eGeMAPS是GeMAPS的扩展，在18个LLDs的基础上加了一些特征，包括5个谱特征：MFCC1-4和Spectral flux（两个相邻帧的频谱差异）和2个频率相关特征：第二个共振峰和第三个共振峰的带宽。（2）对这扩展的7个LLDs做算术平均和coefficient of variation（计算标准差然后用算术平均规范化）可以得到14个统计特征。对于共振峰带宽只在voiced region做，对于5个谱特征在voiced region和unvoiced region一起做。（3）另外，只在unvoiced region计算spectral flux的算术平均，然后只在voiced region计算5个谱特征的算术平均和coefficient of variation，得到11个统计特征。（4）另外，还加多一个equivalent sound level 。（5）所以总共得到14+11+1=26个扩展特征，加上原GeMAPS的62个特征，得到88个特征，这88个特征就是eGeMAPS的特征集。 ComParE特征集（1）ComParE，Computational Paralinguistics ChallengE，是InterSpeech上的一个挑战赛，从13年至今（2018年），每年都举办，每年有不一样的挑战任务。（2）从13年开始至今（2018年），ComParE的挑战都会要求使用一个设计好的特征集，这个特征集包含了6373个静态特征，是在LLD上计算各种函数得到的，称为ComParE特征集。（3）可以通过openSmile开源包来获得，另外前面提到的eGeMAPS也可以用openSmile获得。五：2009 InterSpeech挑战赛特征（1）前面说的6373维特征集ComparE是13年至今InterSpeech挑战赛中用的。（2）有论文还用了09年InterSpeech上Emotion Challenge提到的特征，总共有384个特征，计算方法如下。（3）首先计算16个LLD，过零率，能量平方根，F0，HNR（信噪比，有些论文也叫vp，voice probability 人声概率），MFCC1-12，然后计算这16个LLD的一阶差分，可以得到32个LLD。（4）对这32个LLD应用12个统计函数，最后得到32x12 = 384个特征。（5）同样可以通过openSmile来获得。 BoAW（BoAW，bag-of-audio-words，是特征的进一步组织表示，是根据一个codebook对LLDs做计算得到的。这个codebook可以是k-means的结果，也可以是对LLDs的随机采样。在论文会看到BoAW特征集的说法，指的是某个特征集的BoAW形式。比如根据上下文“使用特征集有ComparE和BoAW”，可以知道，这样的说法其实是指原来的特征集ComparE，和ComparE经过计算后得到的BoAW表示。可通过openXBOW开源包来获得BoAW表示。 YAAFE：使用YAAFE库提取到的特征，具体特征见YAAFE主页。 参考https://www.zhihu.com/question/24190826https://blog.csdn.net/weixin_42300798/article/details/113627332","categories":[{"name":"语音特征","slug":"语音特征","permalink":"http://weiquanfan.xyz/categories/语音特征/"}],"tags":[]},{"title":"VSCode的服务器和github同步","slug":"vscode","date":"2021-07-28T15:27:44.000Z","updated":"2021-07-28T15:29:41.939Z","comments":true,"path":"2021/07/28/vscode/","link":"","permalink":"http://weiquanfan.xyz/2021/07/28/vscode/","excerpt":"","text":"引言近期发现VSCode是一个非常强大的IDE，可以替换掉诸如xshell、winscp等多款软件，实现很好的本地、服务器、甚至github的同步。 公钥私钥配对不管是本地到服务器，本地到github，还是服务器到github，都可以通过配置密钥的方式，实现免密登录，因此先讲述如何进行密钥配对。配对的时候，要把公钥分给远程端，把私钥分给本地端（服务器发给另一个服务器时也可以视为本地端），公钥私钥唯一匹配，则可以成功登录。 生成新的密钥，在windows的cmd，或linux和mac的终端中，输入如下命令，确认后会生成两个文件，id_rsa和id_rsa.pub。前者是私钥，后者是公钥。ssh-keygen -t rsa -C &quot;email@email.com&quot; 找到生成的密钥，默认放置在 C:/Users/lenovo/.ssh/ ，用记事本之类的编辑器打开id_rsa.pub，复制内容。 把公钥复制到远程端。对于服务器，将复制的内容追加到 ~/.ssh/authorized_keys 。对于github，打开github -&gt; 点击头像 -&gt; Settings -&gt; SSH and GPG keys -&gt; New SSH key -&gt;黏贴key，随便写个title -&gt; 配置成功。 对于github，需要额外运行ssh-keyscan -t rsa github.com &gt;&gt; ~/.ssh/known_hosts 本地和服务器的同步 在插件扩展里安装微软官方发布的 Remote-SSH ，安装完后左侧会多一个远程资源管理器的图标，呈电脑状。 点击远程图标，点击SSH TARGETS旁边的齿轮图标进行配置，弹出的多行配置文件里选择第一个，确认进行配置。 进行如下配置，保存，会在SSH TARGETS下面出现名字为Name的远程机，会一直存在于远程资源管理器里，以后只需右键连接即可。Host Name HostName 1.1.1.1 # 填写远程服务器的IP或者Host User username # 填写登陆远程服务器的用户的名字 Port 22 # 填写端口，默认为22 IdentityFile C:\\\\Users\\\\lenovo\\\\.ssh\\\\id_rsa #填写私钥路径 右键连接到服务器后，点击菜单栏的文件，点击新建文件夹，就可直接选择服务器端的文件夹，而后文件夹就挂载到了左侧上边的第一个图标资源管理器，在这里的更改都会实时同步回服务器。 另外，在连接到服务器后，可以点击菜单栏的终端呼出新终端，就可以相当于xshell、putty直接在vscode上执行服务器上的终端命令了。 服务器和github的同步VSCode默认支持了很多git操作，就在左侧的源代码管理图标中，呈分支图状。在首次安装git的时候，需要先设置自己的用户名和邮箱（注册github时的用户名和邮箱）。 git config --global user.name &quot;name&quot; git config --global user.email &quot;email@email.com&quot; 一般而言，本地和github的同步操作如下： git init # 把这个文件夹变成Git可以管理的仓库 git add . # 把当前文件夹下的所有文件添加到暂存区 git add **.py # 把当前文件夹下的**.py添加到暂存区 git status # 可选操作，查看当前状态 git commit -m &quot;注释&quot; # 把暂存区的文件提交到本地仓库 git checkout master # 切换到master分支 git remote add origin https://github.com/name/Project.git # 关联github仓库，一个项目只需关联一次 git push origin main # 把本地仓库推向远程GitHub仓库的main分支 git pull origin main # 把远程GitHub仓库的main分支拉回本地仓库 而在VSCode中，这些命令大多数可以通过更方便的方式来替代。我们一方面可以通过调出终端，使用如上命令来进行同步，也可以通过点击源代码管理图标，直观地对文件夹中地文件进行同步操作。具体来说： git init 可以通过点击源代码管理图标里地初始化按钮替代。 git add 可以通过在文件旁边点击+号替代。 git commit 可以通过文件上面地消息来替代。 git remote add 通过按Ctrl+Shift+P调出命令行，输入git remote add，再进一步输入https://github.com/name/Project.git，再输入origin，完成绑定 git push 通过按Ctrl+Shift+P调出命令行，输入git push git pull 通过按Ctrl+Shift+P调出命令行，输入git pull git checkout 通过点击左下角地分支名字来更换 另外要注意： 一般上传流程是 add、commit、push，在这之前需要先上去github新建项目。 在同步过程中如果遇到了和github端冲突的问题，则需要先解决冲突，再继续上传。 对于本地和github的同步则基本一样，服务器可以作为另一种形式的本地。 总结目前VSCode的使用感受还不错，基本可以替代掉xshell和winscp，并可以实现多平台较好的同步。在与github的同步中，需要小心谨慎的做好版本管理，不要误删文件。一般需要每有一次较大改动就要push一次，并且要充分利用好branch分支功能。","categories":[{"name":"工具使用","slug":"工具使用","permalink":"http://weiquanfan.xyz/categories/工具使用/"}],"tags":[{"name":"工具使用","slug":"工具使用","permalink":"http://weiquanfan.xyz/tags/工具使用/"}]},{"title":"语音的预处理--端点检测","slug":"vad","date":"2021-07-28T15:27:39.000Z","updated":"2021-07-28T15:28:48.796Z","comments":true,"path":"2021/07/28/vad/","link":"","permalink":"http://weiquanfan.xyz/2021/07/28/vad/","excerpt":"","text":"引言语音的实际应用场景中，经常是给定一段包含多句句子的长语音，这就产生了语音端点检测的需求，从而实现对句子的分割。端点检测可以是只检测长语音的开始和结束，也可以细化到每一句句子的开始和结束，以下示例为句子级的端点检测。 方法1使用短时能量和谱质心特征进行端点检测，在matlab上有封装好的函数，以下为python版本。 #!/usr/bin/python3 # -*- coding: utf-8 -*- # @author: fan weiquan import os import numpy as np import matplotlib.pyplot as plt from scipy.io import wavfile import librosa import scipy.signal def ShortTimeEnergy(signal, windowLength, step): &quot;&quot;&quot; 计算短时能量 Parameters ---------- signal : 原始信号. windowLength : 帧长. step : 帧移. Returns ------- E : 每一帧的能量. &quot;&quot;&quot; signal = signal / np.max(signal) # 归一化 curPos = 0 L = len(signal) numOfFrames = np.asarray(np.floor((L-windowLength)/step) + 1, dtype=int) E = np.zeros((numOfFrames, 1)) for i in range(numOfFrames): window = signal[int(curPos):int(curPos+windowLength-1)]; E[i] = (1/(windowLength)) * np.sum(np.abs(window**2)); curPos = curPos + step; return E def SpectralCentroid(signal,windowLength, step, fs): &quot;&quot;&quot; 计算谱质心 Parameters ---------- signal : 原始信号. windowLength : 帧长. step : 帧移. fs : 采样率. Returns ------- C : 每一帧的谱质心. &quot;&quot;&quot; signal = signal / np.max(signal) # 归一化 curPos = 0 L = len(signal) numOfFrames = np.asarray(np.floor((L - windowLength) / step) + 1, dtype=int) H = np.hamming(windowLength) m = ((fs / (2 * windowLength)) * np.arange(1, windowLength, 1)).T C = np.zeros((numOfFrames, 1)) for i in range(numOfFrames): window = H * (signal[int(curPos) : int(curPos + windowLength)]) FFT = np.abs(np.fft.fft(window, 2 * int(windowLength))) FFT = FFT[1 : windowLength] FFT = FFT / np.max(FFT) C[i] = np.sum(m * FFT) / np.sum(FFT) if np.sum(window**2) &lt; 0.010: C[i] = 0.0 curPos = curPos + step; C = C / (fs/2) return C def findMaxima(f, step): &quot;&quot;&quot; 寻找局部最大值 Parameters ---------- f : 输入序列. step : 搜寻窗长. Returns ------- Maxima : 最大值索引 最大值 countMaxima : 最大值的数量 &quot;&quot;&quot; ## STEP 1: 寻找最大值 countMaxima = 0 Maxima = [] for i in range(len(f) - step - 1): # 对于序列中的每一个元素: if i &gt;= step: if (np.mean(f[i - step : i]) &lt; f[i]) and (np.mean(f[i + 1 : i + step + 1]) &lt; f[i]): # IF the current element is larger than its neighbors (2*step window) # --&gt; keep maximum: countMaxima = countMaxima + 1 Maxima.append([i, f[i]]) else: if (np.mean(f[0 : i + 1]) &lt;= f[i]) and (np.mean(f[i + 1 : i + step + 1]) &lt; f[i]): # IF the current element is larger than its neighbors (2*step window) # --&gt; keep maximum: countMaxima = countMaxima + 1 Maxima.append([i, f[i]]) ## STEP 2: 对最大值进行进一步处理 MaximaNew = [] countNewMaxima = 0 i = 0 while i &lt; countMaxima: # get current maximum: curMaxima = Maxima[i][0] curMavVal = Maxima[i][1] tempMax = [Maxima[i][0]] tempVals = [Maxima[i][1]] i = i + 1 # search for &quot;neighbourh maxima&quot;: while (i &lt; countMaxima) and (Maxima[i][0] - tempMax[len(tempMax) - 1] &lt; step / 2): tempMax.append(Maxima[i][0]) tempVals.append(Maxima[i][1]) i = i + 1 MM = np.max(tempVals) MI = np.argmax(tempVals) if MM &gt; 0.02 * np.mean(f): # if the current maximum is &quot;large&quot; enough: # keep the maximum of all maxima in the region: MaximaNew.append([tempMax[MI], f[tempMax[MI]]]) countNewMaxima = countNewMaxima + 1 # add maxima Maxima = MaximaNew countMaxima = countNewMaxima return Maxima, countMaxima def VAD(signal, fs): win = 0.05 step = 0.05 Eor = ShortTimeEnergy(signal, int(win * fs), int(step * fs)); Cor = SpectralCentroid(signal, int(win * fs), int(step * fs), fs); E = scipy.signal.medfilt(Eor[:, 0], 5) E = scipy.signal.medfilt(E, 5) C = scipy.signal.medfilt(Cor[:, 0], 5) C = scipy.signal.medfilt(C, 5) E_mean = np.mean(E); Z_mean = np.mean(C); Weight = 100 # 阈值估计的参数 # 寻找短时能量的阈值 Hist = np.histogram(E, bins=10) # 计算直方图 HistE = Hist[0] X_E = Hist[1] MaximaE, countMaximaE = findMaxima(HistE, 3) # 寻找直方图的局部最大值 if len(MaximaE) &gt;= 2: # 如果找到了两个以上局部最大值 T_E = (Weight*X_E[MaximaE[0][0]] + X_E[MaximaE[1][0]]) / (Weight + 1) else: T_E = E_mean / 2 # 寻找谱质心的阈值 Hist = np.histogram(C, bins=10) HistC = Hist[0] X_C = Hist[1] MaximaC, countMaximaC = findMaxima(HistC, 3) if len(MaximaC)&gt;=2: T_C = (Weight*X_C[MaximaC[0][0]]+X_C[MaximaC[1][0]]) / (Weight+1) else: T_C = Z_mean / 2 # 阈值判断 Flags1 = (E&gt;=T_E) Flags2 = (C&gt;=T_C) flags = np.array(Flags1 &amp; Flags2, dtype=int) ## 提取语音片段 count = 1 segments = [] while count &lt; len(flags): # 当还有未处理的帧时 # 初始化 curX = [] countTemp = 1 while ((flags[count - 1] == 1) and (count &lt; len(flags))): if countTemp == 1: # 如果是该语音段的第一帧 Limit1 = np.round((count-1)*step*fs)+1 # 设置该语音段的开始边界 if Limit1 &lt; 1: Limit1 = 1 count = count + 1 # 计数器加一 countTemp = countTemp + 1 # 当前语音段的计数器加一 if countTemp &gt; 1: # 如果当前循环中有语音段 Limit2 = np.round((count - 1) * step * fs) # 设置该语音段的结束边界 if Limit2 &gt; len(signal): Limit2 = len(signal) # 将该语音段的首尾位置加入到segments的最后一行 segments.append([int(Limit1), int(Limit2)]) count = count + 1 # 合并语音段 i = 0 while i &lt; len(segments)-1: # for i in range(len(segments) - 1): # 对每一个语音段进行处理 if segments[i][1] &gt;= segments[i + 1][0] - fs//2: segments[i][1] = segments[i + 1][1] # segments[i + 1, :] = [] del segments[i+1] i -= 1 i += 1 # 删除语音段 i = 0 while i &lt; len(segments): if segments[i][1] - segments[i][0] &lt; fs//2: del segments[i] i += 1 # seg_tuple = () # for i in range(len(segments)): seg_tuple+=tuple(segments[i][:]) return segments if __name__ == &quot;__main__&quot;: path_audio = &#39;test.wav&#39; signal, fs = librosa.load(path_audio, mono=True) segments = VAD(signal, fs) # 端点检测 print(segments) # for i, seg in enumerate(segments): # wavfile.write(str(i)+&#39;.wav&#39;, fs, signal[seg[0]:seg[1]]) 方法2基于webrtcvad的端点检测，逐帧判断是否是否静音，并通过平滑扩张等操作进一步调整端点。 from scipy.ndimage.morphology import binary_dilation import librosa import numpy as np import struct import webrtcvad import soundfile as sf # ** a的b次方 32767 int16_max = (2 ** 15) - 1 #输入 wav, sr = librosa.load(&quot;test.wav&quot;, sr=None) # 计算语音检测窗口大小 #为整除 30秒X16000=总帧长 samples_per_window = (30 * 16000) // 1000 # 修剪音频的结尾，使其具有窗口大小的倍数。使wav的长度能被 samples_per_window整除 wav = wav[:len(wav) - (len(wav) % samples_per_window)] # 浮点数波形转换为16位单声道PCM *：接收到的参数会形成一个元组，**：接收到的参数会形成一个字典。如下代码。 # webrtcvad 的 is_speech 接收的是buf 所以这里需要转换 pcm_wave = struct.pack(&quot;%dh&quot; % len(wav), *(np.round(wav * int16_max)).astype(np.int16)) # 执行语音激活检测 # timestamps = [] # flag = False voice_flags = [] # 这里共有三种帧长可以用到，分别是80/10ms，160/20ms，240/30ms。其它采样率 # 的48k，32k，24k，16k会重采样到8k来计算VAD。之所以选择上述三种帧长度，是因为语 # 音信号是短时平稳信号，其在10ms~30ms之间可看成平稳信号，高斯马尔科夫等比较 # 的信号处理方法基于的前提是信号是平稳的，在10ms~30ms，平稳信号处理方法是可 # 以使用的。 # 从vad的代码中可以看出，实际上，系统只处理默认10ms,20ms,30ms长度的数据， # 其它长度的数据没有支持，笔者修改过可以支持其它在10ms-30ms之间长度的帧长度 # 发现也是可以的。 # vad检测共四种模式，用数字0~3来区分，激进程度与数值大小正相关。 # 0: Normal，1：low Bitrate， 2：Aggressive；3：Very Aggressive 可以根据实际的使用 vad = webrtcvad.Vad(mode=0) for window_start in range(0, len(wav), samples_per_window): window_end = window_start + samples_per_window # append 进来的都是Boolean 这里以samples_per_windowx2 的长度去检测是否为人声 isspeech_bool = vad.is_speech(pcm_wave[window_start * 2:window_end * 2], sample_rate=16000) voice_flags.append(isspeech_bool) # if flag ^ isspeech_bool: # timestamps.append(window_start) # flag = isspeech_bool # for i in range(0, len(timestamps), 2): # tmp = wav[timestamps[i]:timestamps[i+1]] # sf.write(str(i)+&quot;.wav&quot;, tmp.astype(np.float32), sr, subtype=&#39;PCM_24&#39;) voice_flags = np.array(voice_flags) # 𝑣_𝑏𝑖𝑎𝑠𝑒𝑑𝑡=𝑣𝑡/(1−𝛽𝑡) # 滑动平均计算 def moving_average(array, width): # 拼接 bool 二值化 # width 执行滑动平均平滑时，帧的平均数。 # 该值越大，VAD变化必须越大才能平滑。 array_padded = np.concatenate((np.zeros((width - 1) // 2), array, np.zeros(width // 2))) # 一维数组累加 ret = np.cumsum(array_padded, dtype=float) ret[width:] = ret[width:] - ret[:-width] return ret[width - 1:] / width #滑动平均计算 audio_mask = moving_average(voice_flags, 8) #将平均数四舍五入 转bool audio_mask = np.round(audio_mask).astype(np.bool) # 扩张浊音区 使用多维二元膨胀 是数学形态学的方法 类似opencv 也有开闭运算 腐蚀膨胀 audio_mask = binary_dilation(audio_mask, np.ones(6 + 1)) #使其与wav一样大小 audio_mask = np.repeat(audio_mask, samples_per_window) timestamps = [] flag = False for i, t in enumerate(audio_mask): if flag ^ t: timestamps.append(i) flag = t for i in range(0, len(timestamps)-1, 2): res = wav[timestamps[i]:timestamps[i+1]] sf.write(str(i)+&quot;.wav&quot;, res.astype(np.float32), sr, subtype=&#39;PCM_24&#39;) # #通过这个遮罩扣掉没有声音那部分 # res=wav[audio_mask == True] # sf.write(&quot;out.wav&quot;, res.astype(np.float32), sr, subtype=&#39;PCM_24&#39;) 总结端点检测方法非常多，除这之外还有比较经典的双门限法。实际测试中，这些方法在干净的语音中效果非常好，在现实带噪场景中，可能容易出现误分成很多子段的现象，为此，可以通过增加时间规则，对分割后的时间段和上下段的连接段判断是否合理，来精简检测结果。 参考https://blog.csdn.net/qq_42688495/article/details/109333598https://blog.csdn.net/weixin_43928944/article/details/108378413","categories":[{"name":"语音预处理","slug":"语音预处理","permalink":"http://weiquanfan.xyz/categories/语音预处理/"}],"tags":[{"name":"语音预处理","slug":"语音预处理","permalink":"http://weiquanfan.xyz/tags/语音预处理/"}]},{"title":"语音的预处理--去噪","slug":"denose","date":"2021-07-27T15:50:34.000Z","updated":"2021-07-28T15:20:40.577Z","comments":true,"path":"2021/07/27/denose/","link":"","permalink":"http://weiquanfan.xyz/2021/07/27/denose/","excerpt":"","text":"引言在人工智能中，算法固然很重要，但语音的预处理却直接地决定了算法的性能上限，因此有必要对语音进行去噪处理。 方法1通过截取音频中的已知噪音部分，根据该噪音样本对整个音频进行降噪。截取噪音使用ffmpeg，降噪使用sox。 1. 将音频流和视频流拆分为2个不同的文件: 视频: ffmpeg -i input.mp4 -vcodec copy -an tmpvid.mp4 音频: ffmpeg -i input.mp4 -acodec pcm_s16le -ar 128k -vn tmpaud.wav 2. 从上一步的音频结果文件中剪切一个噪声样本: ffmpeg -i itmpaud.wav -ss 00:00:00.0 -t 00:00:00.5 noiseaud.wav -ss: 从开始的时间偏移. (h: m: s.ms). -t duration: 表示要剪切的音频段的持续时间（h: m: s.ms），以便下一步用来作为噪声文件。 选择一段没有语音、只有噪音的音频（例如，讲话者静音时的那一秒钟）。 3. 使用sox生成噪音profile: sox noiseaud.wav -n noiseprof noise.prof 4. 清除音频流中的噪声样本： sox tmpaud.wav tmpaud-clean.wav noisered noise.prof 0.21 更改0.21以调整采样率的灵敏度级别（0.2-0.3通常提供最佳结果）。 5. 使用ffmpeg将新的音频和视频流合并到一起: ffmpeg -i tmpvid.mp4 -i tmpaud-clean.wav -map 0:v -map 1:a -c:v copy -c:a aac -b:a 128k out.mp4 如果只是要简单的实现语音去噪，那么直接进行3、4步的操作，将整段噪声语音作为噪声文件也可。 sox tmpaud.wav -n noiseprof noise.prof sox tmpaud.wav tmpaud-clean.wav noisered noise.prof 0.21 方法2谱减法：谱减算法为最早的语音降噪算法之一，它的提出，基于一个简单的原理：假设语音中的噪声只有加性噪声，只要将带噪语音谱减去噪声谱，就可以得到纯净语音幅度。这么做的前提是噪声信号是平稳的或者缓慢变化的。 #!/usr/bin/env python import numpy as np import wave import math import ctypes as ct class FloatBits(ct.Structure): _fields_ = [ (&#39;M&#39;, ct.c_uint, 23), (&#39;E&#39;, ct.c_uint, 8), (&#39;S&#39;, ct.c_uint, 1) ] class Float(ct.Union): _anonymous_ = (&#39;bits&#39;,) _fields_ = [ (&#39;value&#39;, ct.c_float), (&#39;bits&#39;, FloatBits) ] def nextpow2(x): if x &lt; 0: x = -x if x == 0: return 0 d = Float() d.value = x if d.M == 0: return d.E - 127 return d.E - 127 + 1 # 打开WAV文档 f = wave.open(&quot;input.wav&quot;) # 读取格式信息 # (nchannels, sampwidth, framerate, nframes, comptype, compname) params = f.getparams() nchannels, sampwidth, framerate, nframes = params[:4] fs = framerate # 读取波形数据 str_data = f.readframes(nframes) f.close() # 将波形数据转换为数组 x = np.fromstring(str_data, dtype=np.short) # 计算参数 len_ = 20 * fs // 1000 # 样本中帧的大小 PERC = 50 # 窗口重叠占帧的百分比 len1 = len_ * PERC // 100 # 重叠窗口 len2 = len_ - len1 # 非重叠窗口 # 设置默认参数 Thres = 3 Expnt = 2.0 beta = 0.002 G = 0.9 # 初始化汉明窗 win = np.hamming(len_) # normalization gain for overlap+add with 50% overlap winGain = len2 / sum(win) # Noise magnitude calculations - assuming that the first 5 frames is noise/silence nFFT = 2 * 2 ** (nextpow2(len_)) noise_mean = np.zeros(nFFT) j = 0 for k in range(1, 6): noise_mean = noise_mean + abs(np.fft.fft(win * x[j:j + len_], nFFT)) j = j + len_ noise_mu = noise_mean / 5 # --- allocate memory and initialize various variables k = 1 img = 1j x_old = np.zeros(len1) Nframes = len(x) // len2 - 1 xfinal = np.zeros(Nframes * len2) # ========================= Start Processing =============================== for n in range(0, Nframes): # Windowing insign = win * x[k-1:k + len_ - 1] # compute fourier transform of a frame spec = np.fft.fft(insign, nFFT) # compute the magnitude sig = abs(spec) # save the noisy phase information theta = np.angle(spec) SNRseg = 10 * np.log10(np.linalg.norm(sig, 2) ** 2 / np.linalg.norm(noise_mu, 2) ** 2) def berouti(SNR): if -5.0 &lt;= SNR &lt;= 20.0: a = 4 - SNR * 3 / 20 else: if SNR &lt; -5.0: a = 5 if SNR &gt; 20: a = 1 return a def berouti1(SNR): if -5.0 &lt;= SNR &lt;= 20.0: a = 3 - SNR * 2 / 20 else: if SNR &lt; -5.0: a = 4 if SNR &gt; 20: a = 1 return a if Expnt == 1.0: # 幅度谱 alpha = berouti1(SNRseg) else: # 功率谱 alpha = berouti(SNRseg) ############# sub_speech = sig ** Expnt - alpha * noise_mu ** Expnt; # 当纯净信号小于噪声信号的功率时 diffw = sub_speech - beta * noise_mu ** Expnt # beta negative components def find_index(x_list): index_list = [] for i in range(len(x_list)): if x_list[i] &lt; 0: index_list.append(i) return index_list z = find_index(diffw) if len(z) &gt; 0: # 用估计出来的噪声信号表示下限值 sub_speech[z] = beta * noise_mu[z] ** Expnt # --- implement a simple VAD detector -------------- if SNRseg &lt; Thres: # Update noise spectrum noise_temp = G * noise_mu ** Expnt + (1 - G) * sig ** Expnt # 平滑处理噪声功率谱 noise_mu = noise_temp ** (1 / Expnt) # 新的噪声幅度谱 # flipud函数实现矩阵的上下翻转，是以矩阵的“水平中线”为对称轴 # 交换上下对称元素 sub_speech[nFFT // 2 + 1:nFFT] = np.flipud(sub_speech[1:nFFT // 2]) x_phase = (sub_speech ** (1 / Expnt)) * (np.array([math.cos(x) for x in theta]) + img * (np.array([math.sin(x) for x in theta]))) # take the IFFT xi = np.fft.ifft(x_phase).real # --- Overlap and add --------------- xfinal[k-1:k + len2 - 1] = x_old + xi[0:len1] x_old = xi[0 + len1:len_] k = k + len2 # 保存文件 wf = wave.open(&#39;output.wav&#39;, &#39;wb&#39;) # 设置参数 wf.setparams(params) # 设置波形文件 .tostring()将array转换为data wave_data = (winGain * xfinal).astype(np.short) wf.writeframes(wave_data.tostring()) wf.close() 总结个人感觉谱减法更加耗时，且适用场景相对有限。利用sox来去噪可能是一种相对更成熟的方法。 参考http://www.zoharbabin.com/how-to-do-noise-reduction-using-ffmpeg-and-sox/https://github.com/itaa/soja-box/tree/master/enhance_speach","categories":[{"name":"语音预处理","slug":"语音预处理","permalink":"http://weiquanfan.xyz/categories/语音预处理/"}],"tags":[{"name":"语音预处理","slug":"语音预处理","permalink":"http://weiquanfan.xyz/tags/语音预处理/"}]},{"title":"kaldi的安装和使用案例(librispeech)","slug":"kaldi-librispeech","date":"2021-04-22T14:21:43.000Z","updated":"2021-04-22T15:53:33.865Z","comments":true,"path":"2021/04/22/kaldi-librispeech/","link":"","permalink":"http://weiquanfan.xyz/2021/04/22/kaldi-librispeech/","excerpt":"","text":"1. kaldi的安装按照官网教程，kaldi的安装首先通过git获取项目，再进行编译。 git clone https://github.com/kaldi-asr/kaldi.git cd kaldi/tools/; make; cd ../src; ./configure; make 如果报错，则可能是相关的依赖项没有安装，可按照提示一步步安装(需要root权限)。 sudo apt-get install zlib1g-dev automake autoconf sox subversion sudo bash extras/install_mkl.sh 2. kaldi的使用2.1 librispeech的ASR模型训练egs目录下放着各个数据库的样例代码，一个文件夹就是一个数据库，非常全面。进入egs/librispeech/s5/.每个代码里边都会有一份cmd.sh(引入单机多卡run.pl或者多机多卡quene.pl模式), path.sh(引入各种kaldi的路径), run.sh(训练及测试的整个主流程)。以下主要细看run.sh，整体流程为 导入参数-&gt;下载部分数据并预处理-&gt;准备并创建语言模型-&gt;提取特征-&gt;训练部分数据集-&gt;训练单因素、三音素模型并变换训练-&gt;加入更多数据集-&gt;变换训练-&gt;加入全部数据集-&gt;变换训练-&gt;解码-&gt;训练tdnn模型。具体如下： #!/usr/bin/env bash ## 导入参数 data=/home/fwq/Project/kaldi/kaldi/data data_url=www.openslr.org/resources/12 lm_url=www.openslr.org/resources/11 mfccdir=mfcc stage=1 . ./cmd.sh . ./path.sh . parse_options.sh set -e ## 下载数据 if [ $stage -le 1 ]; then for part in dev-clean test-clean dev-other test-other train-clean-100; do local/download_and_untar.sh $data $data_url $part done local/download_lm.sh $lm_url data/local/lm fi ## 生成各种数据的各种文件，如wav.scp，text，utt2spk，spk2gender，utt2dur if [ $stage -le 2 ]; then for part in dev-clean test-clean dev-other test-other train-clean-100; do local/data_prep.sh $data/LibriSpeech/$part data/$(echo $part | sed s/-/_/g) done fi ## 准备语言模型，准备字典(local/prepare_dict_sh)，准备语言相关数据(utils/prepare_lang.sh)，格式化数据(local/format_lms.sh) if [ $stage -le 3 ]; then local/prepare_dict.sh --stage 3 --nj 30 --cmd &quot;$train_cmd&quot; \\ data/local/lm data/local/lm data/local/dict_nosp utils/prepare_lang.sh data/local/dict_nosp \\ &quot;&lt;UNK&gt;&quot; data/local/lang_tmp_nosp data/lang_nosp local/format_lms.sh --src-dir data/lang_nosp data/local/lm fi ## 用3-gram和4-gram语言模型创建ConstArpaLm格式语言模型 if [ $stage -le 4 ]; then utils/build_const_arpa_lm.sh data/local/lm/lm_tglarge.arpa.gz \\ data/lang_nosp data/lang_nosp_test_tglarge utils/build_const_arpa_lm.sh data/local/lm/lm_fglarge.arpa.gz \\ data/lang_nosp data/lang_nosp_test_fglarge fi ## 数据特征提取，提取mfcc，计算每条wav文件的均值方差 if [ $stage -le 5 ]; then if [[ $(hostname -f) == *.clsp.jhu.edu ]]; then utils/create_split_dir.pl /export/b{02,11,12,13}/$USER/kaldi-data/egs/librispeech/s5/$mfcc/storage \\ $mfccdir/storage fi fi if [ $stage -le 6 ]; then for part in dev_clean test_clean dev_other test_other train_clean_100; do steps/make_mfcc.sh --cmd &quot;$train_cmd&quot; --nj 40 data/$part exp/make_mfcc/$part $mfccdir steps/compute_cmvn_stats.sh data/$part exp/make_mfcc/$part $mfccdir done fi ## 训练100小时的小数据集 if [ $stage -le 7 ]; then utils/subset_data_dir.sh --shortest data/train_clean_100 2000 data/train_2kshort utils/subset_data_dir.sh data/train_clean_100 5000 data/train_5k utils/subset_data_dir.sh data/train_clean_100 10000 data/train_10k fi ## 训练单音素模型(mono) if [ $stage -le 8 ]; then steps/train_mono.sh --boost-silence 1.25 --nj 20 --cmd &quot;$train_cmd&quot; \\ data/train_2kshort data/lang_nosp exp/mono fi ## 对齐，训练三音素模型(tri1) if [ $stage -le 9 ]; then steps/align_si.sh --boost-silence 1.25 --nj 10 --cmd &quot;$train_cmd&quot; \\ data/train_5k data/lang_nosp exp/mono exp/mono_ali_5k steps/train_deltas.sh --boost-silence 1.25 --cmd &quot;$train_cmd&quot; \\ 2000 10000 data/train_5k data/lang_nosp exp/mono_ali_5k exp/tri1 fi ## 对齐，对三音素做LDA+MLLT变换(tri2b) if [ $stage -le 10 ]; then steps/align_si.sh --nj 10 --cmd &quot;$train_cmd&quot; \\ data/train_10k data/lang_nosp exp/tri1 exp/tri1_ali_10k steps/train_lda_mllt.sh --cmd &quot;$train_cmd&quot; \\ --splice-opts &quot;--left-context=3 --right-context=3&quot; 2500 15000 \\ data/train_10k data/lang_nosp exp/tri1_ali_10k exp/tri2b fi ## 对齐，对三音素做LDA+MLLT+SAT变换(tri3b) if [ $stage -le 11 ]; then steps/align_si.sh --nj 10 --cmd &quot;$train_cmd&quot; --use-graphs true \\ data/train_10k data/lang_nosp exp/tri2b exp/tri2b_ali_10k steps/train_sat.sh --cmd &quot;$train_cmd&quot; 2500 15000 \\ data/train_10k data/lang_nosp exp/tri2b_ali_10k exp/tri3b fi ## 对齐，对三音素做LDA+MLLT+SAT变换(tri4b) if [ $stage -le 12 ]; then steps/align_fmllr.sh --nj 20 --cmd &quot;$train_cmd&quot; \\ data/train_clean_100 data/lang_nosp \\ exp/tri3b exp/tri3b_ali_clean_100 steps/train_sat.sh --cmd &quot;$train_cmd&quot; 4200 40000 \\ data/train_clean_100 data/lang_nosp \\ exp/tri3b_ali_clean_100 exp/tri4b fi ## 从训练数据中计算发音和静音概率，并重新创建lang目录 if [ $stage -le 13 ]; then steps/get_prons.sh --cmd &quot;$train_cmd&quot; \\ data/train_clean_100 data/lang_nosp exp/tri4b utils/dict_dir_add_pronprobs.sh --max-normalize true \\ data/local/dict_nosp \\ exp/tri4b/pron_counts_nowb.txt exp/tri4b/sil_counts_nowb.txt \\ exp/tri4b/pron_bigram_counts_nowb.txt data/local/dict utils/prepare_lang.sh data/local/dict \\ &quot;&lt;UNK&gt;&quot; data/local/lang_tmp data/lang local/format_lms.sh --src-dir data/lang data/local/lm utils/build_const_arpa_lm.sh \\ data/local/lm/lm_tglarge.arpa.gz data/lang data/lang_test_tglarge utils/build_const_arpa_lm.sh \\ data/local/lm/lm_fglarge.arpa.gz data/lang data/lang_test_fglarge fi ## 对齐，训练nnet2模型，现在已经不这么用了，所以and了个false if [ $stage -le 14 ] &amp;&amp; false; then steps/align_fmllr.sh --nj 30 --cmd &quot;$train_cmd&quot; \\ data/train_clean_100 data/lang exp/tri4b exp/tri4b_ali_clean_100 local/nnet2/run_5a_clean_100.sh fi ## 合并360小时的数据，变成460小时 if [ $stage -le 15 ]; then local/download_and_untar.sh $data $data_url train-clean-360 local/data_prep.sh \\ $data/LibriSpeech/train-clean-360 data/train_clean_360 steps/make_mfcc.sh --cmd &quot;$train_cmd&quot; --nj 40 data/train_clean_360 \\ exp/make_mfcc/train_clean_360 $mfccdir steps/compute_cmvn_stats.sh \\ data/train_clean_360 exp/make_mfcc/train_clean_360 $mfccdir utils/combine_data.sh \\ data/train_clean_460 data/train_clean_100 data/train_clean_360 fi ## 对齐，做LDA+MLLT+SAT变换(tri5b) if [ $stage -le 16 ]; then steps/align_fmllr.sh --nj 40 --cmd &quot;$train_cmd&quot; \\ data/train_clean_460 data/lang exp/tri4b exp/tri4b_ali_clean_460 steps/train_sat.sh --cmd &quot;$train_cmd&quot; 5000 100000 \\ data/train_clean_460 data/lang exp/tri4b_ali_clean_460 exp/tri5b fi #local/nnet2/run_6a_clean_460.sh ## 合并500小时的数据，变成960小时 if [ $stage -le 17 ]; then local/download_and_untar.sh $data $data_url train-other-500 local/data_prep.sh \\ $data/LibriSpeech/train-other-500 data/train_other_500 steps/make_mfcc.sh --cmd &quot;$train_cmd&quot; --nj 40 data/train_other_500 \\ exp/make_mfcc/train_other_500 $mfccdir steps/compute_cmvn_stats.sh \\ data/train_other_500 exp/make_mfcc/train_other_500 $mfccdir utils/combine_data.sh \\ data/train_960 data/train_clean_460 data/train_other_500 fi ## 对齐，做LDA+MLLT+SAT变换(tri6b)，解码 if [ $stage -le 18 ]; then steps/align_fmllr.sh --nj 40 --cmd &quot;$train_cmd&quot; \\ data/train_960 data/lang exp/tri5b exp/tri5b_ali_960 steps/train_quick.sh --cmd &quot;$train_cmd&quot; \\ 7000 150000 data/train_960 data/lang exp/tri5b_ali_960 exp/tri6b utils/mkgraph.sh data/lang_test_tgsmall \\ exp/tri6b exp/tri6b/graph_tgsmall for test in test_clean test_other dev_clean dev_other; do steps/decode_fmllr.sh --nj 20 --cmd &quot;$decode_cmd&quot; \\ exp/tri6b/graph_tgsmall data/$test exp/tri6b/decode_tgsmall_$test steps/lmrescore.sh --cmd &quot;$decode_cmd&quot; data/lang_test_{tgsmall,tgmed} \\ data/$test exp/tri6b/decode_{tgsmall,tgmed}_$test steps/lmrescore_const_arpa.sh \\ --cmd &quot;$decode_cmd&quot; data/lang_test_{tgsmall,tglarge} \\ data/$test exp/tri6b/decode_{tgsmall,tglarge}_$test steps/lmrescore_const_arpa.sh \\ --cmd &quot;$decode_cmd&quot; data/lang_test_{tgsmall,fglarge} \\ data/$test exp/tri6b/decode_{tgsmall,fglarge}_$test done fi ## 划分“好”的数据来训练数据（tri6b_cleaned） if [ $stage -le 19 ]; then local/run_cleanup_segmentation.sh fi ## 训练和测试nnet3 tdnn模型 if [ $stage -le 20 ]; then local/chain/run_tdnn.sh fi 2.2 使用预训练模型测试自己的数据集首先，新建自己数据库的文件夹，并设置steps、utils、rnnlm的软链接。 ln -s /home/fwq/Project/kaldi/kaldi/egs/wsj/s5/utils utils ln -s /home/fwq/Project/kaldi/kaldi/egs/wsj/s5/steps steps ln -s /home/fwq/Project/kaldi/kaldi/scripts/rnnlm rnnlm 然后开始准备自己的数据库，kaldi需要的文件如下，这部分需要根据自己的数据库格式来编写生成，放置在data/corpus_name/里，以下corpus命名为test。 wav.scp：此列表包含系统中的语音ID和相应的WAV位置 utt2spk：话语ID和相应的说话者ID的列表。如果您没有发言人信息，则可以将utt-id复制为spk-id。 text：话语的转录。这将需要对您的解码输出进行评分。 再对数据进行排序、复制等处理。 utils/fix_data_dir.sh data/test utils/utt2spk_to_spk2utt.pl data/test/utt2spk &gt; data/test/spk2utt for datadir in test; do utils/copy_data_dir.sh data/$datadir data/${datadir}_hires done 有了数据，就要准备生成mfcc特征，需要新建一个conf文件夹，并新建conf/mfcc_hires.conf的配置文件，添加如下： -use-energy=false # use average of log energy, not energy. --num-mel-bins=40 # similar to Google&#39;s setup. --num-ceps=40 # there is no dimensionality reduction. --low-freq=20 # low cutoff frequency for mel bins... this is high-bandwidth data, so # there might be some information at the low end. --high-freq=-400 # high cutoff frequently, relative to Nyquist of 8000 (=7600) 然后就可以为数据计算特征和CMVN统计信息。 for datadir in test; do steps/make_mfcc.sh --nj 20 --mfcc-config conf/mfcc_hires.conf --cmd &quot;$train_cmd&quot; data/${datadir}_hires steps/compute_cmvn_stats.sh data/${datadir}_hires utils/fix_data_dir.sh data/${datadir}_hires done 接下来是预训练模型的下载和导入。默认情况下，内容将提取到data和exp目录。这里提供了2种语言模型：（tgsmall小三元组模型）和rnnlm（基于LSTM），这两种语言模型都经过LibriSpeech训练转录本的训练。我们将使用tgsmall模型进行解码，并使用RNNLM进行记录。 wget http://kaldi-asr.org/models/13/0013_librispeech_v1_chain.tar.gz wget http://kaldi-asr.org/models/13/0013_librispeech_v1_extractor.tar.gz wget http://kaldi-asr.org/models/13/0013_librispeech_v1_lm.tar.gz tar -xvzf 0013_librispeech_v1_chain.tar.gz tar -xvzf 0013_librispeech_v1_extractor.tar.gz tar -xvzf 0013_librispeech_v1_lm.tar.gz 使用i-vector提取器来获取测试数据的i-vector。 这会将100维i-向量提取到exp/nnet3_cleaned。 for data in test; do nspk=$(wc -l &lt;data/${data}_hires/spk2utt) steps/online/nnet2/extract_ivectors_online.sh --cmd &quot;$train_cmd&quot; --nj &quot;${nspk}&quot; \\ data/${data}_hires exp/nnet3_cleaned/extractor \\ exp/nnet3_cleaned/ivectors_${data}_hires done 使用tgsmallLM创建解码图。 export dir=exp/chain_cleaned/tdnn_1d_sp export graph_dir=$dir/graph_tgsmall utils/mkgraph.sh --self-loop-scale 1.0 --remove-oov \\ data/lang_test_tgsmall $dir $graph_dir 使用创建的图形进行解码。 export decode_cmd=&quot;run.pl&quot; for decode_set in test; do steps/nnet3/decode.sh --acwt 1.0 --post-decode-acwt 10.0 \\ --nj 8 --cmd &quot;$decode_cmd&quot; \\ --online-ivector-dir exp/nnet3_cleaned/ivectors_${decode_set}_hires \\ $graph_dir data/${decode_set}_hires $dir/decode_${decode_set}_tgsmall done 在核对之前检查WER。在这里，我们使用sclite评分，这在Kaldi中可用，并用于大多数egs。 for decode_set in test; do steps/score_kaldi.sh --cmd &quot;run.pl&quot; data/${decode_set}_hires $graph_dir $dir/decode_${decode_set}_tgsmall done cat exp/chain_cleaned/tdnn_1d_sp/decode_test_tgsmall/scoring_kaldi/best_wer %WER 57.15 [ 14722 / 25761, 2501 ins, 2559 del, 9662 sub ] exp/chain_cleaned/tdnn_1d_sp/decode_test_tgsmall/wer_17_1.0 使用RNNLM重新评分。 export decode_cmd=&quot;run.pl&quot; for decode_set in test; do decode_dir=exp/chain_cleaned/tdnn_1d_sp/decode_${decode_set}_tgsmall; rnnlm/lmrescore_pruned.sh \\ --cmd &quot;$decode_cmd&quot; \\ --weight 0.45 --max-ngram-order 4 \\ data/lang_test_tgsmall exp/rnnlm_lstm_1a \\ data/${decode_set}_hires ${decode_dir} \\ exp/chain_cleaned/tdnn_1d_sp/decode_${decode_set}_rescore done 计分包含在lmrescore_pruned.sh脚本中。 cat exp/chain_cleaned/tdnn_1d_sp/decode_test_rescore/wer_17_1.0 # %WER 56.12 [ 14456 / 25761, 2607 ins, 2452 del, 9397 sub ] 3. kaldi使用感受不同于以往用的python，kaldi的初步使用通过shell脚本来实现，它基于c++的底层，通过社区不同发展，现在已经有了非常庞大的脚本库。很多函数都实现了高效的封装，但如果要自己提特征训模型的话，还需要细看shell代码进一步看c++代码。 4. 参考文献参考1参考2","categories":[],"tags":[]},{"title":"了解一下Faster RCNN","slug":"fasterrcnn","date":"2020-11-19T08:27:38.000Z","updated":"2020-11-21T10:02:09.208Z","comments":true,"path":"2020/11/19/fasterrcnn/","link":"","permalink":"http://weiquanfan.xyz/2020/11/19/fasterrcnn/","excerpt":"","text":"1. 前言Faster RCNN 由 论文提出，是继R-CNN和Fast RCNN之后的目标检测上的又一力作。R-CNN提出selective search(SS)来搜索region proposal(RP)；Fast RCNN指出不必对每个RP各自提CNN特征，可以对原图提好CNN特征，再将SS找到的RP映射到CNN特征层上；Faster RCNN则提出了RPN层，将特征提取，proposal提取，bounding box整合在了一个网络中，极大地提高了检测速度。 2. 框架与流程Faster RCNN的模型框架如图。 可以分为4个主要内容： Conv layers。作为一种CNN网络目标检测方法，Faster RCNN首先使用一组基础的conv+relu+pooling层提取image的feature maps。该feature maps被共享用于后续RPN层和全连接层。 Region Proposal Networks。RPN网络用于生成region proposals。该层通过softmax判断anchors属于positive或者negative，再利用bounding box regression修正anchors获得精确的proposals。 Roi Pooling。该层收集输入的feature maps和proposals，综合这些信息后提取proposal feature maps，送入后续全连接层判定目标类别。 Classification。利用proposal feature maps计算proposal的类别，同时再次bounding box regression获得检测框最终的精确位置。 完整的网络图如下。 该网络对于一副任意大小PxQ的图像，首先缩放至固定大小MxN，然后将MxN图像送入网络； Conv layers中包含了13个conv层+13个relu层+4个pooling层； RPN网络首先经过3x3卷积，再分别生成positive anchors和对应bounding box regression偏移量，然后计算出proposals； Roi Pooling层则利用proposals从feature maps中提取proposal feature送入后续全连接和softmax网络作classification。 3. 模型细节3.1 Region Proposal Networks(RPN)从网络总图上可以看出，RPN 层可以分为上下两条支路，上面一条通过softmax分类anchors获得positive和negative分类，下面一条用于计算对于anchors的bounding box regression偏移量，以获得精确的proposal。而最后的Proposal层则负责综合positive anchors和对应bounding box regression偏移量获取proposals，同时剔除太小和超出边界的proposals。 3.1.1 anchorsanchors 是一组预设好的矩形。对于缩放至800×600的图，作者预设了9个anchors，坐标如下。 [[ -84. -40. 99. 55.] [-176. -88. 191. 103.] [-360. -184. 375. 199.] [ -56. -56. 71. 71.] [-120. -120. 135. 135.] [-248. -248. 263. 263.] [ -36. -80. 51. 95.] [ -80. -168. 95. 183.] [-168. -344. 183. 359.]] 其中每行的4个值表示矩形的左上和右下角点坐标。这9个矩形的长宽比为0.5、1或2，anchors中长宽1:2中最大为352x704，长宽2:1中最大736x384，这样就可以基本覆盖到整张图。 有了这些anchors，我们遍历Conv layers计算获得的feature maps，为每一个点都配备这9种anchors作为初始的检测框。这里如果有超出图像边缘的框，我们就对框进行裁剪，丢弃掉框外的部分。那么总共就有 (800//16) (600//16) 9 = 17100个anchor。 3.1.2 softmax分类一副MxN大小的矩阵送入Faster RCNN网络后，到RPN网络变为(M/16)x(N/16)，设为W×H。 在进入reshape与softmax之前，先做了1x1卷积，输出18（即2×9）层feature maps. 9表示九种anchor，2表示该anchor是否含有目标。 这里，为了进行softmax输出二分类结果，需要单独把‘2’这个维度孤立出来，因此在softmax前后各有一个reshape。数据的尺寸变化为：[1, 2x9, H, W] -&gt; [1, 2, Hx9, W], softmax -&gt; [1, 2x9, H, W]. 3.1.3 bounding box regression对于窗口一般使用四维向量(x,y,w,h)表示，分别表示窗口的中心点坐标和宽高。对于positive Anchors(设为A)，和groundtruth(设为G’)，我们的目标是寻找一种关系，使得输入原始的anchor A经过映射得到一个跟真实窗口G更接近的回归窗口G’。比较简单的思路就是: 注意，这里的平移dx和dy可以理解为相对于原宽长的平移因子，即相对于原宽长平移了多少倍的距离。缩放dw和dh可以理解为缩放了ln的dw和dh倍。 那么，对应于Faster RCNN原文，positive anchor与ground truth之间的平移量(tx, ty)与尺度因子(tw, th)如下： 训练bouding box regression网络回归分支时，标签是(tx,ty,tw,th)。 输入cnn feature，输出36（即4×9）层feature maps. 9表示九种anchor，4表示该anchor的平移量和缩放量。注意这里的平移缩放量是针对原M×N的尺寸的，而输入的feature是pooling后的尺寸。 3.1.4 Proposal LayerVGG输出 5038512 的特征，对应设置 5038k个anchors，而RPN输出： 大小为 50382k 的positive/negative softmax分类特征矩阵 大小为 50384k 的regression坐标回归特征矩阵 Proposal Layer负责综合所有平移缩放量和positive anchors，计算出精准的proposal，送入后续RoI Pooling Layer。 Proposal Layer forward 按照以下顺序依次处理： 按照输入的positive softmax scores由大到小排序anchors，提取前pre_nms_topN(e.g. 6000)个anchors，即提取修正位置后的positive anchors 限定超出图像边界的positive anchors为图像边界，防止后续roi pooling时proposal超出图像边界 剔除尺寸非常小的positive anchors 对剩余的positive anchors进行NMS（nonmaximum suppression），去掉大量重复框 Proposal Layer有3个输入：anchors是否有目标的分类器结果rpn_cls_prob_reshape，对应的bbox坐标(e.g. 300)，包含了缩放信息的im_info=[M, N, scale_factor]。然后输出300个 proposal=[x1, y1, x2, y2]。 3.2 RoI pooling由于RPN层输出的proposal尺寸不一，故提出了RoI pooling变换到统一的尺寸。Rol pooling层有2个输入： 原始的feature maps RPN输出的proposal boxes（大小各不相同） RoI Pooling layer forward过程： 由于proposal是对应MXN尺度的，所以首先使用spatial_scale=1/16将其映射回(M/16)X(N/16)大小的feature map尺度； 再将每个proposal对应的feature map区域水平分为 pooled_w × pooled_h 的网格； 对网格的每一份都进行max pooling处理。 这样处理后，即使大小不同的proposal输出结果都是 pooled_w × pooled_h 固定大小，实现了固定长度输出。 3.3 ClassificationClassification环节，利用已经获得的proposal feature maps，通过full connect层与softmax计算每个proposal具体属于那个类别（如人，车，电视等），输出cls_prob概率向量；同时再次利用bounding box regression获得每个proposal的位置偏移量bbox_pred，用于回归更加精确的目标检测框。 4. Faster RCNN的训练Faster RCNN训练过程分为6个步骤： 在已经训练好的model上，训练RPN网络，对应stage1_rpn_train.pt 利用步骤1中训练好的RPN网络，收集proposals，对应rpn_test.pt 第一次训练Fast RCNN网络，对应stage1_fast_rcnn_train.pt 第二训练RPN网络，对应stage2_rpn_train.pt 再次利用步骤4中训练好的RPN网络，收集proposals，对应rpn_test.pt 第二次训练Fast RCNN网络，对应stage2_fast_rcnn_train.pt 可以看到训练过程类似于一种“迭代”的过程，不过只循环了2次。至于只循环了2次的原因是应为作者提到：”A similar alternating training can be run for more iterations, but we have observed negligible improvements”，即循环更多次没有提升了。注意，在第二次训练时，RPN和Fast RCNN共享的网络层是冻结的。 5. 总结Faster RCNN是目标检测里two-stage的代表性杰作，在这之后还有一款用于目标检测和实例分割的Mask RCNN也为人称道。Mask RCNN类似于Faster RCNN的两个输出（预测框的坐标和类别），但多一条基于特征金字塔FCN网络的实例分割的mask通路，另外还将RoI pooling换成RoI align解决量化带来的边缘像素损失问题。后来，目标检测又有很多one-stage方法涌现，即一步直接生成预测框的坐标和类别，其中以YOLO和SSD为代表，它们最终输出k×(4+1+c)通道的特征图，其中4是坐标，1是前景、背景的置信度，c是类别数，c是anchor数。两者选择anchor框的策略不同，YOLO的anchor基于训练集所有框聚类得到宽和长，SSD由数学公式得到，且SSD在不同尺度的特征图上选取了不同的anchor数量(从而实现多尺度的检测)。两者的anchor框都只有宽度和高度，坐标x和y都默认在网格中心。 6. 参考文献https://zhuanlan.zhihu.com/p/31426458","categories":[],"tags":[]},{"title":"使用端点检测和百度语音识别技术实现视频的字幕生成","slug":"gen-srt","date":"2020-06-26T02:47:38.000Z","updated":"2020-06-26T02:49:49.855Z","comments":true,"path":"2020/06/26/gen-srt/","link":"","permalink":"http://weiquanfan.xyz/2020/06/26/gen-srt/","excerpt":"","text":"前言字幕文件中包含很多段信息，每一段表示了一句话的起始结束时间和内容，因此便涉及到了端点检测技术和语音识别技术。 端点检测：pydub.silence.detect_nonsilent 语音识别：aip.AipSpeech（百度接口）pip install pydub pip install baidu-aip 流程 视频提取音频 对音频进行端点检测，生成一句一句的音频 对各句音频进行语音识别 整合成字幕srt格式 代码#!/usr/bin/env python3 # -*- coding: utf-8 -*- from moviepy.editor import * from pydub import * from aip import AipSpeech video_file = r&#39;C:\\Users\\Lenovo\\Desktop\\video_sep\\test.mp4&#39; audio_file = r&#39;C:\\Users\\Lenovo\\Desktop\\test.wav&#39; srt_file = r&#39;C:\\Users\\Lenovo\\Desktop\\srt\\test.srt&#39; ## transform to audio video = VideoFileClip(video_file) video.audio.write_audiofile(audio_file, ffmpeg_params=[&#39;-ar&#39;,&#39;16000&#39;,&#39;-ac&#39;,&#39;1&#39;]) ## segment sound = AudioSegment.from_wav(audio_file) timestamp_list = silence.detect_nonsilent(sound, 700, sound.dBFS*1.3, 1) # look here for i in range(len(timestamp_list)): d = timestamp_list[i][1] - timestamp_list[i][0] print(&quot;Section is :&quot;, timestamp_list[i], &quot;duration is:&quot;, d) print(&#39;dBFS: {0}, max_dBFS: {1}, duration: {2}, split: {3}&#39;.format(round(sound.dBFS,2),round(sound.max_dBFS,2),sound.duration_seconds,len(timestamp_list))) def format_time(ms): hours = ms // 3600000 ms = ms % 3600000 minutes = ms // 60000 ms = ms % 60000 seconds = ms // 1000 mseconds = ms % 1000 return &#39;{:0&gt;2d}:{:0&gt;2d}:{:0&gt;2d},{:0&gt;3d}&#39;.format(hours, minutes, seconds, mseconds) ## 以下在百度AI开放平台申请获得 ## https://ai.baidu.com/tech/speech APP_ID = &#39;&#39; API_KEY = &#39;&#39; SECRET_KEY = &#39;&#39; client = AipSpeech(APP_ID, API_KEY, SECRET_KEY) idx = 0 text = [] for i in range(len(timestamp_list)): d = timestamp_list[i][1] - timestamp_list[i][0] data = sound[timestamp_list[i][0]:timestamp_list[i][1]].raw_data ## asr result = client.asr(data, &#39;pcm&#39;, 16000, {&#39;lan&#39;: &#39;zh&#39;,}) ## and look here if result[&#39;err_no&#39;] == 0: text.append(&#39;{0}\\n{1} --&gt; {2}\\n&#39;.format(idx, format_time(timestamp_list[i][0]), format_time(timestamp_list[i][1]))) text.append( result[&#39;result&#39;][0]) #.replace(&quot;，&quot;, &quot;&quot;) text.append(&#39;\\n&#39;) idx = idx + 1 # print(format_time(timestamp_list[i][0]/ 1000), &quot;txt is &quot;, result[&#39;result&#39;][0]) with open(srt_file,&quot;w&quot;) as f: f.writelines(text) 字幕生成的其他方式通过双门限法进行端点检测双门限法的原理是浊音的能量高于清音，清音的过零率高于无声部分。因此，其核心在于：先利用能量，将浊音部分区分出来，再利用过零率，将清音也提取出来，就完成了端点检测。 通过 SpeechRcognition 进行语音识别SpeechRcognition 可以说是一款语音识别集合器，共包含了谷歌、必应、IBM等七个识别器： recognize_bing()：Microsoft Bing Speech recognize_google()： Google Web Speech API recognize_google_cloud()：Google Cloud Speech - requires installation of the google-cloud-speech package recognize_houndify()： Houndify by SoundHound recognize_ibm()：IBM Speech to Text recognize_sphinx()：CMU Sphinx - requires installing PocketSphinx recognize_wit()：Wit.ai 基本使用方法如下： import speech_recognition as sr r = sr.Recognizer() test = sr.AudioFile(r&#39;C:\\Users\\Lenovo\\Desktop\\test.wav&#39;) with test as source: audio = r.record(source) r.recognize_google(audio, language=&#39;zh-CN&#39;, show_all= True) 但好像需要翻墙才能用… 通过autosub包直接生成字幕文件autosub是一个直接可以生成字幕文件的python库，详细可看中文教程.html)基本用法如下： autosub -S zh-CN -D zh-CN [你的视频/音频文件名] 不过这种方法也需要翻墙，我尝试了更改proxy也没什么效果… 总结总体而言，字幕生成需要的两个技术块，各有多种实现方法，而我最终选取的pydub加baidu-aip是相对简单并且有效的一种。不过实测效果并没有达到我的期望，因为一开始端点检测就不是十分准确，导致在错误的句子里上下文关系也不太对，语音识别也会有偏差了。更进一步的端点检测方法还得综合考虑能量和过零率，最好还要自定义地加上各个句子长度不能相差太大的限制等等。","categories":[],"tags":[{"name":"端点检测","slug":"端点检测","permalink":"http://weiquanfan.xyz/tags/端点检测/"},{"name":"语音识别","slug":"语音识别","permalink":"http://weiquanfan.xyz/tags/语音识别/"}]},{"title":"基于keras的简易语音识别","slug":"ASR-with-keras","date":"2020-06-20T07:13:59.000Z","updated":"2020-06-26T02:46:52.004Z","comments":true,"path":"2020/06/20/ASR-with-keras/","link":"","permalink":"http://weiquanfan.xyz/2020/06/20/ASR-with-keras/","excerpt":"","text":"简介最近忽然看到不是基于kaldi的ASR代码，尝试了一下发现效果还不错，搬上来记录一下。 源码地址： https://pan.baidu.com/s/1tFlZkMJmrMTD05cd_zxmAg 提取码：ndrr 数据集需要自行下载。 数据集数据集使用的是清华大学的thchs30中文数据，data文件夹中包含（.wav文件和.trn文件；trn文件里存放的是.wav文件的文字描述:第一行为词，第二行为拼音，第三行为音素）. 模型预测先直接解释有了训好的模型后如何使用，代码如下： # -*- coding: utf-8 -*- from keras.models import load_model from keras import backend as K import numpy as np import librosa from python_speech_features import mfcc import pickle import glob wavs = glob.glob(&#39;A2_8.wav&#39;) print(wavs) with open(&#39;dictionary.pkl&#39;, &#39;rb&#39;) as fr: [char2id, id2char, mfcc_mean, mfcc_std] = pickle.load(fr) mfcc_dim = 13 model = load_model(&#39;asr.h5&#39;) index = np.random.randint(len(wavs)) print(wavs[index]) ## 读取数据，并去除掉没说话的起始结束时间 audio, sr = librosa.load(wavs[index]) energy = librosa.feature.rmse(audio) frames = np.nonzero(energy &gt;= np.max(energy) / 5) indices = librosa.core.frames_to_samples(frames)[1] audio = audio[indices[0]:indices[-1]] if indices.size else audio[0:0] X_data = mfcc(audio, sr, numcep=mfcc_dim, nfft=551) X_data = (X_data - mfcc_mean) / (mfcc_std + 1e-14) print(X_data.shape) pred = model.predict(np.expand_dims(X_data, axis=0)) pred_ids = K.eval(K.ctc_decode(pred, [X_data.shape[0]], greedy=False, beam_width=10, top_paths=1)[0][0]) pred_ids = pred_ids.flatten().tolist() print(&#39;&#39;.join([id2char[i] for i in pred_ids])) 模型训练模型采用了 TDNN 网络结构，并直接通过字符级别来预测，直接根据常见度将字符对应成数字标签。整个流程而言， 先将一个个语音样本变成MFCC特征，即一个样本的维度为time*num_MFCC，time维度将被补齐到batch里最长的time。 将批量样本送入网络，采用1d卷积，仅在时间轴上卷积，一个样本的输出维度为time*(num_words+1)，加的1代表预测了空状态。 通过CTC Loss计算损失 # -*- coding: utf-8 -*- #导入相关的库 from keras.models import Model from keras.layers import Input, Activation, Conv1D, Lambda, Add, Multiply, BatchNormalization from keras.optimizers import Adam, SGD from keras import backend as K from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau import numpy as np import matplotlib.pyplot as plt from mpl_toolkits.axes_grid1 import make_axes_locatable import random import pickle import glob from tqdm import tqdm import os from python_speech_features import mfcc import scipy.io.wavfile as wav import librosa from IPython.display import Audio #读取数据集文件 text_paths = glob.glob(&#39;data/*.trn&#39;) total = len(text_paths) print(total) with open(text_paths[0], &#39;r&#39;, encoding=&#39;utf8&#39;) as fr: lines = fr.readlines() print(lines) #数据集文件trn内容读取保存到数组中 texts = [] paths = [] for path in text_paths: with open(path, &#39;r&#39;, encoding=&#39;utf8&#39;) as fr: lines = fr.readlines() line = lines[0].strip(&#39;\\n&#39;).replace(&#39; &#39;, &#39;&#39;) texts.append(line) paths.append(path.rstrip(&#39;.trn&#39;)) print(paths[0], texts[0]) #定义mfcc数 mfcc_dim = 13 #根据数据集标定的音素读入 def load_and_trim(path): audio, sr = librosa.load(path) energy = librosa.feature.rmse(audio) frames = np.nonzero(energy &gt;= np.max(energy) / 5) indices = librosa.core.frames_to_samples(frames)[1] audio = audio[indices[0]:indices[-1]] if indices.size else audio[0:0] return audio, sr #可视化，显示语音文件的MFCC图 def visualize(index): path = paths[index] text = texts[index] print(&#39;Audio Text:&#39;, text) audio, sr = load_and_trim(path) plt.figure(figsize=(12, 3)) plt.plot(np.arange(len(audio)), audio) plt.title(&#39;Raw Audio Signal&#39;) plt.xlabel(&#39;Time&#39;) plt.ylabel(&#39;Audio Amplitude&#39;) plt.show() feature = mfcc(audio, sr, numcep=mfcc_dim, nfft=551) print(&#39;Shape of MFCC:&#39;, feature.shape) fig = plt.figure(figsize=(12, 5)) ax = fig.add_subplot(111) im = ax.imshow(feature, cmap=plt.cm.jet, aspect=&#39;auto&#39;) plt.title(&#39;Normalized MFCC&#39;) plt.ylabel(&#39;Time&#39;) plt.xlabel(&#39;MFCC Coefficient&#39;) plt.colorbar(im, cax=make_axes_locatable(ax).append_axes(&#39;right&#39;, size=&#39;5%&#39;, pad=0.05)) ax.set_xticks(np.arange(0, 13, 2), minor=False); plt.show() return path Audio(visualize(0)) #提取音频特征并存储 features = [] for i in tqdm(range(total)): path = paths[i] audio, sr = load_and_trim(path) features.append(mfcc(audio, sr, numcep=mfcc_dim, nfft=551)) print(len(features), features[0].shape) #随机选择100个数据集 samples = random.sample(features, 100) samples = np.vstack(samples) #平均MFCC的值为了归一化处理 mfcc_mean = np.mean(samples, axis=0) #计算标准差为了归一化 mfcc_std = np.std(samples, axis=0) print(mfcc_mean) print(mfcc_std) #归一化特征 features = [(feature - mfcc_mean) / (mfcc_std + 1e-14) for feature in features] #将数据集读入的标签和对应id存储列表 chars = {} for text in texts: for c in text: chars[c] = chars.get(c, 0) + 1 chars = sorted(chars.items(), key=lambda x: x[1], reverse=True) chars = [char[0] for char in chars] print(len(chars), chars[:100]) char2id = {c: i for i, c in enumerate(chars)} id2char = {i: c for i, c in enumerate(chars)} data_index = np.arange(total) np.random.shuffle(data_index) train_size = int(0.9 * total) test_size = total - train_size train_index = data_index[:train_size] test_index = data_index[train_size:] #神经网络输入和输出X,Y的读入数据集特征 X_train = [features[i] for i in train_index] Y_train = [texts[i] for i in train_index] X_test = [features[i] for i in test_index] Y_test = [texts[i] for i in test_index] batch_size = 16 #定义训练批次的产生，一次训练16个 def batch_generator(x, y, batch_size=batch_size): offset = 0 while True: offset += batch_size if offset == batch_size or offset &gt;= len(x): data_index = np.arange(len(x)) np.random.shuffle(data_index) x = [x[i] for i in data_index] y = [y[i] for i in data_index] offset = batch_size X_data = x[offset - batch_size: offset] Y_data = y[offset - batch_size: offset] X_maxlen = max([X_data[i].shape[0] for i in range(batch_size)]) Y_maxlen = max([len(Y_data[i]) for i in range(batch_size)]) X_batch = np.zeros([batch_size, X_maxlen, mfcc_dim]) Y_batch = np.ones([batch_size, Y_maxlen]) * len(char2id) X_length = np.zeros([batch_size, 1], dtype=&#39;int32&#39;) Y_length = np.zeros([batch_size, 1], dtype=&#39;int32&#39;) for i in range(batch_size): X_length[i, 0] = X_data[i].shape[0] X_batch[i, :X_length[i, 0], :] = X_data[i] Y_length[i, 0] = len(Y_data[i]) Y_batch[i, :Y_length[i, 0]] = [char2id[c] for c in Y_data[i]] inputs = {&#39;X&#39;: X_batch, &#39;Y&#39;: Y_batch, &#39;X_length&#39;: X_length, &#39;Y_length&#39;: Y_length} outputs = {&#39;ctc&#39;: np.zeros([batch_size])} yield (inputs, outputs) epochs = 50 num_blocks = 3 filters = 128 X = Input(shape=(None, mfcc_dim,), dtype=&#39;float32&#39;, name=&#39;X&#39;) Y = Input(shape=(None,), dtype=&#39;float32&#39;, name=&#39;Y&#39;) X_length = Input(shape=(1,), dtype=&#39;int32&#39;, name=&#39;X_length&#39;) Y_length = Input(shape=(1,), dtype=&#39;int32&#39;, name=&#39;Y_length&#39;) #卷积1层 # 一维卷积，默认channels_last，即通道维(MFCC特征维)放最后，对时间维进行卷积 def conv1d(inputs, filters, kernel_size, dilation_rate): return Conv1D(filters=filters, kernel_size=kernel_size, strides=1, padding=&#39;causal&#39;, activation=None, dilation_rate=dilation_rate)(inputs) #标准化函数 def batchnorm(inputs): return BatchNormalization()(inputs) #激活层函数 def activation(inputs, activation): return Activation(activation)(inputs) #全连接层函数 def res_block(inputs, filters, kernel_size, dilation_rate): hf = activation(batchnorm(conv1d(inputs, filters, kernel_size, dilation_rate)), &#39;tanh&#39;) hg = activation(batchnorm(conv1d(inputs, filters, kernel_size, dilation_rate)), &#39;sigmoid&#39;) h0 = Multiply()([hf, hg]) ha = activation(batchnorm(conv1d(h0, filters, 1, 1)), &#39;tanh&#39;) hs = activation(batchnorm(conv1d(h0, filters, 1, 1)), &#39;tanh&#39;) return Add()([ha, inputs]), hs h0 = activation(batchnorm(conv1d(X, filters, 1, 1)), &#39;tanh&#39;) shortcut = [] for i in range(num_blocks): for r in [1, 2, 4, 8, 16]: h0, s = res_block(h0, filters, 7, r) shortcut.append(s) h1 = activation(Add()(shortcut), &#39;relu&#39;) h1 = activation(batchnorm(conv1d(h1, filters, 1, 1)), &#39;relu&#39;) #softmax损失函数输出结果 Y_pred = activation(batchnorm(conv1d(h1, len(char2id) + 1, 1, 1)), &#39;softmax&#39;) sub_model = Model(inputs=X, outputs=Y_pred) #计算损失函数 def calc_ctc_loss(args): y, yp, ypl, yl = args return K.ctc_batch_cost(y, yp, ypl, yl) ctc_loss = Lambda(calc_ctc_loss, output_shape=(1,), name=&#39;ctc&#39;)([Y, Y_pred, X_length, Y_length]) #加载模型训练 model = Model(inputs=[X, Y, X_length, Y_length], outputs=ctc_loss) #建立优化器 optimizer = SGD(lr=0.02, momentum=0.9, nesterov=True, clipnorm=5) #激活模型开始计算 model.compile(loss={&#39;ctc&#39;: lambda ctc_true, ctc_pred: ctc_pred}, optimizer=optimizer) checkpointer = ModelCheckpoint(filepath=&#39;asr.h5&#39;, verbose=0) lr_decay = ReduceLROnPlateau(monitor=&#39;loss&#39;, factor=0.2, patience=1, min_lr=0.000) #开始训练 history = model.fit_generator( generator=batch_generator(X_train, Y_train), steps_per_epoch=len(X_train) // batch_size, epochs=epochs, validation_data=batch_generator(X_test, Y_test), validation_steps=len(X_test) // batch_size, callbacks=[checkpointer, lr_decay]) #保存模型 sub_model.save(&#39;asr.h5&#39;) #将字保存在pl=pkl中 with open(&#39;dictionary.pkl&#39;, &#39;wb&#39;) as fw: pickle.dump([char2id, id2char, mfcc_mean, mfcc_std], fw) train_loss = history.history[&#39;loss&#39;] plt.plot(np.linspace(1, epochs, epochs), train_loss, label=&#39;train&#39;) plt.legend(loc=&#39;upper right&#39;) plt.xlabel(&#39;Epoch&#39;) plt.ylabel(&#39;Loss&#39;) plt.show() #下面是模型的预测效果，可见main.py from keras.models import load_model import pickle with open(&#39;dictionary.pkl&#39;, &#39;rb&#39;) as fr: [char2id, id2char, mfcc_mean, mfcc_std] = pickle.load(fr) sub_model = load_model(&#39;asr.h5&#39;) def random_predict(x, y): index = np.random.randint(len(x)) feature = x[index] text = y[index] pred = sub_model.predict(np.expand_dims(feature, axis=0)) pred_ids = K.eval(K.ctc_decode(pred, [feature.shape[0]], greedy=False, beam_width=10, top_paths=1)[0][0]) pred_ids = pred_ids.flatten().tolist() print(&#39;True transcription:\\n-- &#39;, text, &#39;\\n&#39;) print(&#39;Predicted transcription:\\n-- &#39; + &#39;&#39;.join([id2char[i] for i in pred_ids]), &#39;\\n&#39;) random_predict(X_train, Y_train) random_predict(X_test, Y_test) 总结对比其他的分类任务，语音识别多了个解码过程，这也导致了目前在常见的深度学习框架中还没有很好的ASR框架，目前而言，CTC的应用也导致出现了一些完全端到端的ASR系统，相信以后这也会是个大趋势。","categories":[{"name":"语音识别","slug":"语音识别","permalink":"http://weiquanfan.xyz/categories/语音识别/"}],"tags":[{"name":"语音识别","slug":"语音识别","permalink":"http://weiquanfan.xyz/tags/语音识别/"}]},{"title":"使用QT designer、python搭建界面程序","slug":"QT","date":"2020-06-10T06:49:28.000Z","updated":"2020-06-20T07:08:22.853Z","comments":true,"path":"2020/06/10/QT/","link":"","permalink":"http://weiquanfan.xyz/2020/06/10/QT/","excerpt":"","text":"前言PyQt 是Python语言的GUI编程解决方案之一，是类似于 Tkinter 的一个高级库。 为了更好的辅助PyQt界面的搭建，可以通过Qt Designer完成GUI界面设计。 使用Qt Designer可以通过拖拽、点击完成GUI界面设计，并且设计完成后生成的.ui程序可以通过 pyuic5 命令直接转换成.py文件以供python程序调用。 搭建完界面并写好逻辑后，还可通过 pyinstaller 将.py文件封装成.exe文件，以供没有python解释器的用户使用。 本文以搭建标注工具界面程序为例。 预安装的软件与库 Qt Designer: pip install —pre pyqt5-tools~=5.11（位于\\Python36\\Lib\\site-packages\\pyqt5_tools\\designer.exe，也可通过这里下载） PyQt5: pip install PyQt5 pip install pyinstaller Qt Designer 的界面设计 Qt Designer 的界面主要分为四大区：项目区、控件区、编辑区、属性区。 具体而言，就是在【控件区】里点击添加需要的控件，这些控件的效果会在【编辑区】里实时显示，并在【属性区】这些控件的属性，【项目区】用于显示控件间的层级关系。 在新建一个窗口后，一般需要通过 Container 确定外部轮廓，可选用常见的 Frame 控件，再在 Frame 里边选用 Layouts 来规范后续控件的排列样式，常用水平或垂直排列，最后再选用具体部件往里边填充。 常用的控件有各种Button（按钮）、Label（静态显示文本框）、Text Edit（输入输出文本框）、listWidget（列表显示框）、Check Box（选中框）、各种Slider（滑动条）等。 每一个组件都有可设置的属性，最重要的通用属性有 objectName （用于在后续逻辑编写时指明时哪个控件），text （用于在GUI里在控件上显示）， geometry （用于设置控件位置和尺寸，但控件位于Layer中时就不可设置了）。 设计好界面之后保存可以生成my_win.ui文件，它可以直接在python代码里被加载使用，但为了在代码里进一步调用修改等，更好的方法是将.ui文件转换成相应的.py文件。这需要借助 \\Python36\\Scripts\\pyuic5.exe工具。 pyuic5 -o my_win.py my_win.ui Qt 逻辑编写很多控件都可以通过点击（或其他操作）触发事件，事件响应可自由编写，通过 connect 函数绑定。 #!/usr/bin/env python3 # -*- coding: utf-8 -*- &quot;&quot;&quot; Created on Sun June 12 12:06:21 2020 @author: weiquan fan &quot;&quot;&quot; import sys, os from PyQt5.QtWidgets import QApplication, QMainWindow, QMessageBox, QCompleter, QFileDialog from PyQt5.QtMultimedia import QMediaContent, QMediaPlayer from PyQt5.QtCore import pyqtSignal, QEvent from PyQt5.Qt import QUrl from my_win import Ui_MainWindow import csv root_path_metadata = &quot;./data/&quot; if not os.path.exists(root_path_metadata): os.makedirs(root_path_metadata) class mainWin(QMainWindow, Ui_MainWindow): doubleClicked_speaker = pyqtSignal() doubleClicked_dialog = pyqtSignal() def __init__(self, parent=None): super(mainWin, self).__init__(parent) self.setupUi(self) ## emotion self.refresh_1() self.radioButton.clicked.connect(self.showPos) self.radioButton_2.clicked.connect(self.showNeu) self.radioButton_3.clicked.connect(self.showNeg) ## DA self.refresh_2() ## dialog identity self.refresh_3() self.frame_9.setHidden(True) self.checkBox_5.stateChanged.connect(self.use_subsysdem3) self.radioButton_5.clicked.connect(self.personA) self.radioButton_4.clicked.connect(self.personB) ## save buttons self.refresh_save() self.btn_save.clicked.connect(self.save_data) self.btn_save_2.clicked.connect(self.save_dialog_data) ## history self.list_speaker = [] self.list_dialog = [] self.lineEdit_speaker.installEventFilter(self) self.lineEdit.installEventFilter(self) self.doubleClicked_speaker.connect(self.completer_name_speaker) self.doubleClicked_dialog.connect(self.completer_name_dialog) ## video player self.player = QMediaPlayer() self.player.setVideoOutput(self.wgt_player) self.btn_open.clicked.connect(self.openVideoFile) self.btn_play_pause.clicked.connect(self.playPause) self.player.durationChanged.connect(self.getDuration) self.player.positionChanged.connect(self.getPosition) self.sld_duration.sliderMoved.connect(self.updatePosition) ## for opening video def openVideoFile(self): name = QFileDialog.getOpenFileName()[0] self.lineEdit.setText(name.split(&#39;/&#39;)[-1]) self.player.setMedia(QMediaContent(QUrl.fromLocalFile(name))) # self.player.setMedia(QMediaContent(QFileDialog.getOpenFileUrl()[0])) self.player.play() def playPause(self): if self.player.state()==1: self.player.pause() else: self.player.play() def getDuration(self, d): self.sld_duration.setRange(0, d) self.sld_duration.setEnabled(True) self.displayTime(d) def getPosition(self, p): self.sld_duration.setValue(p) self.displayTime(p) def displayTime(self, ms): minutes = int(ms/60000) seconds = int((ms-minutes*60000)/1000) dur_ms = self.sld_duration.maximum() dur_min = int(dur_ms/60000) dur_sec = int((dur_ms-dur_min*60000)/1000) self.lab_duration.setText(&#39;{:0&gt;2d}:{:0&gt;2d} / {:0&gt;2d}:{:0&gt;2d}&#39;.format(minutes, seconds, dur_min, dur_sec)) def updatePosition(self, v): self.player.setPosition(v) self.displayTime(self.sld_duration.maximum()-v) ## for history def eventFilter(self, widget, event): if widget == self.lineEdit_speaker: if event.type() == QEvent.MouseButtonDblClick: self.doubleClicked_speaker.emit() elif widget == self.lineEdit: if event.type() == QEvent.MouseButtonDblClick: self.doubleClicked_dialog.emit() return super().eventFilter(widget, event) def completer_name_dialog(self): self.completer = QCompleter(self.list_dialog) self.lineEdit.setCompleter(self.completer) self.completer.setCompletionMode(QCompleter.UnfilteredPopupCompletion) self.completer.complete() self.completer.popup() def completer_name_speaker(self): self.completer = QCompleter(self.list_speaker) self.lineEdit_speaker.setCompleter(self.completer) self.completer.setCompletionMode(QCompleter.UnfilteredPopupCompletion) self.completer.complete() self.completer.popup() ## for label 1 def showPos(self): self.listWidget.clear() self.listWidget.addItem(&quot;高兴&quot;) self.listWidget.addItem(&quot;兴奋&quot;) self.listWidget.addItem(&quot;自豪&quot;) self.listWidget.addItem(&quot;满足&quot;) self.listWidget.addItem(&quot;感激&quot;) self.listWidget.addItem(&quot;自信&quot;) self.listWidget.addItem(&quot;轻松&quot;) self.listWidget.addItem(&quot;羡慕&quot;) def showNeg(self): self.listWidget.clear() self.listWidget.addItem(&quot;生气&quot;) self.listWidget.addItem(&quot;伤心&quot;) self.listWidget.addItem(&quot;害怕&quot;) self.listWidget.addItem(&quot;烦恼&quot;) self.listWidget.addItem(&quot;孤独&quot;) self.listWidget.addItem(&quot;羞愧&quot;) self.listWidget.addItem(&quot;恶心&quot;) self.listWidget.addItem(&quot;失望&quot;) self.listWidget.addItem(&quot;郁闷&quot;) self.listWidget.addItem(&quot;不安&quot;) self.listWidget.addItem(&quot;紧张&quot;) self.listWidget.addItem(&quot;无奈&quot;) self.listWidget.addItem(&quot;纠结&quot;) def showNeu(self): self.listWidget.clear() self.listWidget.addItem(&quot;共情&quot;) self.listWidget.addItem(&quot;平静&quot;) ## for label 3 def use_subsysdem3(self): if self.checkBox_5.isChecked(): self.frame_9.setHidden(False) else: self.refresh_3() self.frame_9.setHidden(True) def personA(self): self.frame_3.setHidden(False) self.frame_8.setHidden(True) def personB(self): self.frame_3.setHidden(True) self.frame_8.setHidden(False) def refresh_gui(self): self.refresh_1() self.refresh_2() self.refresh_3() self.refresh_save() def refresh_1(self): self.buttonGroup_2.setExclusive(False) self.radioButton.setChecked(False) self.radioButton_2.setChecked(False) self.radioButton_3.setChecked(False) self.buttonGroup_2.setExclusive(True) self.listWidget.clear() self.checkBox_3.setChecked(True) self.checkBox_2.setChecked(True) self.checkBox.setChecked(True) self.checkBox_4.setChecked(False) def refresh_2(self): self.listWidget_2.clear() self.listWidget_2.addItem(&quot;问候&quot;) self.listWidget_2.addItem(&quot;提问&quot;) self.listWidget_2.addItem(&quot;回答&quot;) self.listWidget_2.addItem(&quot;陈述观点&quot;) self.listWidget_2.addItem(&quot;陈述非观点&quot;) self.listWidget_2.addItem(&quot;道歉&quot;) self.listWidget_2.addItem(&quot;命令&quot;) self.listWidget_2.addItem(&quot;赞同&quot;) self.listWidget_2.addItem(&quot;反对&quot;) self.listWidget_2.addItem(&quot;表达知会&quot;) self.listWidget_2.addItem(&quot;欣赏&quot;) self.listWidget_2.addItem(&quot;叹词&quot;) self.listWidget_2.addItem(&quot;结束对话&quot;) self.listWidget_2.addItem(&quot;引用&quot;) self.listWidget_2.addItem(&quot;其他&quot;) def refresh_3(self): # self.checkBox_5.setChecked(False) self.buttonGroup.setExclusive(False) self.radioButton_4.setChecked(False) self.radioButton_5.setChecked(False) self.buttonGroup.setExclusive(True) self.buttonGroup_3.setExclusive(False) self.radioButton_6.setChecked(False) self.radioButton_7.setChecked(False) self.radioButton_8.setChecked(False) self.radioButton_9.setChecked(False) self.radioButton_10.setChecked(False) self.buttonGroup_3.setExclusive(True) # self.frame_9.setHidden(True) self.frame_3.setHidden(True) self.frame_8.setHidden(True) def refresh_save(self): self.lineEdit_2.setText(&#39;0&#39;) self.lineEdit_3.setText(&#39;0&#39;) self.lineEdit_4.setText(&#39;0&#39;) self.lineEdit_5.setText(&#39;0&#39;) self.lineEdit_6.setText(&#39;0&#39;) self.lineEdit_7.setText(&#39;0&#39;) self.lineEdit_speaker.setText(&#39;&#39;) def save_data(self): ## check many things try: self.label_val = self.buttonGroup_2.checkedButton().text() self.label_emotion = self.listWidget.selectedItems()[0].text() except: QMessageBox.information(self,&#39;提示&#39;,&#39;请选择具体情感后再重新保存&#39;, QMessageBox.Yes) return False try: self.label_da = self.listWidget_2.selectedItems()[0].text() except: QMessageBox.information(self,&#39;提示&#39;,&#39;请选择对话状态后再重新保存&#39;, QMessageBox.Yes) return False self.label_iden_isok = self.checkBox_5.isChecked() if self.label_iden_isok: if self.buttonGroup.checkedId() == -1: QMessageBox.information(self,&#39;提示&#39;,&#39;您已勾选该对话身份可标，请选择说话人身份后再重新保存&#39;, QMessageBox.Yes) return False else: self.label_iden = self.buttonGroup.checkedButton().text() if self.label_iden == &quot;倾诉者&quot;: self.label_reason = self.lineEdit_reason.text() self.label_result = self.lineEdit_result.text() self.label_reaction = &quot;空&quot; else: self.label_reason = &quot;空&quot; self.label_result = &quot;空&quot; try: self.label_reaction = self.buttonGroup_3.checkedButton().text() except: QMessageBox.information(self,&#39;提示&#39;,&#39;您已勾选该对话身份可标，请选择倾诉者反应后再重新保存&#39;, QMessageBox.Yes) return False else: self.label_iden = &quot;不可标&quot; self.label_reason = &quot;不可标&quot; self.label_result = &quot;不可标&quot; self.label_reaction = &quot;不可标&quot; if self.lineEdit_speaker.text() == &#39;&#39;: QMessageBox.information(self,&#39;提示&#39;,&#39;请输入说话人姓名&#39;, QMessageBox.Yes) return False else: self.name_speaker = self.lineEdit_speaker.text() try: self.start_time = &quot;{}:{}:{}&quot;.format(int(self.lineEdit_2.text()), int(self.lineEdit_3.text()), int(self.lineEdit_4.text())) self.end_time = &quot;{}:{}:{}&quot;.format(int(self.lineEdit_5.text()), int(self.lineEdit_6.text()), int(self.lineEdit_7.text())) except: QMessageBox.information(self,&#39;提示&#39;,&#39;时间应输入整数&#39;, QMessageBox.Yes) return False if self.lineEdit.text() == &#39;&#39;: QMessageBox.information(self,&#39;提示&#39;,&#39;请输入视频名字&#39;, QMessageBox.Yes) return False else: self.name_dialog = self.lineEdit.text() if not os.path.exists(root_path_metadata+self.name_dialog+&#39;.csv&#39;): with open(root_path_metadata+self.name_dialog+&#39;.csv&#39;,&quot;a&quot;,newline=&#39;&#39;,encoding=&#39;utf_8_sig&#39;) as csvfile: writer = csv.writer(csvfile, delimiter=&#39;,&#39;) writer.writerow([&#39;视频名字&#39;, &#39;说话者姓名&#39;, &#39;起始时间&#39;, &#39;结束时间&#39;, &#39;情绪（粗粒度）&#39;, &#39;情绪（细粒度）&#39;, &#39;是否基于音频&#39;, &#39;是否基于视频&#39;, &#39;是否基于文本&#39;, &#39;是否难以标注&#39;, &#39;对话状态&#39;, &#39;是否可标对话身份&#39;, &#39;说话人身份&#39;, &#39;起因&#39;, &#39;结果&#39;, &#39;倾诉者反应&#39;]) ## save self.label_emotion_audio_based = self.checkBox_3.isChecked() self.label_emotion_video_based = self.checkBox_2.isChecked() self.label_emotion_text_based = self.checkBox.isChecked() self.label_emotion_hard = self.checkBox_4.isChecked() onelist = [self.name_dialog, self.name_speaker, self.start_time, self.end_time, self.label_val, self.label_emotion, self.label_emotion_audio_based, self.label_emotion_video_based, self.label_emotion_text_based, self.label_emotion_hard, self.label_da, self.label_iden_isok, self.label_iden, self.label_reason, self.label_result, self.label_reaction] with open(root_path_metadata+self.name_dialog+&#39;.csv&#39;,&quot;a&quot;,newline=&#39;&#39;,encoding=&#39;utf_8_sig&#39;) as csvfile: writer = csv.writer(csvfile, delimiter=&#39;,&#39;) writer.writerow(onelist) self.refresh_gui() self.list_dialog.append(self.name_dialog) self.list_speaker.append(self.name_speaker) self.list_dialog = list(set(self.list_dialog)) self.list_speaker = list(set(self.list_speaker)) # self.lineEdit.setCompleter(QCompleter(self.list_dialog)) # self.lineEdit_speaker.setCompleter(QCompleter(self.list_speaker)) return True def save_dialog_data(self): flag_save_success = self.save_data() if flag_save_success == False: return 0 QMessageBox.about(self,&#39;提示&#39;,&#39;对话保存成功&#39;) self.refresh_gui() self.lineEdit.setText(&#39;&#39;) self.lineEdit_reason.setText(&#39;&#39;) self.lineEdit_result.setText(&#39;&#39;) self.checkBox_5.setChecked(False) self.frame_9.setHidden(True) def del_last_data(self): try: with open(root_path_metadata+self.name_dialog+&#39;.csv&#39;,&quot;r&quot;,newline=&#39;&#39;,encoding=&#39;utf_8_sig&#39;) as csvfile: data = csvfile.readlines() del data[-1] with open(root_path_metadata+self.name_dialog+&#39;.csv&#39;,&quot;w&quot;,newline=&#39;&#39;,encoding=&#39;utf_8_sig&#39;) as csvfile: writer = csv.writer(csvfile, delimiter=&#39;,&#39;) for row in data: writer.writerow(row.strip().split(&#39;,&#39;)) # writer.writerows(data) QMessageBox.about(self,&#39;提示&#39;,&#39;上一句的标注已删除&#39;) except: QMessageBox.information(self,&#39;提示&#39;,&#39;该视频尚未保存任何数据&#39;, QMessageBox.Yes) if __name__ == &#39;__main__&#39;: app = QApplication(sys.argv) main_win = mainWin() main_win.show() # main_win.showFullScreen() sys.exit(app.exec_()) 封装成可执行文件对于编写好的.py文件，若是需要更好的给没有python编辑器的人使用，则需要封装成.exe文件，这可以通过 pyinstaller 命令来完成。 pyinstaller -F biaozhu.py pyinstaller 有两种常见的模式：-F: 將程式打包成单一的执行文件(适合比较简单的代码)-D: 打包多個文件，exe及依赖的东西会一起放置在dist資料夾里(适合框架形式的程式) 在打包过程中，包含如下步骤 在路径下生成了biaozhu.spec: 包含打包时相关的设定 建立build 文件夹，放置了log记录和相关文件 建立dist 文件夹，放置了可执行文件，若是 —F 模式，则里边仅有一个.exe文件 另外，若是打包失败，可以通过改写.spec文件，再通过 pyinstaller -D XXX.spec 重新打包。 如果是库的 import 问题，可以通过 hiddenimport 里放置库名来hidden掉该错误。 总结以前只知道 Tkinter 可以来实现 python 的界面设计，但感觉并不那么友好。 而这次学习到的 PyQt 以及相应的 Qt designer则很好的解决了这一问题，可以通过拉拽进行布局，就像c++的SDL一样。完成界面设计后的逻辑编写才是更让人头疼的问题，容易产生各种bug，只能慢慢调。 最后完成程序之后还可以转成.exe文件，从而可以直接给别人使用，这个是意料之外的惊喜。","categories":[{"name":"界面","slug":"界面","permalink":"http://weiquanfan.xyz/categories/界面/"}],"tags":[{"name":"界面","slug":"界面","permalink":"http://weiquanfan.xyz/tags/界面/"}]},{"title":"深度学习主流框架的代码实例","slug":"DL-Framework","date":"2020-05-27T14:57:23.000Z","updated":"2020-05-27T15:29:06.309Z","comments":true,"path":"2020/05/27/DL-Framework/","link":"","permalink":"http://weiquanfan.xyz/2020/05/27/DL-Framework/","excerpt":"","text":"前言深度学习框架从一开始的 Theano、TensorFlow，到后来封装程度更高的Pytorch、Keras等，层出不穷。此文通过一个简单的分类任务，综合进这些框架的代码。代码来源于莫烦python。 Theanofrom __future__ import print_function import numpy as np import theano import theano.tensor as T def compute_accuracy(y_target, y_predict): correct_prediction = np.equal(y_predict, y_target) accuracy = np.sum(correct_prediction)/len(correct_prediction) return accuracy rng = np.random N = 400 # training sample size feats = 784 # number of input variables # generate a dataset: D = (input_values, target_class) D = (rng.randn(N, feats), rng.randint(size=N, low=0, high=2)) # Declare Theano symbolic variables x = T.dmatrix(&quot;x&quot;) y = T.dvector(&quot;y&quot;) # initialize the weights and biases W = theano.shared(rng.randn(feats), name=&quot;w&quot;) b = theano.shared(0., name=&quot;b&quot;) # Construct Theano expression graph p_1 = T.nnet.sigmoid(T.dot(x, W) + b) # Logistic Probability that target = 1 (activation function) prediction = p_1 &gt; 0.5 # The prediction thresholded xent = -y * T.log(p_1) - (1-y) * T.log(1-p_1) # Cross-entropy loss function # or # xent = T.nnet.binary_crossentropy(p_1, y) # this is provided by theano cost = xent.mean() + 0.01 * (W ** 2).sum()# The cost to minimize (l2 regularization) gW, gb = T.grad(cost, [W, b]) # Compute the gradient of the cost # Compile learning_rate = 0.1 train = theano.function( inputs=[x, y], outputs=[prediction, xent.mean()], updates=((W, W - learning_rate * gW), (b, b - learning_rate * gb))) predict = theano.function(inputs=[x], outputs=prediction) # Training for i in range(500): pred, err = train(D[0], D[1]) if i % 50 == 0: print(&#39;cost:&#39;, err) print(&quot;accuracy:&quot;, compute_accuracy(D[1], predict(D[0]))) print(&quot;target values for D:&quot;) print(D[1]) print(&quot;prediction on D:&quot;) print(predict(D[0]) 先搭建计算图，再通过theano.function绑定好输入和输出，形成一个函数（如train，predict） TensorFlowfrom __future__ import print_function import tensorflow as tf from tensorflow.examples.tutorials.mnist import input_data # number 1 to 10 data mnist = input_data.read_data_sets(&#39;MNIST_data&#39;, one_hot=True) def compute_accuracy(v_xs, v_ys): global prediction y_pre = sess.run(prediction, feed_dict={xs: v_xs, keep_prob: 1}) correct_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(v_ys,1)) accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) result = sess.run(accuracy, feed_dict={xs: v_xs, ys: v_ys, keep_prob: 1}) return result def weight_variable(shape): initial = tf.truncated_normal(shape, stddev=0.1) return tf.Variable(initial) def bias_variable(shape): initial = tf.constant(0.1, shape=shape) return tf.Variable(initial) def conv2d(x, W): # stride [1, x_movement, y_movement, 1] # Must have strides[0] = strides[3] = 1 return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=&#39;SAME&#39;) def max_pool_2x2(x): # stride [1, x_movement, y_movement, 1] return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding=&#39;SAME&#39;) # define placeholder for inputs to network xs = tf.placeholder(tf.float32, [None, 784])/255. # 28x28 ys = tf.placeholder(tf.float32, [None, 10]) keep_prob = tf.placeholder(tf.float32) x_image = tf.reshape(xs, [-1, 28, 28, 1]) # print(x_image.shape) # [n_samples, 28,28,1] ## conv1 layer ## W_conv1 = weight_variable([5,5, 1,32]) # patch 5x5, in size 1, out size 32 b_conv1 = bias_variable([32]) h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1) # output size 28x28x32 h_pool1 = max_pool_2x2(h_conv1) # output size 14x14x32 ## conv2 layer ## W_conv2 = weight_variable([5,5, 32, 64]) # patch 5x5, in size 32, out size 64 b_conv2 = bias_variable([64]) h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2) # output size 14x14x64 h_pool2 = max_pool_2x2(h_conv2) # output size 7x7x64 ## fc1 layer ## W_fc1 = weight_variable([7*7*64, 1024]) b_fc1 = bias_variable([1024]) # [n_samples, 7, 7, 64] -&gt;&gt; [n_samples, 7*7*64] h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64]) h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1) h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob) ## fc2 layer ## W_fc2 = weight_variable([1024, 10]) b_fc2 = bias_variable([10]) prediction = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2) # the error between prediction and real data cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction), reduction_indices=[1])) # loss train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy) sess = tf.Session() # important step # tf.initialize_all_variables() no long valid from # 2017-03-02 if using tensorflow &gt;= 0.12 if int((tf.__version__).split(&#39;.&#39;)[1]) &lt; 12 and int((tf.__version__).split(&#39;.&#39;)[0]) &lt; 1: init = tf.initialize_all_variables() else: init = tf.global_variables_initializer() sess.run(init) for i in range(1000): batch_xs, batch_ys = mnist.train.next_batch(100) sess.run(train_step, feed_dict={xs: batch_xs, ys: batch_ys, keep_prob: 0.5}) if i % 50 == 0: print(compute_accuracy( mnist.test.images[:1000], mnist.test.labels[:1000])) 先搭建计算图，并创建一个sess会话，通过sess.run(train_step, feed_dict={xs: batch_xs, ys: batch_ys, keep_prob: 0.5})这样的形式进行实际训练 Pytorch# library # standard library import os # third-party library import torch import torch.nn as nn import torch.utils.data as Data import torchvision import matplotlib.pyplot as plt # torch.manual_seed(1) # reproducible # Hyper Parameters EPOCH = 1 # train the training data n times, to save time, we just train 1 epoch BATCH_SIZE = 50 LR = 0.001 # learning rate DOWNLOAD_MNIST = False # Mnist digits dataset if not(os.path.exists(&#39;./mnist/&#39;)) or not os.listdir(&#39;./mnist/&#39;): # not mnist dir or mnist is empyt dir DOWNLOAD_MNIST = True train_data = torchvision.datasets.MNIST( root=&#39;./mnist/&#39;, train=True, # this is training data transform=torchvision.transforms.ToTensor(), # Converts a PIL.Image or numpy.ndarray to # torch.FloatTensor of shape (C x H x W) and normalize in the range [0.0, 1.0] download=DOWNLOAD_MNIST, ) # plot one example print(train_data.train_data.size()) # (60000, 28, 28) print(train_data.train_labels.size()) # (60000) plt.imshow(train_data.train_data[0].numpy(), cmap=&#39;gray&#39;) plt.title(&#39;%i&#39; % train_data.train_labels[0]) plt.show() # Data Loader for easy mini-batch return in training, the image batch shape will be (50, 1, 28, 28) train_loader = Data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True) # pick 2000 samples to speed up testing test_data = torchvision.datasets.MNIST(root=&#39;./mnist/&#39;, train=False) test_x = torch.unsqueeze(test_data.test_data, dim=1).type(torch.FloatTensor)[:2000]/255. # shape from (2000, 28, 28) to (2000, 1, 28, 28), value in range(0,1) test_y = test_data.test_labels[:2000] class CNN(nn.Module): def __init__(self): super(CNN, self).__init__() self.conv1 = nn.Sequential( # input shape (1, 28, 28) nn.Conv2d( in_channels=1, # input height out_channels=16, # n_filters kernel_size=5, # filter size stride=1, # filter movement/step padding=2, # if want same width and length of this image after Conv2d, padding=(kernel_size-1)/2 if stride=1 ), # output shape (16, 28, 28) nn.ReLU(), # activation nn.MaxPool2d(kernel_size=2), # choose max value in 2x2 area, output shape (16, 14, 14) ) self.conv2 = nn.Sequential( # input shape (16, 14, 14) nn.Conv2d(16, 32, 5, 1, 2), # output shape (32, 14, 14) nn.ReLU(), # activation nn.MaxPool2d(2), # output shape (32, 7, 7) ) self.out = nn.Linear(32 * 7 * 7, 10) # fully connected layer, output 10 classes def forward(self, x): x = self.conv1(x) x = self.conv2(x) x = x.view(x.size(0), -1) # flatten the output of conv2 to (batch_size, 32 * 7 * 7) output = self.out(x) return output, x # return x for visualization cnn = CNN() print(cnn) # net architecture optimizer = torch.optim.Adam(cnn.parameters(), lr=LR) # optimize all cnn parameters loss_func = nn.CrossEntropyLoss() # the target label is not one-hotted # following function (plot_with_labels) is for visualization, can be ignored if not interested from matplotlib import cm try: from sklearn.manifold import TSNE; HAS_SK = True except: HAS_SK = False; print(&#39;Please install sklearn for layer visualization&#39;) def plot_with_labels(lowDWeights, labels): plt.cla() X, Y = lowDWeights[:, 0], lowDWeights[:, 1] for x, y, s in zip(X, Y, labels): c = cm.rainbow(int(255 * s / 9)); plt.text(x, y, s, backgroundcolor=c, fontsize=9) plt.xlim(X.min(), X.max()); plt.ylim(Y.min(), Y.max()); plt.title(&#39;Visualize last layer&#39;); plt.show(); plt.pause(0.01) plt.ion() # training and testing for epoch in range(EPOCH): for step, (b_x, b_y) in enumerate(train_loader): # gives batch data, normalize x when iterate train_loader output = cnn(b_x)[0] # cnn output loss = loss_func(output, b_y) # cross entropy loss optimizer.zero_grad() # clear gradients for this training step loss.backward() # backpropagation, compute gradients optimizer.step() # apply gradients if step % 50 == 0: test_output, last_layer = cnn(test_x) pred_y = torch.max(test_output, 1)[1].data.numpy() accuracy = float((pred_y == test_y.data.numpy()).astype(int).sum()) / float(test_y.size(0)) print(&#39;Epoch: &#39;, epoch, &#39;| train loss: %.4f&#39; % loss.data.numpy(), &#39;| test accuracy: %.2f&#39; % accuracy) if HAS_SK: # Visualization of trained flatten layer (T-SNE) tsne = TSNE(perplexity=30, n_components=2, init=&#39;pca&#39;, n_iter=5000) plot_only = 500 low_dim_embs = tsne.fit_transform(last_layer.data.numpy()[:plot_only, :]) labels = test_y.numpy()[:plot_only] plot_with_labels(low_dim_embs, labels) plt.ioff() # print 10 predictions from test data test_output, _ = cnn(test_x[:10]) pred_y = torch.max(test_output, 1)[1].data.numpy() print(pred_y, &#39;prediction number&#39;) print(test_y[:10].numpy(), &#39;real number&#39;) 动态搭建网络，一般数据导入，网络搭建 损失函数，训练都是用各自的模块完成。通过继承封装好的父类，如nn.Module进行网络搭建，torch.utils.data.Dataset导入数据等等 Keras# to try tensorflow, un-comment following two lines # import os # os.environ[&#39;KERAS_BACKEND&#39;]=&#39;tensorflow&#39; import numpy as np np.random.seed(1337) # for reproducibility from keras.datasets import mnist from keras.utils import np_utils from keras.models import Sequential from keras.layers import Dense, Activation, Convolution2D, MaxPooling2D, Flatten from keras.optimizers import Adam # download the mnist to the path &#39;~/.keras/datasets/&#39; if it is the first time to be called # training X shape (60000, 28x28), Y shape (60000, ). test X shape (10000, 28x28), Y shape (10000, ) (X_train, y_train), (X_test, y_test) = mnist.load_data() # data pre-processing X_train = X_train.reshape(-1, 1,28, 28)/255. X_test = X_test.reshape(-1, 1,28, 28)/255. y_train = np_utils.to_categorical(y_train, num_classes=10) y_test = np_utils.to_categorical(y_test, num_classes=10) # Another way to build your CNN model = Sequential() # Conv layer 1 output shape (32, 28, 28) model.add(Convolution2D( batch_input_shape=(None, 1, 28, 28), filters=32, kernel_size=5, strides=1, padding=&#39;same&#39;, # Padding method data_format=&#39;channels_first&#39;, )) model.add(Activation(&#39;relu&#39;)) # Pooling layer 1 (max pooling) output shape (32, 14, 14) model.add(MaxPooling2D( pool_size=2, strides=2, padding=&#39;same&#39;, # Padding method data_format=&#39;channels_first&#39;, )) # Conv layer 2 output shape (64, 14, 14) model.add(Convolution2D(64, 5, strides=1, padding=&#39;same&#39;, data_format=&#39;channels_first&#39;)) model.add(Activation(&#39;relu&#39;)) # Pooling layer 2 (max pooling) output shape (64, 7, 7) model.add(MaxPooling2D(2, 2, &#39;same&#39;, data_format=&#39;channels_first&#39;)) # Fully connected layer 1 input shape (64 * 7 * 7) = (3136), output shape (1024) model.add(Flatten()) model.add(Dense(1024)) model.add(Activation(&#39;relu&#39;)) # Fully connected layer 2 to shape (10) for 10 classes model.add(Dense(10)) model.add(Activation(&#39;softmax&#39;)) # Another way to define your optimizer adam = Adam(lr=1e-4) # We add metrics to get more results you want to see model.compile(optimizer=adam, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) print(&#39;Training ------------&#39;) # Another way to train the model model.fit(X_train, y_train, epochs=1, batch_size=64,) print(&#39;\\nTesting ------------&#39;) # Evaluate the model with the metrics we defined earlier loss, accuracy = model.evaluate(X_test, y_test) print(&#39;\\ntest loss: &#39;, loss) print(&#39;\\ntest accuracy: &#39;, accuracy) Keras基于Theano或TensorFlow的内核，形成了更高层的封装，有点类似Pytorch。通过model.compile()绑定模型、损失函数和优化器，并通过model.fit即可进行训练 总结这四种框架各自都在不断地自我完善推陈出新，像是目前TensorFlow 2已经把Keras合并进来了，Keras用户迁移过来也十分简单，Pytorch也在不断地让自身更简洁，如去掉了Variable变量的使用等。目前而言，TensorFlow 和 Pytorch 是两大巨头，个人感觉企业用 TensorFlow 更多，高校用 Pytorch 更多吧。","categories":[],"tags":[]},{"title":"强化学习简介","slug":"RL","date":"2020-05-27T11:34:29.000Z","updated":"2020-06-20T07:06:22.829Z","comments":true,"path":"2020/05/27/RL/","link":"","permalink":"http://weiquanfan.xyz/2020/05/27/RL/","excerpt":"","text":"前言强化学习是机器学习中的一大类，它可以让机器学着如何在环境中拿到高分, 表现出优秀的成绩. 而这些成绩背后却是他所付出的辛苦劳动, 不断的试错, 不断地尝试, 累积经验, 学习经验. 强化学习的方法可以分为理不理解所处环境。不理解环境，环境给什么就是什么，称为model-free，包含 Q learning, Sarsa, Policy Gradients 等方法。 理解环境，用多一个模型去表示环境，就是 model-based 方法。 OpenAI gym 环境库是一个编写好了多种交互环境的库，而自己编写环境是一个很耗时间的过程，以下均不涉及环境的编写。 Q learningQ learning 是一种model-free方法，它的核心在于构建一个Q表，这个表表示了处于每一种状态(state)时进行各个行动(action)的奖励值。 举例而言(莫烦python的例子)，下图就是一个强化学习的过程，有16个state(位置)，4个可选的action(上下左右)。让探索者(红框)学会走迷宫. 黄色的是天堂 (reward 1), 黑色的地狱 (reward -1)。 那么，Q learning 的流程如下。 包含了不断重复的三个步骤。 给定当前状态s和Q表， 使用贪婪算法采取一个行动a 给定当前状态s和行动a，由环境交互给出下一个状态s’和奖励r 由s、s’、a、Q表，更新得到新的Q表每次更新我们都用到了 Q 现实和 Q 估计, 而且 Q learning 的迷人之处就是 在 Q(s1, a2) 现实 中, 也包含了一个 Q(s2) 的最大估计值, 将对下一步的衰减的最大估计和当前所得到的奖励当成这一步的现实. 代码如下： import numpy as np import pandas as pd class QLearningTable: def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9): self.actions = actions # a list self.lr = learning_rate self.gamma = reward_decay self.epsilon = e_greedy self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64) def choose_action(self, observation): self.check_state_exist(observation) # action selection if np.random.uniform() &lt; self.epsilon: # choose best action state_action = self.q_table.loc[observation, :] # some actions may have the same value, randomly choose on in these actions action = np.random.choice(state_action[state_action == np.max(state_action)].index) else: # choose random action action = np.random.choice(self.actions) return action def learn(self, s, a, r, s_): self.check_state_exist(s_) q_predict = self.q_table.loc[s, a] if s_ != &#39;terminal&#39;: q_target = r + self.gamma * self.q_table.loc[s_, :].max() # next state is not terminal else: q_target = r # next state is terminal self.q_table.loc[s, a] += self.lr * (q_target - q_predict) # update def check_state_exist(self, state): if state not in self.q_table.index: # append new state to q table self.q_table = self.q_table.append( pd.Series( [0]*len(self.actions), index=self.q_table.columns, name=state, ) ) from maze_env import Maze from RL_brain import QLearningTable def update(): for episode in range(100): # initial observation observation = env.reset() while True: # fresh env env.render() # RL choose action based on observation action = RL.choose_action(str(observation)) # RL take action and get next observation and reward observation_, reward, done = env.step(action) # RL learn from this transition RL.learn(str(observation), action, reward, str(observation_)) # swap observation observation = observation_ # break while loop when end of this episode if done: break # end of game print(&#39;game over&#39;) env.destroy() if __name__ == &quot;__main__&quot;: env = Maze() RL = QLearningTable(actions=list(range(env.n_actions))) env.after(100, update) env.mainloop() SarsaSarsa 和 Q learning 很类似，差别在于Sarsa会更‘胆小’一点，不太敢尝试。它的流程如下。 可以看出，它和 Q learning 差别仅在于更新环节，具体来讲： 他在当前 state 已经想好了 state 对应的 action, 而且想好了 下一个 state_ 和下一个 action_ (Qlearning 还没有想好下一个 action_) 更新 Q(s,a) 的时候基于的是下一个贪婪算法的 Q(s_, a_) (Qlearning 是基于 maxQ(s_))这种不同之处使得 Sarsa 相对于 Qlearning, 更加的胆小. 因为 Qlearning 永远都是想着 maxQ 最大化, 因为这个 maxQ 而变得贪婪, 不考虑其他非 maxQ 的结果. 我们可以理解成 Qlearning 是一种贪婪, 大胆, 勇敢的算法, 对于错误, 死亡并不在乎. 而 Sarsa 是一种保守的算法, 他在乎每一步决策, 对于错误和死亡比较铭感. import numpy as np import pandas as pd class RL(object): def __init__(self, action_space, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9): self.actions = action_space # a list self.lr = learning_rate self.gamma = reward_decay self.epsilon = e_greedy self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64) def check_state_exist(self, state): if state not in self.q_table.index: # append new state to q table self.q_table = self.q_table.append( pd.Series( [0]*len(self.actions), index=self.q_table.columns, name=state, ) ) def choose_action(self, observation): self.check_state_exist(observation) # action selection if np.random.rand() &lt; self.epsilon: # choose best action state_action = self.q_table.loc[observation, :] # some actions may have the same value, randomly choose on in these actions action = np.random.choice(state_action[state_action == np.max(state_action)].index) else: # choose random action action = np.random.choice(self.actions) return action def learn(self, *args): pass # off-policy class QLearningTable(RL): def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9): super(QLearningTable, self).__init__(actions, learning_rate, reward_decay, e_greedy) def learn(self, s, a, r, s_): self.check_state_exist(s_) q_predict = self.q_table.loc[s, a] if s_ != &#39;terminal&#39;: q_target = r + self.gamma * self.q_table.loc[s_, :].max() # next state is not terminal else: q_target = r # next state is terminal self.q_table.loc[s, a] += self.lr * (q_target - q_predict) # update # on-policy class SarsaTable(RL): def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9): super(SarsaTable, self).__init__(actions, learning_rate, reward_decay, e_greedy) def learn(self, s, a, r, s_, a_): self.check_state_exist(s_) q_predict = self.q_table.loc[s, a] if s_ != &#39;terminal&#39;: q_target = r + self.gamma * self.q_table.loc[s_, a_] # next state is not terminal else: q_target = r # next state is terminal self.q_table.loc[s, a] += self.lr * (q_target - q_predict) # update from maze_env import Maze from RL_brain import SarsaTable def update(): for episode in range(100): # 初始化环境 observation = env.reset() # Sarsa 根据 state 观测选择行为 action = RL.choose_action(str(observation)) while True: # 刷新环境 env.render() # 在环境中采取行为, 获得下一个 state_ (obervation_), reward, 和是否终止 observation_, reward, done = env.step(action) # 根据下一个 state (obervation_) 选取下一个 action_ action_ = RL.choose_action(str(observation_)) # 从 (s, a, r, s, a) 中学习, 更新 Q_tabel 的参数 ==&gt; Sarsa RL.learn(str(observation), action, reward, str(observation_), action_) # 将下一个当成下一步的 state (observation) and action observation = observation_ action = action_ # 终止时跳出循环 if done: break # 大循环完毕 print(&#39;game over&#39;) env.destroy() if __name__ == &quot;__main__&quot;: env = Maze() RL = SarsaTable(actions=list(range(env.n_actions))) env.after(100, update) env.mainloop() Deep Q Network(DQN)DQN 是一种结合了神经网络的强化学习。普通的强化学习中需要生成一个Q表，而如果状态数太多的话Q表也极为耗内存，所以 DQN 提出了用神经网络来代替Q表的功能。网络输入一个状态，输出各个动作的Q值。网络通过对Q估计和Q现实使用RMSprop来更新参数。Q估计就是网络输出，而Q现实等于奖励+下一状态的前模型的Q估计。流程图如下： 整个算法乍看起来很复杂, 不过我们拆分一下, 就变简单了. 也就是个 Q learning 主框架上加了些装饰，包括: 记忆库 (用于重复学习) 神经网络计算 Q 值 暂时冻结 q_target 参数 (切断相关性) 具体而言，记忆库是通过存储一堆数据在一个不断更新的记忆库里，训练时随机抽取数据出来训练。神经网络用来针对输入的状态来输出采取各个行动的Q值。共用了两个网络，他们的结构一模一样，但 q_target 网络用的是主网络之前很多个step的参数，这是为了形成一种延迟，切断他们的相关性。 import numpy as np import pandas as pd import tensorflow as tf np.random.seed(1) tf.set_random_seed(1) # Deep Q Network off-policy class DeepQNetwork: def __init__( self, n_actions, n_features, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9, replace_target_iter=300, memory_size=500, batch_size=32, e_greedy_increment=None, output_graph=False, ): self.n_actions = n_actions self.n_features = n_features self.lr = learning_rate self.gamma = reward_decay self.epsilon_max = e_greedy self.replace_target_iter = replace_target_iter self.memory_size = memory_size self.batch_size = batch_size self.epsilon_increment = e_greedy_increment self.epsilon = 0 if e_greedy_increment is not None else self.epsilon_max # total learning step self.learn_step_counter = 0 # initialize zero memory [s, a, r, s_] self.memory = np.zeros((self.memory_size, n_features * 2 + 2)) # consist of [target_net, evaluate_net] self._build_net() t_params = tf.get_collection(&#39;target_net_params&#39;) e_params = tf.get_collection(&#39;eval_net_params&#39;) self.replace_target_op = [tf.assign(t, e) for t, e in zip(t_params, e_params)] self.sess = tf.Session() if output_graph: # $ tensorboard --logdir=logs # tf.train.SummaryWriter soon be deprecated, use following tf.summary.FileWriter(&quot;logs/&quot;, self.sess.graph) self.sess.run(tf.global_variables_initializer()) self.cost_his = [] def _build_net(self): # ------------------ build evaluate_net ------------------ self.s = tf.placeholder(tf.float32, [None, self.n_features], name=&#39;s&#39;) # input self.q_target = tf.placeholder(tf.float32, [None, self.n_actions], name=&#39;Q_target&#39;) # for calculating loss with tf.variable_scope(&#39;eval_net&#39;): # c_names(collections_names) are the collections to store variables c_names, n_l1, w_initializer, b_initializer = \\ [&#39;eval_net_params&#39;, tf.GraphKeys.GLOBAL_VARIABLES], 10, \\ tf.random_normal_initializer(0., 0.3), tf.constant_initializer(0.1) # config of layers # first layer. collections is used later when assign to target net with tf.variable_scope(&#39;l1&#39;): w1 = tf.get_variable(&#39;w1&#39;, [self.n_features, n_l1], initializer=w_initializer, collections=c_names) b1 = tf.get_variable(&#39;b1&#39;, [1, n_l1], initializer=b_initializer, collections=c_names) l1 = tf.nn.relu(tf.matmul(self.s, w1) + b1) # second layer. collections is used later when assign to target net with tf.variable_scope(&#39;l2&#39;): w2 = tf.get_variable(&#39;w2&#39;, [n_l1, self.n_actions], initializer=w_initializer, collections=c_names) b2 = tf.get_variable(&#39;b2&#39;, [1, self.n_actions], initializer=b_initializer, collections=c_names) self.q_eval = tf.matmul(l1, w2) + b2 with tf.variable_scope(&#39;loss&#39;): self.loss = tf.reduce_mean(tf.squared_difference(self.q_target, self.q_eval)) with tf.variable_scope(&#39;train&#39;): self._train_op = tf.train.RMSPropOptimizer(self.lr).minimize(self.loss) # ------------------ build target_net ------------------ self.s_ = tf.placeholder(tf.float32, [None, self.n_features], name=&#39;s_&#39;) # input with tf.variable_scope(&#39;target_net&#39;): # c_names(collections_names) are the collections to store variables c_names = [&#39;target_net_params&#39;, tf.GraphKeys.GLOBAL_VARIABLES] # first layer. collections is used later when assign to target net with tf.variable_scope(&#39;l1&#39;): w1 = tf.get_variable(&#39;w1&#39;, [self.n_features, n_l1], initializer=w_initializer, collections=c_names) b1 = tf.get_variable(&#39;b1&#39;, [1, n_l1], initializer=b_initializer, collections=c_names) l1 = tf.nn.relu(tf.matmul(self.s_, w1) + b1) # second layer. collections is used later when assign to target net with tf.variable_scope(&#39;l2&#39;): w2 = tf.get_variable(&#39;w2&#39;, [n_l1, self.n_actions], initializer=w_initializer, collections=c_names) b2 = tf.get_variable(&#39;b2&#39;, [1, self.n_actions], initializer=b_initializer, collections=c_names) self.q_next = tf.matmul(l1, w2) + b2 def store_transition(self, s, a, r, s_): if not hasattr(self, &#39;memory_counter&#39;): self.memory_counter = 0 transition = np.hstack((s, [a, r], s_)) # replace the old memory with new memory index = self.memory_counter % self.memory_size self.memory[index, :] = transition self.memory_counter += 1 def choose_action(self, observation): # to have batch dimension when feed into tf placeholder observation = observation[np.newaxis, :] if np.random.uniform() &lt; self.epsilon: # forward feed the observation and get q value for every actions actions_value = self.sess.run(self.q_eval, feed_dict={self.s: observation}) action = np.argmax(actions_value) else: action = np.random.randint(0, self.n_actions) return action def learn(self): # check to replace target parameters if self.learn_step_counter % self.replace_target_iter == 0: self.sess.run(self.replace_target_op) print(&#39;\\ntarget_params_replaced\\n&#39;) # sample batch memory from all memory if self.memory_counter &gt; self.memory_size: sample_index = np.random.choice(self.memory_size, size=self.batch_size) else: sample_index = np.random.choice(self.memory_counter, size=self.batch_size) batch_memory = self.memory[sample_index, :] q_next, q_eval = self.sess.run( [self.q_next, self.q_eval], feed_dict={ self.s_: batch_memory[:, -self.n_features:], # fixed params self.s: batch_memory[:, :self.n_features], # newest params }) # change q_target w.r.t q_eval&#39;s action q_target = q_eval.copy() batch_index = np.arange(self.batch_size, dtype=np.int32) eval_act_index = batch_memory[:, self.n_features].astype(int) reward = batch_memory[:, self.n_features + 1] q_target[batch_index, eval_act_index] = reward + self.gamma * np.max(q_next, axis=1) &quot;&quot;&quot; For example in this batch I have 2 samples and 3 actions: q_eval = [[1, 2, 3], [4, 5, 6]] q_target = q_eval = [[1, 2, 3], [4, 5, 6]] Then change q_target with the real q_target value w.r.t the q_eval&#39;s action. For example in: sample 0, I took action 0, and the max q_target value is -1; sample 1, I took action 2, and the max q_target value is -2: q_target = [[-1, 2, 3], [4, 5, -2]] So the (q_target - q_eval) becomes: [[(-1)-(1), 0, 0], [0, 0, (-2)-(6)]] We then backpropagate this error w.r.t the corresponding action to network, leave other action as error=0 cause we didn&#39;t choose it. &quot;&quot;&quot; # train eval network _, self.cost = self.sess.run([self._train_op, self.loss], feed_dict={self.s: batch_memory[:, :self.n_features], self.q_target: q_target}) self.cost_his.append(self.cost) # increasing epsilon self.epsilon = self.epsilon + self.epsilon_increment if self.epsilon &lt; self.epsilon_max else self.epsilon_max self.learn_step_counter += 1 def plot_cost(self): import matplotlib.pyplot as plt plt.plot(np.arange(len(self.cost_his)), self.cost_his) plt.ylabel(&#39;Cost&#39;) plt.xlabel(&#39;training steps&#39;) plt.show() from maze_env import Maze from RL_brain import DeepQNetwork def run_maze(): step = 0 # 用来控制什么时候学习 for episode in range(300): # 初始化环境 observation = env.reset() while True: # 刷新环境 env.render() # DQN 根据观测值选择行为 action = RL.choose_action(observation) # 环境根据行为给出下一个 state, reward, 是否终止 observation_, reward, done = env.step(action) # DQN 存储记忆 RL.store_transition(observation, action, reward, observation_) # 控制学习起始时间和频率 (先累积一些记忆再开始学习) if (step &gt; 200) and (step % 5 == 0): RL.learn() # 将下一个 state_ 变为 下次循环的 state observation = observation_ # 如果终止, 就跳出循环 if done: break step += 1 # 总步数 # end of game print(&#39;game over&#39;) env.destroy() if __name__ == &quot;__main__&quot;: env = Maze() RL = DeepQNetwork(env.n_actions, env.n_features, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9, replace_target_iter=200, # 每 200 步替换一次 target_net 的参数 memory_size=2000, # 记忆上限 # output_graph=True # 是否输出 tensorboard 文件 ) env.after(100, run_maze) env.mainloop() RL.plot_cost() # 观看神经网络的误差曲线 总结强化学习本身是不依赖于深度学习的，它更多的是一种思想，通过行为与环境的交互产生奖励值，从而来更新Q表(或相同功能的神经网络)。它没有一种固定的代码，只有一套模式，具体代码还得根据实际应用与交互环境来编写。","categories":[{"name":"强化学习","slug":"强化学习","permalink":"http://weiquanfan.xyz/categories/强化学习/"}],"tags":[{"name":"强化学习","slug":"强化学习","permalink":"http://weiquanfan.xyz/tags/强化学习/"}]},{"title":"使用BeautifulSoup、requests和you_get爬虫下载B站视频","slug":"BeautifulSoup","date":"2020-05-19T12:13:21.000Z","updated":"2020-05-19T12:36:15.497Z","comments":true,"path":"2020/05/19/BeautifulSoup/","link":"","permalink":"http://weiquanfan.xyz/2020/05/19/BeautifulSoup/","excerpt":"","text":"前言BeautifulSoup 是一个可以从HTML或XML文件中提取数据并解析的Python库， Requests 是一常用的可以获取和发送http的请求库， you_get 则是方便的下载各大网站的视频的命令行工具。整体流程上是，先用 Requests 请求获得网站源代码，再用 BeautifulSoup 解析网站并筛选出自己要的信息（如视频的url），最后用 you_get 下载。 例子以下代码实现的是下载B站电影。 #!/usr/bin/env python3 # -*- coding: utf-8 -*- &quot;&quot;&quot; Created on Sun Mar 15 12:06:21 2020 @author: weiquan fan &quot;&quot;&quot; from bs4 import BeautifulSoup as bs import requests,re,os def download(url, filename): path_root = &#39;./Videos&#39; os.system(&#39;you-get -o {} -O {} {}&#39;.format(path_root, filename, url)) url_base = &#39;https://www.bilibili.com/movie/?spm_id_from=333.851.b_62696c695f7265706f72745f6d6f766965.2&#39; response = requests.get(url_base) page = response.text soup = bs(page, &#39;html.parser&#39;) vids = soup.findAll(&#39;li&#39;,attrs={&#39;class&#39;:re.compile(&#39;video-item-biref.*?&#39;)})# bilibili video_urls = [] counter=1 if(vids): for v in vids: #v_link = v.find(&#39;a&#39;)[&#39;href&#39;] #v_name = v.find(&#39;img&#39;)[&#39;alt&#39;] print(v) v_link = v.find(&#39;a&#39;)[&#39;href&#39;] v_name = v.find(&#39;img&#39;)[&#39;alt&#39;] video_urls.append([v_link, v_name]) print(v_link,v_name) try: download(v_link, v_name) except Exception: print(&#39;can\\&#39;t download &#39;+v_name+&#39; in &#39;+v_link) counter -= 1 counter += 1 if(counter&gt;15): break","categories":[{"name":"爬虫","slug":"爬虫","permalink":"http://weiquanfan.xyz/categories/爬虫/"}],"tags":[]},{"title":"CNN的进击之路——讲讲ResNet, Inception, ResNeXt和Densenet等常见网络","slug":"resnet","date":"2020-05-13T06:50:15.000Z","updated":"2020-06-20T06:33:46.242Z","comments":true,"path":"2020/05/13/resnet/","link":"","permalink":"http://weiquanfan.xyz/2020/05/13/resnet/","excerpt":"","text":"前言本文是一篇大杂烩，按照发布时间总结了CNN的一些常见网络。 AlexNetAlexNet来源于ImageNet Classification with Deep Convolutional Neural Networks。在ImageNet LSVRC-2010上以远超第二的准确率夺得了冠军，拉开了深度学习热潮的大幕。 模型结构： 模型特点： 提出了非线性激活函数ReLU (之前普遍使用Sigmoid或者tanh) 提出Dropout（每次迭代训练时随机删除一些神经元） 重叠池化（池化的时候，每次移动的步长小于池化的窗口长度） 数据扩充（水平翻转图像，从原始图像中随机裁剪、平移变换，颜色、光照变换） LRN归一化层（利用临近的数据做归一化） 多GPU实现（受当时GPU限制，在每个GPU中放置一半神经元，将网络分布在两个GPU上进行并行计） VGGVGG来源于Oxford的Visual Geometry Group的组提出的Very Deep Convolutional Networks for Large-Scale Image Recognition，在ILSVRC 2014获得亚军。 模型结构： 其中D、E列就是著名的VGG-16、VGG-19。 模型特点：使用了3个3x3卷积核来代替7x7卷积核，使用了2个3x3卷积核来代替5x5卷积核。因此模型结构很统一简洁（卷积核尺寸3x3和最大池化尺寸2x2），并不断加深网络。 GoogLeNet V1GoogLeNet V1来源于Going deeper with convolutions，在ILSVRC 2014获得冠军。 该网络的核心在于提出了Inception Module。该模块有4个分支，初始版本如下图左，包含三个不同尺度的卷积核层和一个最大池化层，并在输出通道维度上合并。由于5×5的计算量大，就进一步先通过1×1卷积降低维度再通过大卷积核。这里的最大池化也是重叠池化的，经padding后不会缩小特征图尺寸。 模型结构： 模型特点： 多尺度卷积的思想让网络变宽 提出1×1卷积 GoogLeNet V2GoogLeNet V2来源于Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift。该网络基于V1版本，吸收了VGG的分解操作，使用了2个3x3卷积核来代替5x5卷积核。 模型特点： 提出了著名的BN层。 另外，为了适配BN层，增大学习速率并加快学习衰减速度以适用BN规范化后的数据；去除Dropout并减轻L2正则（因BN已起到正则化的作用）；去除LRN；更彻底地对训练样本进行shuffle；减少数据增强过程中对数据的光学畸变（因为BN训练更快，每个样本被训练的次数更少，因此更真实的样本对训练更有帮助）。 GoogLeNet V3GoogLeNet V3来源于Rethinking the Inception Architecture for Computer Vision。该网络基于V2版本，进一步改进了Inception，将3x3分解成1x3和3x1。同理，nxn可以分解成1xn和nx1。 ResNetResNet来源于大神何凯明的Deep Residual Learning for Image Recognition，在ILSVRC和COCO 2015上都夺得了冠军，有着里程碑的意义。 深度模型当深度到了几十层之后，由于梯度消失或者爆炸的原因，就容易发生退化问题：网络深度增加时，网络准确度出现饱和，甚至出现下降。现在假设我们有一个浅层网络，我们想通过向上堆积新层来建立深层网络，一个极端情况是这些增加的层什么也不学习，仅仅复制浅层网络的特征，即这样新层是恒等映射（Identity mapping）。在这种情况下，深层网络应该至少和浅层网络性能一样，也不应该出现退化现象。这引发了残差学习，即我们的目标是学习到残差F(x)=H(x)-x，则该层学习到的最终特征H(x)=F(x)+x。当残差为0时，此时堆积层仅仅做了恒等映射，至少网络性能不会下降，实际上残差不会为0，这也会使得堆积层在输入特征基础上学习到新的特征，从而拥有更好的性能。残差学习的结构下图所示。这有点类似与电路中的“短路”，所以是一种短路连接（shortcut connection）。 模型结构：ResNet网络参考VGG19网络，引入残差单元。如下图，第三列即是ResNet-34。 模型特点： 提出残差模块 模型开始变得很深，可以达到152层 卷积层由Conv+BN+ReLU变成BN+ReLU+Conv GoogLeNet V4GoogLeNet V4来源于Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning。该论文一方面沿袭v3版本，使用更多的Inception module得到GoogLeNet V4。另一方面吸收了ResNet的残差单元，提出了两种Inception-ResNet。 模型结构：下图为其中一种，Inception-ResNet-v1，具有如下特点： Inception module都是简化版，没有使用那么多的分支，因为identity部分（直接相连的线）本身包含丰富的特征信息； Inception module每个分支都没有使用pooling； 每个Inception module最后都使用了一个1x1的卷积（linear activation），作用是保证identity部分和Inception部分输出特征维度相同，这样才能保证两部分特征能够相加。 模型特点： 使得宽模型变得更深 DenseNetDenseNet来源于Densely Connected Convolutional Networks，斩获了CVPR 2017的最佳论文奖。 模型结构：DenseNet有点类似于ResNet，但本质上又有很大的不同。结构上，把以前所有层的特征图都沿着通道轴拼接起来（而不是相加）。这可以理解为充分利用产生过的特征。 如下为ResNet： 如下为DenseNet： 模型特点： 建立了不同层的连接关系，充分利用特征图 MobileNetMobileNet来源于Google提出的MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications，是一种小巧而高效的CNN模型。 模型结构：MobileNet的核心在于提出了深度可分离卷积，它把传统卷积分解成了深度卷积(depthwise convolution)和逐点卷积(pointwise convolution)，从而大量减少参数量。 对于输入特征图(DF,DF,M)，输出特征图(DG,DG,N)，传统卷积核的尺寸为(K,K,M,N)，如下图(a)。而对于深度可分离卷积，深度卷积的尺寸为(K,K,1,M)，它将这M个卷积核各自应用于输入特征图的各个通道（这与传统卷积不同，这里相乘后不需要沿着通道轴相加），输出特征为(DG,DG,M)，如(b)所示。逐点卷积的尺寸为(1,1,M,N)，这个就是普通的1×1卷积了，输出特征为(DG,DG,N)，如(c)所示。可以看到，参数量从（K×K×M×N）变成（K×K×1×M + 1×1×M×N），减小了 M(KKN - KK -N)。 模型特点： 轻型模型，可用于移动端 ResNeXtResNeXt来源于Aggregated Residual Transformations for Deep Neural Networks。它是基于ResNet，吸收了GoogLeNet的Inception，所以和谷歌的Inception-ResNet很像。 模型结构：如下图，左图是是ResNet，右图是新的ResNeXt。 该结构可以做如下等效，第三种就是等效的分组结构。 模型特点： ResNeXt的分支的拓扑结构是相同的，而Inception V4需要人工设计 提出了一种介于普通卷积核深度可分离卷积的这种策略：分组卷积 XceptionXception来源于Xception: Deep Learning with Depthwise Separable Convolutions。它是Inception-V3的另一种改进，吸收了深度可分离卷积，造就了一种参数量相对少一些的网络结构。 模型结构：Inception-V3可做如下简化，可以看到，如下图和深度可分离卷积是很像的，只是下图是先进行1×1的卷积，再进行channel-wise的spatial convolution，最后concat，而后者是先进行一个channel-wise的spatial convolution，然后是1×1的卷积。所以作者干脆把它换成深度可分离卷积。 最终整体结构如下，其中SeparalbeConv即是深度可分离卷积。 模型特点： 虽然使用了深度可分离卷积，但网络也加宽了，总体参数量和Inception-V3差不多，性能提升了。 提出时间和MobileNet相近，它们从不同的角度揭示了深度可分离卷积的强大作用，MobileNet的思路是通过将 3×3 卷积拆分的形式来减少参数数量，而Xception是通过对Inception的充分解耦来完成的。 ShuffleNetXception来源于ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices。这也是一款效率极高的轻型CNN模型，通过逐点群卷积(pointwise group convolution)和通道混洗(channel shuffle)大大降低计算量。 模型结构：如下图左是普通的分组卷积，但是经过多层分组卷积后某个输出channel仅仅来自输入channel的一小部分，学出来的特征也很局限，因此作者提出了通道混洗channel shuffle，过程如下图中，在进行GConv2之前，对其输入feature map做一个分配，也就是每个group分成几个subgroup，然后将不同group的subgroup作为GConv2的一个group的输入，使得GConv2的每一个group都能卷积输入的所有group的feature map，结果图下图右。 pointwise group convolution，其实就是带group的卷积核为1×1的卷积。下图左是一个深度可分离卷积，而中间的图则是一个使用了pointwise group convolution的ShuffleNet unit，它将1×1卷积变成分组卷积，并在第一组分组卷积后加上通道混洗而成。右边的图则是带有降采样的ShuffleNet unit，它一方面在辅分支加入步长为2的3×3平均池化，一方面将最后的相加变成了通道级联。 模型特征： 应用了1×1的通道卷积 提出了通道混洗 总结其实总的来说，创新性的应该包含了inception，残差学习，深度可分离卷积，分组卷积几种。inception有GoogLeNet V1-V4、Xception、ResNeXt。残差学习有ResNet、ResNeXt、DenseNet、GoogLeNet V4。深度可分离卷积有MobileNet、ShuffleNet、Xception。分组卷积有ResNeXt、ShuffleNet。","categories":[{"name":"深度学习模型","slug":"深度学习模型","permalink":"http://weiquanfan.xyz/categories/深度学习模型/"}],"tags":[]},{"title":"cv中Attention的奇妙旅途——讲讲Self-Attention, SENet和CBAM","slug":"SENet","date":"2020-05-08T07:57:04.000Z","updated":"2020-06-20T06:49:17.115Z","comments":true,"path":"2020/05/08/SENet/","link":"","permalink":"http://weiquanfan.xyz/2020/05/08/SENet/","excerpt":"","text":"前言由于注意力机制的高速发展，我尝试着对attention形成一种比较系统化的理解，选了比较有代表性的Self-Attention, SENet和CBAM，整理成本文。 Self-Attention在谷歌发表的Attention Is All You Need之后，Self-Attention开始广为人知。正如我此前对这篇论文的讲解，最终的注意力可以表示为下图，其中Q为Query，K为Key，V为Value，三者都是由输入X经过不同的映射得来的。这个公式可以这么记，先通过相乘得到Query和Key的相似度，而后归一化加softmax成为注意力权重，该权重乘以Value值就是输出的新表达了。 然而，这个公式在这里的输入X的维度是time × embedding，那么怎么用于三维的图像呢？很自然的可以想到，时序信号中的时间可以类比到图像中的空间，那么只需要把长和宽两个维度拉成一个维度，就形成了一行地的空间信息。那么接下来，剩下的通道（单通道或三通道），就可以类比成时序信号中token的embedding。因此，Self-Attention公式在图像上的输入的维度是spatial × channel。那么，图像上的Self-Attention本质上是计算一种空间权重。 SENet2017年Squeeze-and-Excitation Networks获得了ILSVRC的冠军，使得SENet名声大噪。其实这篇论文的核心在于提出了一种很方便嵌入其他模型的模块————SE block。它的结构图如下。前边是传统的卷积操作，得到了特征图U(H x W x C)。而后看上边的支路，共有两个操作，一个是Squeeze一个是Excitation，可以得到一串通道上的注意力权重，再把它乘进各个通道，就得到了新的特征图。 Squeeze操作对U进行Global Average Pooling，得到一串(1 x 1 x C)的权重，这里就是注意力的雏形了，比起一开始Self注意力的QK矩阵真是简单粗暴了很多。 Excitation操作也很直接，就是把刚才得到的权重经过两层全连接层（后边带ReLU）再加一层Sigmoid。至于两层全连接的神经元数，第一层是输入C个而输出C/r个神经元，第二次则是输入C/r而输出C个神经元。可以看成一种压缩再恢复的过程，r表示压缩程度。作者的实验表明r取16时效果比较好。所以，图像上的SENet本质上是计算一种通道权重。 CBAM现在我们知道了有空间的注意力，有通道的注意力，那么也可以想到一种两者都有的注意力。CBAM: Convolutional Block Attention Module就干了这么一件事。它也是一种可嵌入的模块，结构图如下,包含通道注意力模块和空间注意力模块。 这里分别讲解这两种模块。 通道注意力模块 通道注意力模块其实基本就是SE block，不同点在于除了SE用的AvgPool之外还用了MaxPool，相当于有两种Squeeze方式，而后得到的两串注意力雏形各自同样经过两层带ReLU的全连接层（注意这里avg和max使用的全连接是共享的，有点奇怪，个人感觉不共享会好一些），相加起来再经过Sigmoid，就得到了通道上的注意力，然后乘回去得到了通道上更新了的特征图。 空间注意力模块 空间注意力模块可以说是用Self的思想和SENet的操作形成的。可以看到，刚才的通道注意力是在空间上进行Pool，那么，空间注意力是不是也可以在通道上进行Pool呢？这就形成了空间注意力模块。首先，它基于通道上进行global max pooling 和global average pooling，得到的两张空间图拼接一下形成2通道，再进行一下卷积(实验表明7 * 7卷积效果好些)降成单通道，类似的经过一个Sigmoid，就得到了空间注意力权重。乘回去就得到了空间上更新了的特征图。 所以，CBAM基于senet的通道注意力，引入空间注意力，本质上是计算了通道和空间的权重。 总结经过这么一个流程可以看到： 注意力就是计算通道(嵌入)和空间(时间)的注意力权重，Self是空间，SENet是通道，CBAM是空间加通道。甚至于空间上的注意力还可以拆分，只有横轴的注意力或只有纵轴的注意力，这些都视实际输入的图像需求来选择。 注意力的形式变得简单。一开始的Self需要经过QK矩阵算出一个三维（nlp中是二维，空间拉出长宽就变成了三维）的注意力权重，表示空间上每一个点与空间上所有点的注意力。CBAM中的空间注意力模块，只需经过pooling和一些简单的其它操作即得到了一个二维的注意力权重，表示一种整体上应该关注空间上的哪些位置。","categories":[{"name":"深度学习模型","slug":"深度学习模型","permalink":"http://weiquanfan.xyz/categories/深度学习模型/"}],"tags":[{"name":"attention","slug":"attention","permalink":"http://weiquanfan.xyz/tags/attention/"}]},{"title":"讲讲横扫nlp任务的BERT模型","slug":"BERT","date":"2020-05-07T08:08:20.000Z","updated":"2020-06-20T06:36:37.133Z","comments":true,"path":"2020/05/07/BERT/","link":"","permalink":"http://weiquanfan.xyz/2020/05/07/BERT/","excerpt":"","text":"前言本文讲解Google在2019年发表的论文BERT: Pre-training of Deep Bidirectional Transformers forLanguage Understanding 。从标题可以看出，该论文基于Transformer模型，提出了一款用于语言理解的预训练模型，并在GLUE, SQuAD等nlp任务中都取得了很好的效果。该模型的创新点实际上不在于模型结构，而在于预训练的方法。以下围绕这两方面都进行一些讲解。 模型结构总体框图首先最好还是先理解一下Transfomer，这是BERT模型的基础所在。简而言之，Transfomer包含N个编码器和解码器。编码器将输入序列编码成带有全局新的特征序列，解码器将编码器的特征序列解码成预测结果。BERT使用的正是其中的编码器，模型如下所示。 注意：并不是一个Tm代表一个Transformer，可以看成一行Tm表示一个Transformer的编码器，而编码器也不是两层可以有多层。那么，输入序列送入BERT后，最后一层Tm将对每一个输入token都生成一个新的特征序列，而T则表示任务，一般都是用全连接层来完成分类任务。另外用来对比的是，与GPT是单向的Transformer连接，ELMo是双向的LSTM连接。 Embedding从框图上可以看到，输入序列是各token的embedding。在Transfomer中，这种嵌入由word embedding加上positional embedding而来。而在BERT中，还要额外加上一个segment embedding，用于指示各个token属于输入的第几个句子。这是因为有的nlp任务是输入一对句子的，需要借此加以区分。而且，positional embedding也不是沿用三角函数，三种embedding都是学习出来的。另外，一开始的token，除了要在一开始添加一个起始标志[CLS]之外，还要在不同句子的过渡位置插一个[SEP]标志（如果有多句子的话）。 迁移策略BERT是一个预训练模型，所以我们要怎么拿过来用呢？官方为我们提供了各种任务的应用方法。首先，NLP的下游任务可以分为4类： 句子关系判断：识别蕴含(entailment)、识别语义相似等 分类任务：文本分类、情感计算等 序列标注：分词、实体识别、语义标注等 生成式任务：机器翻译、文本摘要等 图中 (a) 解决的是句子关系判断问题，MultiNLI(识别蕴含，M推理出N，蕴含/矛盾/中立），QQP（识别语义相似），QNLI（识别是否回答了问题），STS-B（识别语义相似），MRPC（识别语义等价，微软）、RTE（识别蕴含，小数据），SWAG（识别回答问题，大数据)。(b) 解决的是分类任务，SST-2（情感计算，斯坦福），CoLA（句子语言性判断，是否能成句）。(c) 解决的是序列标注任务，SQuAD（判断回答的起始和结束时刻，斯坦福问答数据集，从phrase中选取answer）。(d) 解决的是序列标注任务，NER（命名实体识别）。总的来说 (a) 和 (b) 都是分类任务，差别在于输入的是一个句子还是一对句子。这类任务，只需通过在第一个token（即[CLS]标志）的特征序列送入全连接层即可获取识别结果。(c) 和 (d) 都是序列标注任务，这类则在多个token的特征序列送入全连接层，获取各自的标注结果。可以看到，目前只有生成式任务还没有被ko。 p.s. 大名鼎鼎的GLUE任务集则包含了MultiNLI、QQP、QNLI、STS-B、MRPC、RTE、WNLI(也是识别蕴含)、SST-2、CoLA。 预训练方法常见的预训练方法一般是给前面的序列去预测下一个token（像是GPT）。而BERT就提出了两个比较有意思的训练任务 —— Masked LM 和 Next Sentence Prediction。 Masked LM为了实现模型的双向，就不能一直给定前面预测后面，于是作者提出了一个trick。在训练过程中，随机mask掉15%的token，即把相应位置的token替换成一个[MASK]标识，而任务的目标就是要去恢复这个句子（包含MASK的词），而损失函数只考虑了MASK位置的预测值，忽视掉非masked的值。这样，MASK的词可前可后，就实现了模型的双向性。 由此也衍生出了一个问题，就是在实际预测的时候是不会碰到[MASK]的，用了太多[MASK]就容易影响到模型。所以作者又用了个小技巧，选中了要mask的token后，其中10%的token会被替代成其他token，10%的token不替换，剩下的80%才被替换为[MASK]。 Next Sentence Prediction由于nlp中存在需要输入两个句子的句子关系判断任务，所以需要增设一个让模型理解句子之间关系的任务，于是Next Sentence Prediction应运而生。具体而言，就是输入两个句子，由模型来判断这两个句子是不是连续的上下句。其中，为了保持样本平衡性，选了50%的连续的正样本，再随机选50%的无关的负样本。其实这个任务和seq2seq的任务有点异曲同工之妙，只是从单词级别变到了句子级别。举个例子：正样本：今天[MASK]（天气）真好，正好我们[MASK]（出去）吃饭吧。负样本：今天[MASK]（天气）真好，[MASK]（我）吃饱了。 总结其实BERT模型除了提出这两种训练方法外，大量的数据肯定对这个预训练模型有很强的作用，不过一般人没这种计算资源…所以还是很感谢谷歌开源出来的预训练模型，可以很方便的使用并达到非常好的效果。在使用时，如果需要用到自己的数据库上，要么就是完全自己写然后导入BERT模型，要么可以直接使用官方的代码，如run_glue.py，只需要修改数据的预处理，定义好新的类，然后指定类别数等参数，就可以直接使用了。 参考文献https://www.cnblogs.com/rucwxb/p/10277217.htmlhttps://zhuanlan.zhihu.com/p/46652512","categories":[{"name":"深度学习模型","slug":"深度学习模型","permalink":"http://weiquanfan.xyz/categories/深度学习模型/"}],"tags":[{"name":"BERT","slug":"BERT","permalink":"http://weiquanfan.xyz/tags/BERT/"},{"name":"Transformer","slug":"Transformer","permalink":"http://weiquanfan.xyz/tags/Transformer/"},{"name":"GLUE","slug":"GLUE","permalink":"http://weiquanfan.xyz/tags/GLUE/"}]},{"title":"Transfomer以及Self-Attention讲解","slug":"transfomer","date":"2020-05-05T12:23:39.000Z","updated":"2020-06-20T07:02:34.144Z","comments":true,"path":"2020/05/05/transfomer/","link":"","permalink":"http://weiquanfan.xyz/2020/05/05/transfomer/","excerpt":"","text":"前言这一篇主要讲解谷歌发表的Attention Is All You Need。这篇论文提出了驰名的一种注意力机制 —— self-attention 模块，并进一步提出了 Transformer 架构，从而将以往用的计算代价较大的RNN替换掉了。目前，nlp任务中效果非常好的BERT模型就是大量应用了Transformer架构的Encoder。 下边是一个很好的使用Transformer进行机器翻译任务的例子。在预测过程中，编码阶段，输入的“I arrived at the”中的每个单词都会计算与所有单词的注意力权重，并加权求和得出新的自己的表示，逐层编码。解码阶段，输入由encoder出来的所有单词的表示和上一个位置输出的embedding，经过类似的注意力操作得到这一个位置的输出，是一种随着预测位置移动的迭代过程。戳我看例子 总体框架与流程框架对照着以上例子，看下边的Transfomer总体框架图。左边为编码器，右边为解码器。编码器和解码器中都包含了Positional Encoding模块，Multi-Head Attention模块，Feed-Forward模块。下一章节会对此着重讲解。 流程定义一下符号。 emb_dim：嵌入的尺寸 input_length：输入序列的长度 target_length：目标序列的长度+1。+1是因为要移位。 vocab_size：目标词汇表中的单词数量。 则Transformer的流程可表示为： 该模型将每个token表示为维度emb_dim的向量。然后，对于特定的输入序列，我们有了尺寸为（input_length）x（emb_dimb）的矩阵。 然后添加位置信息（位置编码）。与上一步一样，此步骤将返回尺寸为（input_length）x（emb_dim）的矩阵。 数据通过N=6个编码器块。之后，我们获得尺寸为（input_length）x（emb_dim）的矩阵。 目标序列经过等同于1和2的操作，并进行mask屏蔽。输出的尺寸为（target_length）x（emb_dim）。 4的结果经过N=6个解码器块。在每个迭代中，解码器都使用编码器的输出3）。这在总框图中由从编码器到解码器的箭头表示。输出的尺寸为（target_length）x（emb_dim）。 最后，逐行使用全连接层和softmax。输出的尺寸为（target_length）x（vocab_size）。 编码器对于训练阶段和测试阶段是一样的编码过程，而解码器的流程则有所不同，因此先讲解一下解码器的训练和测试。在测试阶段，由于没有groundtruth，所以我们需要从零开始不断迭代一个词一个词地生成。具体操作如下： 计算输入序列的嵌入表示。 使用起始token例如’‘，作为第一个目标序列。该模型将预测输出一个token。 将最后一个预测token添加到目标序列，并使用它生成新的预测。 重复执行步骤3，每次的输入token和输出token都增加，直到预测的token是表示序列结束的token，例如。 在训练阶段中，由于我们事先有roundtruth，因此我们将直接为模型提供整个已移位目标序列，并要求其预测未移位目标。举个例子，目标是将句子从英语翻译成西班牙语：X = [‘Hello’，’，’，’how’，’are’，’you’，’？’]（输入序列）Y = [‘Hola’，’，’，’como’，’estas’， ‘？’]（目标序列）在前面的示例之后，我们将给解码器输入：[‘‘，’Hola’，’，’，’como’，’estas’，’？’]预期的预测将是：[‘Hola’，’，’，’como’，’estas’，’？’，’‘] 因此可以看到，解码器在训练时直接从target_length-&gt;target_length，而测试时则是从1-&gt;1 2-&gt;2 3-&gt;3 … target_length-&gt;target_length的过程，最后预测的是每次迭代中最后一个预测的token串联起来。 Positional EncodingTransformer抛弃了RNN，而RNN最大的优点就是在时间序列上对数据的抽象，所以文章中作者提出两种Positional Encoding的方法，将encoding后的数据与embedding数据求和，加入了相对位置信息。 用不同频率的sine和cosine函数直接计算 学习出一份positional embedding实验后发现两者结果一样，所以用了第一种方法，优点是不需要训练参数，而且即使在训练集中没有出现过的句子长度上也能用 对于输入序列，经过word embedding后，加上positional embedding后即可得到该序列的 representation，序列中的每个token都转换成包含 word 的特征和 word 在句子中的位置信息的向量。 Multi-Head AttentionMulti-Head Attention其实就是多个Self-Attention结构的结合。因此，首先我们需要着重学习论文的重点Self-Attention。 Self-Attention从一个比较知名的例子讲起。假如我们要翻译一个词组Thinking Machines，其中Thinking的输入的embedding vector用x1表示，Machines的embedding vector用x2表示。当我们处理Thinking这个词时，我们需要计算句子中所有词与它的Attention Score，这就像将当前词作为搜索的query，去和句子中所有词（包含该词本身）的key去匹配（点乘），看看相关度有多高。相关度进行尺度缩放与softmax归一化可以得到注意力权重，注意力与相应的value加权求和就得到新的表达。 如果将输入的所有向量合并为矩阵形式，则所有query, key, value向量也可以合并为矩阵形式表示 则上述操作可简化为矩阵形式 这就是著名的注意力公式： Multi-Head Attention基于上边的Self-Attention， 我们进一步拓展，对输入序列使用不同的Q，K，V进行多次以上操作，而后拼接起来，再转换成最终的表示。这样每个head可以学习到在不同表示空间中的特征。 可视化如下： Masked Multi-Head Attention在训练过程的解码器中，需要对输入的注意力矩阵（即上边QK经过softmax的矩阵）进行masked操作，从而不给模型看见未来信息，解决了信息泄露问题。举例来说，对于目标序列（I have a dream），I作为第一个单词，只能有和自身的attention。have作为第二个单词，有和I, have 两个attention。 a 作为第三个单词，有和I,have,a 前面三个单词的attention。到了最后一个单词dream的时候，才有对整个句子4个单词的attention。 其它操作和上述的Multi-Head Attention一致。 Encoder-Decoder Multi-Head Attention在解码器的第二层attention里，需要整合encoder的输入序列和decoder的目标序列的信息，算出相互之间的注意力。与Multi-Head Attention的不同点在于，Encoder-Decoder Multi-Head Attention的Q矩阵来自decoder，而K和V来自encoder。其实也很好理解，就是注意力矩阵是由来自解码器的Query和来自编码器的Key之间计算得来，其它操作都相同。 Feed-Forward这个就很简单了，就是简单的映射层。 Produce Output Probabilities这个其实也是普通的映射层，它将每一个目标序列的token由emb_dim映射到vocab_size，因此就可得到各个token，串成目标序列了。 总结不得不说，这确实是一篇很经典的论文，将seq2seq模型推到了一个新高度，避免了RNN的大量计算代价，从此用CNN操作序列信号就有很好的效果了。另外，Self-Attention还跨界在cv行业也有了非常多的研究。可以说cv和nlp是同源的，只需要将图像的长宽拉成一列（空间信息）类比成序列信号的序列，图像的通道类比成序列信号的embedding即可。因此，Self-Attention模块的输入在nlp上是time × embedding，在cv上是spatial × channel。此外，当下横扫nlp的BERT模型也是基于Transfomer的encoder，这也表明这个模型的重要性了。 参考文献 https://medium.com/dissecting-bert/dissecting-bert-appendix-the-decoder-3b86f66b0e5f https://zhuanlan.zhihu.com/p/80986272 https://zhuanlan.zhihu.com/p/44121378 https://zhuanlan.zhihu.com/p/39034683 https://zhuanlan.zhihu.com/p/47282410","categories":[{"name":"深度学习模型","slug":"深度学习模型","permalink":"http://weiquanfan.xyz/categories/深度学习模型/"}],"tags":[{"name":"Transformer","slug":"Transformer","permalink":"http://weiquanfan.xyz/tags/Transformer/"},{"name":"attention","slug":"attention","permalink":"http://weiquanfan.xyz/tags/attention/"}]},{"title":"常见的梯度下降算法原理","slug":"gradient-descent","date":"2020-05-04T04:58:54.000Z","updated":"2020-06-20T06:41:47.220Z","comments":true,"path":"2020/05/04/gradient-descent/","link":"","permalink":"http://weiquanfan.xyz/2020/05/04/gradient-descent/","excerpt":"","text":"前言梯度下降算法（Gradient Descent Optimization）是神经网络模型训练最常用的优化算法。对于深度学习模型，基本都是采用梯度下降算法来进行优化训练的。梯度下降算法背后的原理：目标函数 $J(\\theta)$ 关于参数 $\\theta$ 的梯度将是损失函数（loss function）上升最快的方向。而我们要最小化loss，只需要将参数沿着梯度相反的方向前进一个步长，就可以实现目标函数（loss function）的下降。这个步长 $\\eta$ 又称为学习速率。 原始的梯度下降Batch gradient descent 批梯度下降，对所有的样本计算梯度后求平均，并更新参数。 因为在执行每次更新时，我们需要在整个数据集上计算所有的梯度，所以批梯度下降法的速度会很慢，同时，批梯度下降法无法处理超出内存容量限制的数据集。批梯度下降法同样也不能在线更新模型，即在运行的过程中，不能增加新的样本。 对于凸误差函数，批梯度下降法能够保证收敛到全局最小值，对于非凸函数，则收敛到一个局部最小值。 SGD 随机梯度下降，对每个样本计算梯度，并更新一次参数。 SGD的运行速度更快 可以用于在线学习 SGD以高方差频繁地更新，导致目标函数出现剧烈波动。 与批梯度下降法的收敛会使得损失函数陷入局部最小相比，由于SGD的波动性，一方面，波动性使得SGD可以跳到新的和潜在更好的局部最优。另一方面，这使得最终收敛到特定最小值的过程变得复杂，因为SGD会一直持续波动。然而，已经证明当我们缓慢减小学习率，SGD与批梯度下降法具有相同的收敛行为，对于非凸优化和凸优化，可以分别收敛到局部最小值和全局最小值。 Mini-batch GD 小批量梯度下降法最终结合了上述两种方法的优点，在每次更新时使用个小批量训练样本 减少参数更新的方差，这样可以得到更加稳定的收敛结果 可以利用最新的深度学习库中高度优化的矩阵优化方法，高效地求解每个小批量数据的梯度。 小结原始的梯度下降方法有以下问题： 在梯度平缓的维度下降非常慢，在梯度险峻的维度容易抖动 容易陷入局部极小值或鞍点。Zero gradient,gradient descent gets stuck （在高维空间中，鞍点比局部极小值更容易出现）-选择一个合适的学习率可能是困难的。学习率太小会导致收敛的速度很慢，学习率太大会妨碍收敛，导致损失函数在最小值附近波动甚至偏离最小值-学习率调整试图在训练的过程中通过例如退火的方法调整学习率，即根据预定义的策略或者当相邻两代之间的下降值小于某个阈值时减小学习率。然而，策略和阈值需要预先设定好，因此无法适应数据集的特点-对所有的参数更新使用同样的学习率。如果数据是稀疏的，同时，特征的频率差异很大时，我们也许不想以同样的学习率更新所有的参数，对于出现次数较少的特征，我们对其执行更大的学习率 带冲量的梯度下降Momentum optimization冲量梯度下降算法是Boris Polyak在1964年提出的，其基于这样一个物理事实：将一个小球从山顶滚下，其初始速率很慢，但在加速度作用下速率很快增加，并最终由于阻力的存在达到一个稳定速率。对于冲量梯度下降算法，其更新方程如下： 可以看到，参数更新时不仅考虑当前梯度值，而且加上了一个积累项（冲量），但多了一个超参，一般取接近1的值如0.9。相比原始梯度下降算法，冲量梯度下降算法有助于加速收敛。当梯度与冲量方向一致时，冲量项会增加，而相反时，冲量项减少，因此冲量梯度下降算法可以减少训练的震荡过程。 Nesterov Accelerated Gradient (NAG)NAG算法是Yurii Nesterov在1983年提出的对冲量梯度下降算法的改进版本，其速度更快。其变化之处在于计算“超前梯度”更新冲量项，具体公式如下： 学习率自适应的梯度下降AdaGradAdaGrad是Duchi在2011年提出的一种学习速率自适应的梯度下降算法。在训练迭代过程，其学习速率是逐渐衰减的，经常更新的参数其学习速率衰减更快，这是一种自适应算法。 其更新过程如下： 把每一维度的梯度^2和记录下来，每次学习率都除以这个和 每一维度的学习率不一样，且都在不断减小 在梯度大的维度，减小下降速度；在梯度小的维度，加快下降速度 让学习率适应参数，对于出现次数较少的特征，我们对其采用更大的学习率，对于出现次数较多的特征，我们对其采用较小的学习率。因此，Adagrad非常适合处理稀疏数据。 Adagrad算法的一个主要优点是无需手动调整学习率 Adagrad的一个主要缺点是它在分母中累加梯度的平方：由于每增加一个正项，在整个训练过程中，累加的和会持续增长。这会导致学习率变小以至于最终变得无限小，在学习率无限小时，Adagrad算法将无法取得额外的信息。 RMSpropRMSprop是Hinton在他的课程上讲到的，其算是对Adagrad算法的改进，主要是解决学习速率过快衰减的问题。其实思路很简单，类似Momentum思想，引入一个超参数，在积累梯度平方项进行衰减： 此时可以看到s是梯度平方的指数加权移动平均值，其中\\gamma一般取值0.9，此时s更平稳，减少了出现的爆炸情况，因此有助于避免学习速率很快下降的问题。同时Hinton也建议学习速率设置为0.001。 Adaptive moment estimation (Adam)Adam是Kingma等在2015年提出的一种新的优化算法，其结合了Momentum和RMSprop算法的思想。相比Momentum算法，其学习速率是自适应的，而相比RMSprop，其增加了冲量项。所以，Adam是两者的结合体： 可以看到前两项和Momentum和RMSprop是非常一致的， 由于和的初始值一般设置为0，在训练初期其可能较小，第三和第四项主要是为了放大它们。最后一项是参数更新。其中超参数的建议值是 总结本文沿着梯度下降的发展大致介绍了各种常用的梯度下降算法，目前比较常用的应该仍是 Adam ， 不过我感觉其实 SGD 加梯度衰减策略可能能取得更好的效果，当然这需要设置得比较合适。 彩蛋","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://weiquanfan.xyz/categories/机器学习/"}],"tags":[]},{"title":"opensmile 工具的使用和批处理","slug":"opensmile","date":"2020-05-02T12:32:13.000Z","updated":"2020-06-20T06:13:11.980Z","comments":true,"path":"2020/05/02/opensmile/","link":"","permalink":"http://weiquanfan.xyz/2020/05/02/opensmile/","excerpt":"","text":"前言openSMILE是一款以命令行形式运行的工具，通过配置config文件来提取音频特征。主要应用于语音识别、情感计算、音乐信息获取。2.0版本之后的openSMILE包括了openCV库，可以用于视频处理和视频特征提取。官网下载有linux和windows版本提供下载，windows可以不编译直接用，建议在命令行里指明 openSMILE 绝对路径。 openSMILE的输入输出格式文件输入格式 RIFF-WAVE (PCM) (for MP3, MP4, OGG, etc. a converter needs to be used) Comma Separated Value (CSV) HTK parameter files WEKA’s ARFF format.（由htk工具产生） Video streams via openCV.（opencv产生的视频流数据） 文件输出格式 RIFF-WAVE (PCM uncompressed audio) Comma Separated Value (CSV) HTK parameter file WEKA ARFF file LibSVM feature file format Binary float matrix format 分类器和其他组件openSMILE还提供了许多VAD算法，用于判断各时间点有没有说话。 Voice Activity Detection based on Fuzzy Logic Voice Activity Detection based on LSTM-RNN with pre-trained models Turn-/Speech-segment detector LibSVM (on-line) LSTM-RNN (Neural Network) classifier which can load RNNLIB and CURRENNT nets GMM (experimental implementation from eNTERFACE’12 project, to be release soon) SVM sink (for loading linear kernel WEKA SMO models) Speech Emotion recognition pre-trained models (openEAR) openSMILE使用流程简介 先切换到处理文件SMILExtract.exe所在的目录 通过如下语句提取：windows下：SMILExtract_Release -C “配置文件” -I “要处理的音频” -O “要保存特征向量的路径及文件名”linux下：SMILExtract -C “配置文件” -I “要处理的音频” -O “要保存特征向量的路径及文件名” 官方配置文件官方提供了许多常见特征集的配置文件，如MFCC，PLP，以及各大语音比赛中效果好的特征集。 MFCC特征为了提取MFCC特征（兼容HTK），提供了以下四个文件（它们是以它们所代表的相应的HTK参数类型命名的）：MFCC12_0_D_A.conf此配置从25毫秒的音频帧中提取梅尔频率倒谱系数（以10毫秒的速率采样）（汉明窗口）。 它由26个Mel频带计算13个MFCC（0-12）组，并应用了一个权重参数为22的倒谱提升滤波器。13个一阶和13个二阶系数被附加到MFCC后。MFCC12_E_D_A.conf此配置跟MFCC12_0_D_A.conf一样，但对数能量是只加在MFCC1-12上。MFCC12_0_D_A_Z.conf这个配置跟MFCC12_0_D_A.conf配置一样，除了所有特征是参考整个输入序列进行了标准化。MFCC12_E_D_A_Z.conf这个配置跟MFCC12_E_D_A.conf配置一样，除了所有特征是参考整个输入序列进行了标准化。帧长为25ms,帧移为10ms，使用的汉明窗，预增强参数为0.97。由26个通过FFT功率谱计算的mel-滤波器组计算MFCC 0/1-12。MEL频谱的频率范围为0-8kHz，同时这些配置文件提供了-I,-O选项。输出文件格式是HTK参数文件格式。如果需要输出其他文件格式，你必须在配置文件中更改‘cHtkSink’组件类型为你想要的类型。命令行示例如下： SMILExtract -C config/MFCC12_E_D_A.conf -I input.wav -O output.mfcc.htk PLP特征用于提取PLP倒谱系数（PLP-CC）（与HTK兼容）以下四个文件（它们是以它们所代表的相应的HTK参数类型命名的）：PLP_0_D_A.conf该配置从25 ms长音频（以10ms的速率采样）帧提取Mel频率倒谱系数（汉明窗口）。它从26个Mel频带，并使用预测阶数为5计算6个PLP（0-5），并应用了一个权重参数为22的倒谱提升滤波器。6个一阶和6个二阶系数被附加到PLP-CC后。PLP_E_D_A.conf该配置与PLP_0_D_A.conf相同，但对数能量是只加在PLP1-12上。PLP_0_D_A_Z.conf此配置与PLP_0_D_A.conf相同，除了所有特征是参考整个输入序列进行了标准化。PLP_E_D_Z.conf此配置与PLP_E_D_A.conf相同，除了所有特征是参考整个输入序列进行了标准化。帧长为25ms,帧移为10ms，使用的汉明窗，预增强参数为0.97。由26个通过FFT功率谱计算的听觉mel-滤波器组(压缩系数为0.33)计算PLP 0/1-5。线性预测器的预测阶数为5。MEL频谱的频率范围为0-8kHz，同时这些配置文件提供了-I,-O选项。输出文件格式是HTK参数文件格式。如果需要输出其他文件格式，你必须在配置文件中更改‘cHtkSink’组件类型为你想要的类型。命令行示例如下： SMILExtract -C config/PLP_E_D_A.conf -I input.wav -O output.plp.htk 情感特征集自openSMILE在openEAR的项目EWS09情感识别中被使用，openSMILE提供了各种情感识别的标准特征集。The INTERSPEECH 2009 Emotion Challenge feature set（参见[SSB09]）由配置文件config/emo IS09.conf提供。它包含对LLDs应用统计函数得到的384个特征。该特征被保存在Arff格式（针对WEKA），新的实例会被附加到一个已存在文件（这是用于批处理，其中openSMILE被反复调用从多个文件提取特征到单个特征文件）。 出现在Arff文件中16个低级描述符（LLDs）的名称，见下面的列表： pcm_RMSenergy 信号帧均方根能量 mfcc 梅尔频率倒谱系数1-12 Pcm_zcr 时间信号的过零率（基于帧） voiceProb 从ACF计算的发声概率。 F0 从倒谱计算的基频 附加到低级描述符名称的后缀_sma表示它们是通过窗口长度为3的移动平均滤波器进行平滑。附加到sma的后缀_de表示当前特征是低级描述符平滑后的一阶delta系数（微分）。 max 轮廓的最大值 min 轮廓的最小值 range = max- min maxPos 最大值的绝对位置（以帧为单位） minPos 最小值的绝对位置（以帧为单位） amean 轮廓的算术平均值 linregc1 轮廓线性逼近的斜率（m） linregc2 轮廓线性逼近的偏移量（t） linregerrQ 计算的二次误差作为线性近似值和实际轮廓的差值 stddev 轮廓上的值的标准偏差 skewness 偏度（3阶矩） kurtosis 峰度（4阶矩） The INTERSPEECH 2010 Paralinguistic Challenge feature set（见2010年INTERSPEECH会议论文集）由配置文件config/IS10_paraling.conf提供。该集包含的1582个特征是由34个低级描述符（LLDs）和34个相应的delta作为68个LLDs轮廓值，在此基础上应用21个函数得到1428个特征，另外，对4个基于音高的LLD及其4个delta系数应用了19个函数得到152个特征，最后附加音高（伪音节）的数量和总数输入的持续时间（2个特征）。该特征被保存在Arff格式（针对WEKA），新的实例会被附加到一个已存在文件（这是用于批处理，其中openSMILE被反复调用从多个文件提取特征到单个特征文件）。 出现在Arff文件中34个低级描述符（LLDs）的名称，见下面的列表： pcm_loudness 归一化强度提高到0.3的幂的响度 mfcc 美尔频率倒谱系数0-14 logMelFreqBand 梅尔频带的对数功率0-7（分布范围内从0到8 kHz） lspFreq 从8个LPC系数计算出的8个线谱对频率。 F0finEnv 平滑的基频轮廓线。 voicingFinalUnclipped 最终基频候选的发声概率。Unclipped的意思是，当其低于浊音阈值时，它不被设置为零。 附加到低级描述符名称的后缀_sma表示它们是通过窗口长度为3的移动平均滤波器进行平滑。附加到sma的后缀_de表示当前特征是低级描述符平滑后的一阶delta系数（微分）。出现在Arff文件中的21个函数的名字,均在以下列表中： maxPos 最大值的绝对位置（以帧为单位） minPos 最小值的绝对位置（以帧为单位） amean 轮廓的算术平均值 linregc1 轮廓线性逼近的斜率（m） linregc2 轮廓线性逼近的偏移量（t） linregerrA 把线性误差计算作为线性近似值和实际的轮廓的误差 linregerrQ 把二次误差计算作为线性近似值和实际的轮廓的误差 stddev 轮廓中的值的标准偏差 skewness 偏度（3阶矩）。 kurtosis 峰度（4阶矩）。 quartile1 第一四分位数（25％百分位数） quartile2 第一四分位数（50％百分位数） quartile3 第一四分位数（75％百分位数） iqr1-2 四分位数间距：quartile2- quartile1 iqr2-3 四分位数间距：quartile3- quartile2 iqr1-3 四分位数间距：quartile3- quartile1 percentile1.0 轮廓的离群值鲁棒最小值，按1％百分位数表示。 percentile99.0 轮廓的离群值鲁棒最大值，按99％百分位数表示。 pctlrange0-1 由1％和99％的百分点的范围表示的离群值鲁棒信号范围“max-min”。 upleveltime75 信号超过（75％*范围+min）的时间百分比。 upleveltime90 信号超过（90％*范围+min）的时间百分比。 四个音高相关的LLD（及相应的delta系数）如下（清音区域均为0，因此功能仅适用于这些轮廓的浊音区域）： F0final 平滑的基频频率 jitterLocal 本地（帧到帧）抖动（音调周期长度偏差） jitterDDP 差分帧间抖动（‘Jitter of the Jitter’） shimmerLocal 本地（帧到帧）闪烁（音调周期幅度偏差） 对这4 + 4个LLD应用了19个函数，即上述21个函数的集合没有最小值（1％百分位数）和范围。 The INTERSPEECH 2011 Speaker State Challenge feature set（见2011年INTERSPEECH会议论文集）由配置文件config/IS11_speake_state.conf提供。该集包含的4368个特征是由4个能量相关+50个频谱相关的低级描述符（LLDs）和54个相应的delta作为108个LLDs，在此基础上应用33个基本函数+平均值、最小值、最大值、标准差得到3996个特征；5个声音相关和5个对应的delta作为10个LLDs，在此基础上应用33个基本函数+二次平均、上升时长、下降时长得到360个特征；6个F0基本函数和对应的delta，12个特征。 The INTERSPEECH 2012 Speaker Trait Challenge feature set（见2012年INTERSPEECH会议论文集）由配置文件config/IS12_speake_trait.conf提供。该集包含的6125个特征。 The INTERSPEECH 2013 ComParE Challenge feature set （见2013年INTERSPEECH会议论文集）由配置文件config/IS13_ComParE.conf提供。该集包含的6373个特征，LLD包括能量，频谱，倒谱（MFCC）、声音、对数谐波噪声比（HNR），频谱谐度和心理声学频谱清晰度。 The MediaEval 2012 TUM feature set for violent video scenes detection 针对好莱坞流行电影的暴力进行检测的特征集在config/mediaeval2012_tum_affect/，里面有不同的设置，参考文章：Florian Eyben, Felix Weninger, Nicolas Lehment, Gerhard Rigoll, Björn Schuller: ”Violent Scenes Detection with Large, Brute-forced Acoustic and Visual Feature Sets”, Proc. MediaEval 2012 Workshop, Pisa, Italy, 04.-05.10.2012. MediaEval Audio IS12based subwin2.conf包含的是从2s的子窗中提取音频特征的配置。MediaEval Audio IS12based subwin2 step0.5.conf提取一样的特征，但是2s子窗的偏移为0.5s。MediaEval VideoFunctionals.conf用于视频特征提取，如文章使用方法，需要一个包含LLDs的CSV文件（由openCV提取）作为输入和输出，ARFF文件作为视频特征。 The openSMILE/openEAR ‘emobase’ set早期的基线集（参照”emobase2”集作为新的基线集），拥有情感识别的998个声学特征，包含以下低级描述符（LLDs）：强度，响度，12 MFCC，音高（F0），浊音概率，F0包络线，8 LSF（线频谱频率），过零率， 以及这些LLD的Delta回归系数。以下函数被应用于上述LLDs及其Delta系数。：Max./Min。输入的相对位置和范围，范围，算术平均值，2线性回归系数，线性和二次误差，标准差，偏度，峰度，四分位数1-3和三位四分位数范围。 The large openSMILE emotion feature set用于提取更多的LLDs和更多的函数(6552个特征)，配置文件为config/emo_large.conf。 The openSMILE ‘emobase2010’ reference set 是基于the INTERSPEECH 2010 Paralinguistic Challenge feature set，配置文件为config/emobase2010.conf。对持续时间和位置特征的规范化进行了一些调整。这个特性集包含了一套大大增强的低级描述符(LLDs)，以及一套“emobase”相比更加精细化选择的函数列表。建议使用此特征集作为比较新的情感识别特征集和方法的参考，因为它代表当前最先进的情感和语言识别功能。该集合包含1582个特征（与INTERSPEECH 2010 Paralinguistic 挑战集相同设置），其由34个低级描述符（LLDs）和34个相应的delta作为68个LLDs轮廓值，在此基础上应用21个函数得到1 428个特征，另外，对4个基于音高的LLD及其4个delta系数应用了19个函数得到152个特征，最后附加音高（伪音节）的数量和总数输入的持续时间（2个特征）。唯一的区别是INTERSPEECH 2010 paralinguistic挑战集标准化的是是“maxPos”和“minPos”特征，本配置被标准化为段长度。 python批处理提取openSMILE特征所有支持标准数据输出格式的配置文件都可以在WINDOWS的批特征提取GUI（使用VS10 C#编写，位于progsrc/openSMILEbatchGUI/）。这个工具允许openSMILE自动的执行文件夹中的若干文件。它可以在图形界面中选择音频文件和指定输出类型。openSMILE本身提供批处理GUI（使用VS10 C#编写，位于progsrc/openSMILEbatchGUI/），但若语音数据的目录结构较复杂，还可以利用python来进行批处理。示例代码如以下： import os from subprocess import call def excute_CMD(path_ExcuteFile, path_Config, path_Audio, path_Output): cmd = path_ExcuteFile + &quot; -C &quot; + path_Config + &quot; -I &quot; + path_Audio + &quot; -O &quot; + path_Output call(cmd, shell=True) def batch_extract_features(path_Config, path_Input_Root, path_Output): path_ExcuteFile = &quot;SMILExtract_Release&quot; filename = os.listdir(path_Input_Root) for i in range(len(filename)): print(&#39;Extracting features of %s&#39; % filename[i]) path_Input = path_Input_Root + &#39;/&#39; + filename[i] + &#39;.wav&#39; excute_CMD(path_ExcuteFile, path_Config, path_Input, path_Output) path_Config = &quot;./config/IS13_ComParE.conf&quot; path_Input_Root = &#39;root_path_to_audio/&#39; path_Output = &#39;features.csv&#39; batch_extract_features(path_Config, path_Input_Root, path_Output) 输出数据格式控制对于不包含统计函数的配置文件，选项定义在config/shared/standard_data_output_lldonly.conf.inc ==============================LLD only============================= ================================CSV================================ -csvoutput &lt;filename&gt; 默认输出选项. CSV格式，存放帧向LLD -appendcsv &lt;0/1&gt; 设为1代表添加到已有CSV文件文末，默认0 -timestampcsv &lt;0/1&gt; 设为0禁止把时间步输出到CSV第二列，默认为1 -headercsv &lt;0/1&gt; 设为0禁止把标题输入到CSV，默认为1 ================================HTK================================ -output &lt;filename&gt; 输出特征汇总（函数）到HTK格式文件 ================================ARFF=============================== -arffoutput &lt;filename&gt; 默认输出选项. ARFF格式，存放帧向LLD -appendarff &lt;0/1&gt; 设为0代表不添加到已有ARFF文件文末，默认1添加 -timestamparff &lt;0/1&gt; 设为0禁止把时间步输出到ARFF第二列，默认为1 arfftargetsfile &lt;file&gt;指定配置包含定义目标域（类）的文，默认为:shared/arff_targets_conf.inc 对于包含统计函数的配置文件，如全部的INTERSPEECH和AVEC挑战集，选项定义在config/shared/standard_data_output.conf.inc =============================LLD and func ========================= -instname &lt;string&gt; 通常是输入文件的名称保存在CSV和ARFF输出的首列。默认是&quot;unknow&quot; ================================ARFF=============================== -lldarffoutput, -D &lt;filename&gt; 启动LLD帧向输出到ARFF格式文件 -appendarfflld &lt;0/1&gt; 设为1代表添加到已有ARFF文件文末，默认0覆盖 -timestamparfflld &lt;0/1&gt; 设为0禁止把时间步输出到ARFF第二列，默认为1 -lldarfftargetsfile &lt;file&gt; 指定配置包含定义目标域（类）的文，默认为: shared/arff_targets_conf.inc ================================CSV================================ -lldcsvoutput, -D &lt;filename&gt; 启动LLD帧向输出到CSV格式文件 -appendcsvlld &lt;0/1&gt; 设为1代表添加到已有CSV文件文末，默认0覆盖 -timestampcsvlld &lt;0/1&gt; 设为0禁止把时间步输出到CSV第二列，默认为1 -headercsvlld &lt;0/1&gt; 设为0禁止把标题输入到CSV，默认为1 ================================HTK================================ -lldhtkoutput &lt;filename&gt; 启动LLD帧向输出到HTK格式文件 ================================ARFF=============================== -output, -O &lt;filename&gt; 默认输出选项. ARFF格式，存放特征汇总 -appendarff &lt;0/1&gt; 设为0代表不添加到已有ARFF文件文末，默认1添加 -timestamparff &lt;0/1&gt; 设为1把时间步输出到ARFF第二列，默认为0 -arfftargetsfile &lt;file&gt;指定配置包含定义目标域（类）的文，默认为: shared/arff_targets_conf.inc ================================CSV================================ -csvoutput &lt;filename&gt; 默认输出选项. CSV格式，存放特征汇总 -appendcsv &lt;0/1&gt; 设为0代表不添加到已有CSV文件文末，默认1 -timestampcsv &lt;0/1&gt; 设为0禁止把时间步输出到CSV第二列，默认为1 -headercsv &lt;0/1&gt; 设为0禁止把标题输入到CSV，默认为1 ================================HTK================================ -htkoutput &lt;filename&gt; 输出特征汇总（函数）到HTK格式文件 如下为lldcsvoutput的定义。注：从2.2版本起，可以指定一个“?”替代文件名。它会禁止相应的输出组件，即它不会产生输出文件，在标准输出接口界面，看到的所有的文件名默认都是”?” [lldsink:cCsvSink] reader.dmLevel = lld;lld_de filename=\\cm[lldcsvoutput(D){?}:output csv file for LLD, disabled by default ?, only written if filename given] instanceName=\\cm[instname(N){unknown}:instance name] append = \\cm[appendcsvlld{0}:set to 1 to append to the LLD output csv file, default is not to append] timestamp = \\cm[timestampcsvlld{1}:set to 0 to suppress timestamp column, default is 1, i.e. to show timestamp in second column] number = 0 printHeader = \\cm[headercsvlld{1}:set to 0 to suppress header line with feature names, default is 1, i.e. to show header line] errorOnNoOutput = 1 那么，当需要同时输出lld和func时，可用如下命令 SMILExtract -C config/IS13_ComParE.conf -I input.wav -lldcsvoutput lld_output.csv -csvoutput func_output.csv 最后一点话其实如果只是用官方配置提特征那么只看批处理那里也够了。官方配置文件可以根据需求时再看需要哪个文件，也可自己按着这个格式自定义编写配置文件。另外输出格式控制感觉最好也是先看一下，我一开始都是直接用 -O 输出统计特征，但想输出lld时跑去源代码里一阵捣鼓，后来才发现它已经封装好了直接一个参数就可以了。 彩蛋","categories":[{"name":"语音特征","slug":"语音特征","permalink":"http://weiquanfan.xyz/categories/语音特征/"}],"tags":[]},{"title":"语谱图的matlab提取和python提取","slug":"specgram","date":"2020-05-02T08:43:28.000Z","updated":"2020-06-20T06:01:56.655Z","comments":true,"path":"2020/05/02/specgram/","link":"","permalink":"http://weiquanfan.xyz/2020/05/02/specgram/","excerpt":"","text":"前言语谱图（spectrogram或specgram），也叫声谱图，可以简单看做一个二维矩阵，其纵轴表示频率，横轴表示时间，矩阵的值表示能量强弱。由于它拥有着频率和时间两个维度的信息，所以是比较综合地表示原语音信息的一种特征。另外，我将其看做语音和图像的一种连接，因为图像领域的模型发展得较快，所以通过这种方式把语音转换成一种特殊的图像再进一步处理。 语谱图流程简介1. 将语音可交叉地分成多帧（由于语音的短时平稳性） 2. 各帧加窗 3. 各帧通过快速傅里叶变化（fft）得到频谱向量 4. 沿着时间轴并联各频谱向量得到语谱图 语谱图的提取语谱图的matlab提取先看一段非官方代码，结合上述步骤进行理解。 [x,Fs,nBits]=wavread(&#39;audio.wav&#39;); s=length(x); % 信号长度 w=256; % 窗长 n=w; % nfft，表示做fft变换需要的点数，一般为刚大于w的2的幂。举例，w=250，则n一般设为256 ov=w/2; % 分帧的交叉程度，常见设为窗长的二分之一或四分之一 h=w-ov; % 不重叠点数 win=hamming(n)&#39;;% 选了常见的汉明窗，并设置nfft c=1; % 指向当前帧的指针 ncols=1+fix((s-n)/h); % 计算总共有多少帧 d=zeros((1+n/2),ncols); % 语谱图初始化 for b=0:h:(s-n) % 以下处理各帧 u=win.*x((b+1):(b+n)); % 各帧加窗 t=fft(u,n); % 各帧进行fft，内容为u，nfft=n。对于fft，输入n个时域点，输出n个频域点 d(:,c)=t(1:(1+n/2))&#39;; % 并联频谱向量，注意只取1+n/2，因为负频率无意义，只留下0和正频率 c=c+1; % 移动指针 end tt=[0:h:(s-n)]/Fs; % 时间轴 ff=[0:(n/2)]*Fs/n; % 频率轴 imagesc(tt/1000,ff/1000,20*log10(abs(d))); % 绘制 colormap(hot); axis xy xlabel(&#39;时间/s&#39;); ylabel(&#39;频率/kHz&#39;); 然而，matlab其实有封装好的函数可以直接调用。 [S,F,T]=specgram(x,nfft,Fs,windows_length,overlap_length) % x 为整段语音 % nfft 为fft变换点数，其实可以直接用默认的刚大于窗长的2的幂。也可自定义为大于窗长的整数，会对帧进行补零操作 % Fs 语音采样频率 % windows_length 窗长 % overlap_length 交叉长度 % S 语谱图 % F 频率值，尺度为1+n/2 % T 时间值，尺度为1+fix((s-n)/h) 语谱图的python提取有了刚才的基础，python的代码就容易理解啦。首先同样看一下不直接调用函数的写法。 import numpy as np from scipy.io import wavfile import matplotlib.pyplot as plt Fs, x = wavfile.read(&#39;audio.wav&#39;) wave = np.array(x[:,0], dtype = &quot;float&quot;) frame_len = 1000 frame_off = frame_len // 2 # 非重叠点数 specg_len = 1024 # 可以想象1是代表第一帧，然后第二帧结尾超出第一帧frame_off个点，第三帧再超出第二帧frame_off个点，总共第二帧到最后一帧共有(wave.size - frame_len) // frame_off 帧 frame_num = (wave.size - frame_len) // frame_off + 1 # 生成汉明窗 hamwindow = np.hamming(frame_len) specg = np.zeros((frame_num, specg_len // 2 + 1)) z = np.zeros(specg_len - frame_len) for idx in range(frame_num): base = idx * frame_off frame = wave[base: base + frame_len] # 分帧 frame = np.append(frame * hamwindow, z) # 加窗 specg[idx:] = np.log10(np.abs(np.fft.rfft(frame))) # FFT，返回幅度谱 specg = np.transpose(specg) io.savemat(&#39;specgram.mat&#39;, {&#39;specg&#39;:specg}) # aspect设为auto即可自动拉宽图 plt.imshow(specg, origin=&quot;lower&quot;, cmap = &quot;jet&quot;, aspect = &quot;auto&quot;, interpolation = &quot;none&quot;) plt.show() plt.xticks([]) plt.yticks([]) plt.savefig(&#39;specgram.png&#39;,bbox_inches=&#39;tight&#39;,pad_inches=0.0) plt.close() 再看看已经封装好的版本。 from scipy import io from scipy.io import wavfile import matplotlib.pyplot as plt Fs, x = wavfile.read(&#39;audio.wav&#39;) # 读取音频 specg = plt.specgram(x, Fs = Fs, pad_to = 256, NFFT = 256, noverlap = 128) # 提取语谱图，一键操作！ io.savemat(&#39;specgram.mat&#39;, {&#39;specg&#39;:specg[0]}) # 保存语谱图 ## 照例解释下参数 # x，Fs和上边一样 # pad_to为上边的nfft # NFFT为上边的windows_length（为什么nfft不设置为上边的nfft呢，迷惑） # noverlap为上边的overlap_length 补充一个librosa版本librosa提取的是梅尔频谱图，即在频谱图基础上再进一步将各帧通过梅尔滤波器(还可加对数操作)。另外若是在此基础上再进行倒谱即获得MFCC。还要注意到，梅尔频谱图的输出尺寸，频率等于梅尔滤波器的个数n_mels, 时间则只取决于窗移(非重叠数)hop_length(还没想明白，推测可能是进行了填充，所以尺寸上忽视了窗长的影响)。此外，还可通过设置power参数来确定要计算梅尔频谱图(设置为1)还是梅尔功率图(设置为2)。 from matplotlib import pyplot as plt import librosa import librosa.display # Load a wav file y, sr = librosa.load(&#39;./test.wav&#39;, sr=None) # plot a wavform plt.figure() librosa.display.waveplot(y, sr) # plt.plot(y) plt.title(&#39;wavform&#39;) plt.show() # extract mel spectrogram feature melspec = librosa.feature.melspectrogram(y, sr, n_fft=1024, win_length=1024, hop_length=512, n_mels=128, power=2.0) # convert to log scale logmelspec = librosa.power_to_db(melspec) # plot mel spectrogram plt.figure() librosa.display.specshow(logmelspec, sr=sr, x_axis=&#39;time&#39;, y_axis=&#39;mel&#39;) plt.title(&#39;spectrogram&#39;) plt.show() # aspect设为auto即可自动拉宽图 plt.imshow(logmelspec, origin=&quot;lower&quot;, cmap = &quot;jet&quot;, aspect = &quot;auto&quot;, interpolation = &quot;none&quot;) plt.show() plt.xticks([]) plt.yticks([]) plt.savefig(&#39;specgram.png&#39;,bbox_inches=&#39;tight&#39;,pad_inches=0.0) plt.close() 下图中，第一张是梅尔频谱图，第二张是梅尔功率图，功率图的声音和噪声区分更明显。而两者都比没有梅尔滤波器的频谱图有更独特明显的能量显示。 语谱图的一些可能有的小疑惑 关于nfftnfft既表示时域的点数也关联频域的点数。该数为2的幂数时更高效，但不是也没问题。nfft需要比窗长的值更大，然后加窗后的帧会被补零到nfft长度再进行fft。 关于频率分辨率频率轴上每一个点对应fs/nfft的频率。另外由于输出nfft/2+1个频率点，所以输出的频率范围为0到nfft/2×fs/nfft=fs/2。 关于自定义输出语谱图的尺寸问题时间轴尺寸为1+fix((s-n)/h)， 由windows_length和overlap_length决定。实际应用时由于各语音长度不同，时间尺寸一般都要进行截断或补零到一个固定值。截断的话可以截一段（起始信息，中间信息），也可以截多段（交叉不交叉都行）。频率轴尺寸为1+n/2，仅决定于nfft（python中的pad_to参数），所以可以通过设置该值控制频率轴尺寸。但是也不要比窗长大太多，否则补零太多可能就没什么信息了。nfft调大时，窗长可以跟着调大，为了防止导致的时间轴太短可以调高overlap_length。另外，其他参数不变时，仅变换nfft，可视化出来时可能肉眼看起来一样，但实际分辨率仍然是不同的。这也导致了一个问题，送入网络的是要用单通道的直接计算出来的语谱图，还是用可视化函数绘制出来的三通道的语谱图，这就根据实际情况去尝试了。 彩蛋 希望疫情早点过去","categories":[{"name":"语音特征","slug":"语音特征","permalink":"http://weiquanfan.xyz/categories/语音特征/"}],"tags":[]},{"title":"卷积当中的补零操作","slug":"padding","date":"2020-05-02T08:43:28.000Z","updated":"2020-06-26T02:45:42.701Z","comments":true,"path":"2020/05/02/padding/","link":"","permalink":"http://weiquanfan.xyz/2020/05/02/padding/","excerpt":"","text":"卷积输入输出尺寸公式 这是一条比较完整的输出尺寸公式，考虑到了stride, padding, dilation, 这里的括号表示的是向下取整，这实际上是卷积图的边边剩下的部分比卷积核小，所以抛弃了这次卷积的结果。其实，若用另一种视角看，可以把空洞卷积当成更改了卷积核尺寸K的值，K -&gt; d × (K-1) +1，因此该公式可以更简洁的被表示为Out = floor((In + 2P − K)/S+1) tensorflow 版本tensorflow 版本的padding是通过直接选模式参数进行的，可选’SAME’,’VALID’.前者是通过padding在前后左右补零，使得输出尺寸保持不变（或以步长倍数缩小），非常常用，后者则是不进行padding，实际上应该是等价于上边总公式P=0的情况。SAME: Out = ceil(In/S)VALID: Out = ceil((In − K + 1)/S) pytorch 版本pytorch中padding是需要自己设置值的，因此输出尺寸就按照一开始的公式来即可。以下再看一些例子。 &gt;&gt;&gt; m = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3) &gt;&gt;&gt; input = torch.randn(20,3,24,24) &gt;&gt;&gt; m(input).shape torch.Size([20, 64, 12, 12]) &gt;&gt;&gt; input = torch.randn(20,3,25,25) &gt;&gt;&gt; m(input).shape torch.Size([20, 64, 13, 13]) &gt;&gt;&gt; input = torch.randn(20,3,24,24) &gt;&gt;&gt; m = nn.Conv2d(3, 64, kernel_size=6, stride=2, padding=3) &gt;&gt;&gt; m(input).shape torch.Size([20, 64, 13, 13]) &gt;&gt;&gt; m = nn.Conv2d(3, 64, kernel_size=6, stride=2, padding=2) &gt;&gt;&gt; m(input).shape torch.Size([20, 64, 12, 12]) &gt;&gt;&gt; input = torch.randn(20,3,25,25) &gt;&gt;&gt; m = nn.Conv2d(3, 64, kernel_size=6, stride=2, padding=3) &gt;&gt;&gt; m(input).shape torch.Size([20, 64, 13, 13]) &gt;&gt;&gt; m = nn.Conv2d(3, 64, kernel_size=6, stride=2, padding=2) &gt;&gt;&gt; m(input).shape torch.Size([20, 64, 12, 12]) 这里可以观察到： 若是需要保持 对于奇数卷积核，通过让padding=(k-1)/2，即可实现SAME的效果。即输出尺寸保持不变或以步长倍数缩小（对于奇数输入尺寸则向上取整） 对于偶数卷积核，若是偶数输入尺寸，则padding=floor((k-1)/2)可实现SAME的效果 对于偶数卷积核，若是奇数输入尺寸，则padding=ceil((k-1)/2)可实现SAME的效果 因此，建议不要用偶数卷积核。。。然后记住padding=(k-1)/2，即可实现tf中SAME的效果了。把padding带入一开始的总公式，可以得到Out = floor((In − 1)/S+1)，这实际上与SAME公式等效，可以看下边代码的暴力验证。 &gt;&gt;&gt; a = lambda x:np.ceil(x/5) &gt;&gt;&gt; b = lambda x:np.floor((x-1)/5+1) &gt;&gt;&gt; c = [np.random.randint(6,100) for i in range(10)] &gt;&gt;&gt; [a(rand)==b(rand) for rand in c] [True, True, True, True, True, True, True, True, True, True] 总结虽然想了半天这个公式，但实际使用时好像也就SAME功能用得比较多，因此记住核心： 用奇数卷积核 padding=(k-1)/2 若是空洞卷积，则代入K -&gt; d × (K-1) +1","categories":[],"tags":[{"name":"卷积","slug":"卷积","permalink":"http://weiquanfan.xyz/tags/卷积/"}]}],"categories":[{"name":"语音特征","slug":"语音特征","permalink":"http://weiquanfan.xyz/categories/语音特征/"},{"name":"工具使用","slug":"工具使用","permalink":"http://weiquanfan.xyz/categories/工具使用/"},{"name":"语音预处理","slug":"语音预处理","permalink":"http://weiquanfan.xyz/categories/语音预处理/"},{"name":"语音识别","slug":"语音识别","permalink":"http://weiquanfan.xyz/categories/语音识别/"},{"name":"界面","slug":"界面","permalink":"http://weiquanfan.xyz/categories/界面/"},{"name":"强化学习","slug":"强化学习","permalink":"http://weiquanfan.xyz/categories/强化学习/"},{"name":"爬虫","slug":"爬虫","permalink":"http://weiquanfan.xyz/categories/爬虫/"},{"name":"深度学习模型","slug":"深度学习模型","permalink":"http://weiquanfan.xyz/categories/深度学习模型/"},{"name":"机器学习","slug":"机器学习","permalink":"http://weiquanfan.xyz/categories/机器学习/"}],"tags":[{"name":"工具使用","slug":"工具使用","permalink":"http://weiquanfan.xyz/tags/工具使用/"},{"name":"语音预处理","slug":"语音预处理","permalink":"http://weiquanfan.xyz/tags/语音预处理/"},{"name":"端点检测","slug":"端点检测","permalink":"http://weiquanfan.xyz/tags/端点检测/"},{"name":"语音识别","slug":"语音识别","permalink":"http://weiquanfan.xyz/tags/语音识别/"},{"name":"界面","slug":"界面","permalink":"http://weiquanfan.xyz/tags/界面/"},{"name":"强化学习","slug":"强化学习","permalink":"http://weiquanfan.xyz/tags/强化学习/"},{"name":"attention","slug":"attention","permalink":"http://weiquanfan.xyz/tags/attention/"},{"name":"BERT","slug":"BERT","permalink":"http://weiquanfan.xyz/tags/BERT/"},{"name":"Transformer","slug":"Transformer","permalink":"http://weiquanfan.xyz/tags/Transformer/"},{"name":"GLUE","slug":"GLUE","permalink":"http://weiquanfan.xyz/tags/GLUE/"},{"name":"卷积","slug":"卷积","permalink":"http://weiquanfan.xyz/tags/卷积/"}]}